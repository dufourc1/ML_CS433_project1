{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ATTENTION: \n",
    "\n",
    "Remember to pull before any change and to push often! (Problem on merging)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test for project1, sandbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "from Regressions import *\n",
    "from proj1_helpers import *\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "y, x_all, ids = load_csv_data('../Data/train.csv')\n",
    "y[y==-1] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = preliminary_treatment_X(x_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Y shape :  (250000,)\n",
      "X shape :  (250000, 30)\n"
     ]
    }
   ],
   "source": [
    "y_all = y\n",
    "print('Y shape : ', y.shape)\n",
    "print('X shape : ', x_all.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.delete(x_all,[4,5,6,12,26,27,28],1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " which Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "which = range(1,2) #Change according to desired features in tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scatter plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scatter plot for 1th feature :\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYkAAAD4CAYAAAAZ1BptAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvhp/UCwAAIABJREFUeJztvXl8HNWd6PvtvdVarMUyliU7xtsxYLzHGBwWG5vtxcFzyWDwBEjgJnkzYW4yd5LJZPlA1nnMy9yZ5GUSZkLiALnjQIZkCLlDwmBswBgMBss2JvjICwbv2lpSt6Re1e+Prm63WlXVi1pSt/p8Px99rD51qupUuXV+5/xWSywWQ6FQKBQKPawTPQCFQqFQFC9KSCgUCoXCECUkFAqFQmGIEhIKhUKhMEQJCYVCoVAYYp/oARSajg5f3u5adXUevN6BQg5nTFHjHVtKbbxQemNW4x1bchlvY2O1Ra9d7SRSsNttEz2EnFDjHVtKbbxQemNW4x1bCjFeJSQUCoVCYYgSEgqFQqEwRAkJhUKhUBiihIRCoVAoDJl03k354hsI8ex/HuL5108wGIjisMP0ukr8gTCDwTDBcIwKhw2L3YLHaafG7SAQieIPRKhw27FYLARDESoddhwOB4PBIBHAabUSjEQZGAxisYDTbqeqwonLacNitVFf42QgGMHvD9HbH8Ris+LzBany2KmqcDMQjOBwWrFiwQp8aEY1U2s89PYHmDurjgqbDd9giEAwylAsxtzmGjp7gxCDBbNqqfY4AQiGo3T0DBIKR3A67EypdDIYjDClyoXLYSMYjtLrDyY/J84xawPo9QepcNmHXcvo3FwY7fkKhaIwZBQSQoiZwOPAdGAI+ImU8gdCiG8AnwY6tK5flVI+q53zFeA+IAr8Dynlc1r7TcAPABvwUynlQ1r7xcATQD2wD7hLShkSQri0e68AuoDNUsoTBXjuJKFIhAd/9jrnvcHh7VE4dq5/WFswEgXAS4TTBC4c6A2l9spwxzDne8MZx+UPhaFnZL9j5/wXPuw9nfE6tZUOptVVcOR0H3q5HKsq7EypdNEfCNPjD1FX5WJeSw2h8BAfnPfT4w9SW+VidlMV1RVO3jnRTVdfEJfdChYIhoewWmAoBvXVTpYtaCQGHDjSSXdfkPoaFwtn1XHnhgXD7mskBAaCEX75fBuHP/Amz1+2oJHN6+Zhs6qNr0Ix3lgyZYEVQjQBTVLKfUKIauAtYBNwO+CXUv5DWv9LgV8Cq4AZwHYgMUO0ARuAU8Be4E4p5R+FEL8CfiOlfEII8S/AASnlw0KIvwAWSyn/byHEHcCfSCk3m4031ziJv3n4VTp7A5k7KkaF22nlhitmc8sVLTz14nFa2zqGCYGPXzeHp148zisHzxAIDY04f/3KFrasX6Bz5cKTEGBzZzfg6x0cl3sWisbGajo6fBM9jKxR4x1bchmvUZxExp2ElPIscFb73SeEeBdoNjnlVuAJKWUQeE8IcZS4wAA4KqU8DiCEeAK4VbveOmCL1ucx4BvAw9q1vqG1PwX8sxDCIqUsSH5z30BICYhxIhAa4pldx2mV7Zxsv7Ab6uoLsv3NUxx+38upjn7D81vbOrnt2rlZq54yqav0jkeHhti2/Qj72zrp8QdprKvgsovrWb+ihfoad05qr/FUl6WrABXGKDVm7uRkkxBCzAaWAa8Da4D7hRB3A28Cfy2l9BIXIHtSTjvFBaFyMq39CqAB6JFSRnT6NyfOkVJGhBC9Wv9OozHW1XmyDiD52aOvZ9VPUThOd/p1280EBIDXF8DmdNA4tdK0XzQ6xNbfvcOeQ2fp6BmksbaC1YuauHfjZdhsVsPj99xyCV/64S6On+lLXqvdO0i79zQ7952msa6CK1Ouk+/9cyEQiuDtC1JX48LtHPmnWsh7TSSNjdVjfo9CvqvxGG8hGe14sxYSQogq4NfAF6SUfUKIh4FvAzHt3/8F3AvobVli6HtSxUz6k+GYLrmEzL/69rms+yoKw9BITVJW1FW7iIbCGbfO27a3sf3NU8nP7d5Bntl1nIHBEFvWLzA8/tbh85w2EVQdWr/+gSB/tkHkfX/IvJqNDg3x5I6jI1Ry6XaZbO5V7IyX+qZQ72qSq5t027MSoUIIB3EB8W9Syt8ASCnPSymjUsoh4BEuqJROATNTTm8Bzpi0dwK1Qgh7Wvuwa2nHpwDd2Yw5G1RNvvHHqqv1zEx/IMyvdh7lbFc/wXBUt08wHKW1rUP3WGtbJ76BkOHxMxl2Mgl2v30u7/sPBMNs297G1x/Zw1f+dQ9ff2QP27a3EU2TnE/uOMr2N0/R1RckxgWV3JM7jmZ9L6MxliPqXY2OjEJCCGEBfga8K6X8x5T2ppRufwIc0n5/BrhDCOHSvJbmA28QN1TPF0JcLIRwAncAz2j2hZ3Ax7Xz7wF+m3Kte7TfPw7sKJQ9QjExeNz5eV0HQkPs3Hearz3yuuHk2usP0t2n713m9QU41e43PJ7tlyoQirsS65Hp/tueP1KwyT/TvXr9mbzsygf1rkZHNjuJNcBdwDohxH7t5xbg/xVCvC2EOAisBf4KQEr5DvAr4I/AH4DPaTuOCHA/8BzwLvArrS/Al4H/qRm5G4gLJbR/G7T2/wn87egfWTGR+AcjmTtlQG9yBZhS5aK+Rt94W1ftpmValeHxXAhF9Fee5vd3cfh9/U1wPpN/pmdVRuwLqHc1OrLxbnoFfdvAsybnfBf4rk77s3rnaR5Pq3TaA8CfZhqjojxJ93iy2yx43A66dCbZZQumUu2Jx3Gk6qbz4ce/eZvlYtoIG4HLYTO8/sJZdbx6SN8Glpj8p9V5khOa3jOkTmhm91q2YKry3ElBvavRoSKuFSVL6uQKcV1+qnttgpnTqti8bh5A8t/Wtk68vgB11W4Wz2tgf1s7Xn/mIEeAbl8oOeGkGz31rr9swVQ2XT2Hwx94Czr5691rzZIZbLxyVlbPUU4Y/b8k2hXGKCGhKFlSJ1ffQIi3Duvr8gcCESLRGDYr2KxWtqxfwG3Xzh3mYWSzWnQnZqfDQiisb7HQi90wuj4wqslfb0LTu1fLjNqi9r6ZqDgFs/8XhTlKSChKlmULpmK3WfjFf0nefLcd36D+TqA7bccB8RV76mejVfmyefV88+dv6l43fSeTSvr1je6R7eRvNqHp3avYMHLrvf/2ZeM6jlJ4V8WGEhKKkqNBm2A2rrmYB376Bme7zWNjLMBzb3zAlg0LDPM/Ga3KT53poSELG0E25DL5T7bI4IRbb4KE84GnwsmmNbMnbmCKjCghoSgpLMD9t13O7rfP8Tc/foWggSoolaEY7Gw9g81mzTnIrFBGz/RJ32g1m20gXS73mmjM3Hr3HDrLzatmFsU4FfooIaEoShKZZdOpr3Hz8v4z7Gw9M/JgBszyP+lNzmuWNLPxylmjMnrmOukbrbhhpJFc716PPP02uw+cLqoMumZuvZ09g4YqO0VxUNZCosoJ/lDmforxx+O268ZULJpTz8FjXXld08yGoDc5p6ZtyKQmMlq95zLpZwqky5TgcDQCZiwxc+udWluh4hSKnNLJAjYGOB155ohQjDkOm5W1y5tp0IKgEuk89h/p0J1ssqG22kUoHB2RhiHbKOeEmih1oo5njtVPtZFrOojRRAYXc+qJhMpOj9WLmpSqqcgp651ENvpsxcTQ2x/ixg/PhFiMna1nkqqn3v7sYhn06O4L8sDWvUnDd0INk83kbKQOMVu9r1/RktN1sw2k02M0zzAeGKns7t14Gd3d2eXNUkwMZS0k+pWqacJwWCFskhG2rtpFhcuet2qpubGSrt4AgdDIFXS6GibfyTnT6n3jVbNzuu5ojOSjETDjgWF8SgmlNC9X1P+QYkK48vLpfOu+VVxx6UW6x/sDYZ7ccdRwdQxQW+XEArgcVpwOKxagocbN+pUtfOUTK/C4zNUYCTWMmTrEbHLOtHofDEZYOn+q7vGl8xt0r7t53TzWr2yhocaN1XLheTIZyfN9hvFGT2WnKG7KeiehmDgOHfdy+7r5uF02XHYrwcjwbUUgNMSrh87hdtp0dwMAFmJcuWg6WzbMJxqNcardT8u0Kqo9Ttq9A3h95lvFVDVMPikuslm9Gyk0jdpHExm8ed08PBVOdh84o1JPKAqGEhKKCcHrC/Ldx97KGAhneg1/mFcPneNku5+BQHiY2+emqy82nMATpKph8klxkUk9BHDgiH4RxQNHuvjT66KGAiCfyGCb1cqnN13OzatmFlWchKK0KWt1U6USkROGw27JSkAEQ1HWLJqe9HLS42S7f0SNhqdePGaofkmQrobJJwht87p5rF3eTF2VC0uaemii6hgolY6ikJT1NBkcfWkDRZ5E9CLldHA5bdy5YT7dvQEe2Lo36+u/2HqGa5c2cf2KZlrbOun2BZMBeg01LhbOqmPT1XMAGAhG+OXzbRz+wJtTXqFEoNzBo514/UFqq5wsnluf9JoyU0c5HTaqPI6sn6dYKbbobkXhKWshoWTExJFtretAKMrTu97jtmvnGuZQMuKl/WdZv7KF735mNb3+IE6HladePM7h97t59dA53n2/m8oKJx09AwRCFwaUbV6hdPfXHn9oWPoPM3VU4rlKpRZ1OoVIH6IoDdT/pqLoaW2L6/UzqY/Mzp1W5+HZPR/w6qFzdPtCxIjXhTjZ7h8mIFLZc+hsnvWsOzjV7iMYjrLp6jm4nfp/ZhMd5DYasqnDrZgclPVOQlEadPUF6O4LsHndPAYDEXYbVHjTI6H7n1LlMpzUjTDLK9TdFzDc1XSlBO196KJqggZCqBiC3PJhtOlDFKWF2kkoSoLn9n5AV2+A29fNo77amfV5CQ8mMyOyEWZ5hba/eTLj+V19QfYd6TR0dy2GILd0guEo7d4B0x3ORBnkFRNDWe8kLBj7qyuKi1cOnOXl/WdpqHFRWeGkO0MMRILF8+JBa2ZGZCNWL2oCoN07MMwwGwxH844ET6WYgtxysTEUe3S3orCUtZBQAqJ0SDhDdfUF6eoLUlWhnyU2nfUrWoC4W+jieVPZue90xnPcThtrLp/OUCzG1x/ZM2LSzGdXAvEkhTGgvgiD3HLJIFuoGhuK0qCshYSidBkIZBYQDTVu6mvcyc/rV7SYConaSieXXlzPlg3zeXrXe/yfV95LHkudNG+7dm7OuxKAWAy+eMdS5jRPKaqJNB8bw2hqbChKCyUkFEWD024hFMluf5dNmEX6qra+xm3oRltb5eSb966i2uOkxx9k1379okaJSdNoJW1GfY276AQE5JdBdjTpQxSlhRISiqIhWwEBxpXrgGGpwFMxU5OsXDgNj9vOtu1tvHzgtOFYEpOm3kp66fwGYsCrb5/TzTdVTKqY1CC40dgY8kkfks8Yi+W9lSNKSChKkhlTKznVMbIOwVWLpnPXjcJwUtGb3BfPa2Dtsma2bT+S0WZRVx2fVM1W0v/tmjlse/4Ih9/30uMPFpUqxshAvXT+VF54a+SzT4RgU4F6xYUSEoqS5FO3XMJr75zT1YmbTSSpk3t3X4Dtb57k4NFOdu47nax+Z8b8ltphk6beStrjcvDfP3ppUa6EjQzU61Y0s35lS1HYGIq1DGu5ooSEoigxUycB/Og3B1kupvHN+z6MfyCc80TsctjY2Xqana0XbA/Z2DluXDUzp3sUU6CcmYH6wJEuvvPpKybcxqAC9YoPtXdTFCWZJuxuX4jtb57i6V3vJTOeZhMIlsBsMjLC7bQxvaEyp3OKiWwM1BOdQVYF6hUfaiehKGla2zrZdPXFPL3rvZx02PnEOlx1+fQxmzzHQzVVCkFwpTDGckMJCUVJ0+0L8L//q40975xPtmWjw84lAruuysGKhRcl9fOFnNDH00hbCkFwpTDGckMJCUVJE4vB6ykCIhUzHXYuEdhL5zeyZf0CokNDbNveVtAJfbyNtKUQBFcKYywnMgoJIcRM4HFgOjAE/ERK+QMhRD3wJDAbOAHcLqX0CiEswA+AW4AB4JNSyn3ate4Bvq5d+jtSyse09hXAo0AF8CzweSllzOgeo35qxaTCyHxhFAiWKDL07vvdwAUjuZGx/OCxboLhKL9+6VhBJ/SJMNKWQhBcKYyxnMhm+RMB/lpKeQmwGvicEOJS4G+BF6SU84EXtM8ANwPztZ/PAA8DaBP+g8AVwCrgQSFEnXbOw1rfxHk3ae1G91CUITYr1OWYAbbCZU8asxM7gS/+6BV2a3Ul4IJgMDKWd/UFONfdbzqhZzKW6xnVJ9JIO9EG6mwohTGWAxl3ElLKs8BZ7XefEOJdoBm4FbhO6/YY8CLwZa39cSllDNgjhKgVQjRpfZ+XUnYDCCGeB24SQrwI1EgpX9PaHwc2Ab83uUdBcADhQl1MMeasXd7CNYubsi5j6nHb+daje5OqIY/bwcl2f173/sPrJ3NOXQHmNgdlpFWUAjnZJIQQs4FlwOvARZoAQUp5VggxTevWDKQm2z+ltZm1n9Jpx+QehtTVebDbs1t5WGxAaRYGKyvsVgu3rLmYezdeRjg6xLS6Ctq9gyP6Wa1ALF4HwuO2c+KsL3kskT02X46f6WVqXQUdOvedWlvB3NkNuJ0j/5weefptXRWVp8LJpzddzpolzTyz6/iI89YsmUHLjNq8xtrYWJ3XeROFGu/YMtrxZi0khBBVwK+BL0gp+4QQRl314lZjebTnhdc7kHXfCieERv7NK4qMyFCM65fNoLs7noZj8dwGXe+Xa5fMYP3KmTz3xvu8cjD76nXZ0NUb4MrLpusKicVzG/D1DuJLaw+Go+w+oG8Y333gDDevmsnGK2cxMBgaYaTdeOUsOjrSr5iZxsbqvM6bKNR4x5ZcxmskTLISEkIIB3EB8W9Syt9ozeeFEE3aCr8JaNfaTwGpYaktwBmt/bq09he19had/mb3KAh9SkCUDI/9/jB//ieLsFmtpt4vT+44yssHCisgIK7+uXPDAirc9qy9brLNrqqMtIpiJhvvJgvwM+BdKeU/phx6BrgHeEj797cp7fcLIZ4gbqTu1Sb554C/SzFW3wB8RUrZLYTwCSFWE1dj3Q38MMM9CoIqOlQ67DvSyZM7jrJl/YIR3i8VLjuDwQg9viBvHc4tijpbli2Yisdlz2lCVzaH7CnGPFeKONnsJNYAdwFvCyH2a21fJT5x/0oIcR/wAfCn2rFnibu/HiXuAvspAE0YfBtIWB2/lTBiA3/OBRfY32s/mNxDUYaku4XabRae23uSVtlBT39oTMrR2qwW1i5vHrZbyDYnUzaBYeWe8TQaLXzsiaKwWGKxybWe7ujwZf1A9z60YyyHoigwFgt8cXO8spvdZuFbj76Zt7dSttRVOfm7z16Z9+r2ghDQz1a7bXubrhBZv7Ilr9iLUtOZP737hK7hPt/nH2tK7f3maJPQzYOsIq4VJYMF+N4T+2moceF22TmtU08iE26njUq3Ha8viNNh0y0OlEpvf2iYe2uuahGb1cpt187lmsVNYLHQWFuRPK/cM54Gw1H2HDqre6wcnr9UUEJCUTIkgt3iOv783FlD4Shf/cRynA4bVR4H//HycV55+yzB0JBu/4TtIB+1UKZz8ikbOpno9Qfp6NH3HimH5y8VlNJPUZRccek06qtdWCxkVQwoW+qq3TTWeZhW58HjcvBnGwTf/8urWX2pfgiOx23HbrMkcyx19QWJcSHe4ckdRw3vlemchGHbaJyT3bA9pcpFY22F7rFyeP5SQQkJRZESY8m8BqoqHFkVA8oWvUyiLoeN+z56KTOnVY3of7Ldz7bn23JOyZFJlRQMR5OG7WzHOdlwOWysXtSke6wcnr9UUOomRVGyr62DcKSw13TarQzFYkSHhkaoiCLRGAMB/SQtb8lOfAMh3WNGapFsVUnlnvH03o2X6QYTlsvzlwJKSCiKkmwEREtjJYPBKN19Aao9TvoMJvIEocgQO946TSwGN3545jDjs9mk3jcQwmLgX2ukFskUI5FIPDilylXWwXQ2m8r4WuwoIaEoOSyWeB0J/0CYxfMbuPHDs3DarXz5X17LSjX1Uutpdu47TUOWyfYgfj89jNQiZjES6YkHE2MoZyNtsdUDV1xA2SQUJUdiwu7pD/Hy/rP8y2/fIRQZytp2keollTAkuxw2ls6fmvFcqyUupBpq3Kxf2WKqFtm8bh7rV7bQUOPGqp0zc1oVJ9v9ORnAFYqJRO0kFCXPyXY/T+96j/pqZ7JGRC4kfPKzkTFDMfjSHfGAvkxqEb30Id96VD/NuYoLUBQraiehmBTsPdxOfyA/S3eiqNCBI51Z9X9TtmO3Ze+Xm1ClDAYjE1ZkSKHIFyUkFJOGYDgeEOd22jT1jouZ06qyircwKyqUzs7WM3mph8o9LkJRmih1k2LS4XHZ+dIdS3HYrTRqxtBef5BnX3+fl/frp4GQ73uprXbh9WUnKPJRD2WT8E+hKDaUkFBMOrp9QX74m7fp9YeGeQ+tXdZiKCR6+kM47dlvrLt9ATp6BmlprMopn1O5x0UoSg8lJBSTkh5/3ICd8B6SH/TQP5g5jiJbYjH4pydbqfK4GAiEs87nlGrM7ugZhFiMxjqPSoutKFqUkFCUBWORUtzrD+P1X4jSTggkwDTNdXRoiF+/dKzgNRSC4ShnO/uJaik/FIpCoISEYtLgtFtz2g2MFZnsFYnEfwmyFS5GDMs26wtSX60K9ygKR1l/g9xqsTVpcDoKJyDcztF9MczcWbNJ/Jcrw7LNxlSAnqKwlLWQsCkhMWkoZDrxqy6fHo+UnuLO63wzd9ZsEv/lwlgIHYUilbIWEqHcg3MVRUhTvYeAQdGgXHA7baxf2cKd189ny/oF/OB/XkdtlTPn65i5sxY6VqLQQkehSKeshYR+YmjFeDKl0kFtZe4TcSrBcASXI7+vsgWor3Zx1aLp/MPnruK2a+fS1RsgGI4ypcrFyoX6xYjQzps5rYqGGlcyN1OmfE6FriGhAvQUY40yXCsmlN7+sGl8QktjJWc6+02T9+WTrynBPTcJli1oxOO2jyg1euXiGUSGhnA7rcmdittpY/Vl09iwchb1NW5cDlvOda+NYiU2XT0nmT48W2GhAvSMyfX/RaGPJWaUA7lE6ejwZf1A9z20I6ukborxx+20cdXl07nz+vn84r+kYRAcxO0Ro6le11DjwuN2ZO0mu35lS15eSOkkJrEqj4Ond72Xt0vsBe+mkQF6xe7d1NhYTUeHr6DXzKceebaMxXjHklzG29hYrWvZK+udhBIQxYkF+LvPXEGFy0FXb4Db187nvTM+w0l8tOVNu/qChnUk9ChUxtZE4r9t29tG5RKbGqBnczqIhsJlvXIutItxuVPWQkJRnMSAv9/WSiQyRHdfkNoqF4vnN3DxjCr2vNNOSEvkZ7NasFstBMc5NsKoZGk+ZPJOykUYuRw2GqdWltRKt9AU8n0q4ighoShKzncPJn/3+oO81HqGpnoPD332SvwDIZ7d8z57/thO1GQbsXbZDNYua072LRSFNAhnWwtbkR3qfRYeJSQUJcPZ7gG+8q+vctWiJo6c6jXs15Cmg77vo5dS5XHS2tZJty+AhdGpqAppEM5UC1t5J+WGep+Fp7itWgpFGsFwjJ2tZ0xtCA6blY9fNydppEzo7L/z6Sv4fz6zmmuXztA9L+HOaoTVAmuXN7N53TyC4Sjt3oFRB6sV2iW23FHvs/ConYRi0nHOO8h3H9/HN+9dNaw9YSjesmEBNptV1xvobGc/D2zVLzEai8H6FS0F95xR6cMLi3qfhUUJCcWk5HSHH99AiGrPyEC9SDTG+hUtbLxqNoPByDA/+vopbtxOG4HQyB1CfY2b7W+eZGfrmWRbITxn0mthK7/+0aHeZ2EpayHhQEVdT1aGYvD+eR+LLm5Itpn5zyd4etd7ugICYH5LDQePdekeK4TnTGKnoygM6n0WhoxCQgixFfgo0C6lXKS1fQP4NJDwNfuqlPJZ7dhXgPuAKPA/pJTPae03AT8AbMBPpZQPae0XA08A9cA+4C4pZUgI4QIeB1YAXcBmKeWJAjxzkkghL6YYEyyWuJonH948fH6YkMjkP2/mPgmYekgpz5nSQkVjZ082StRHgZt02v9JSrlU+0kIiEuBO4DLtHN+LISwCSFswI+Am4FLgTu1vgB/r11rPuAlLmDQ/vVKKecB/6T1KyhlvY0qQird9qThOJHVtabChiXPDK/vvNeTNCxnky3VzH0yE8pzpjSIDg2xbXsbX39kD1/51z18/ZE9bNveRnRo4uuQFCsZhYSU8mWgO8vr3Qo8IaUMSinfA44Cq7Sfo1LK41LKEPGdw61CCAuwDnhKO/8xYFPKtR7Tfn8KuF7rXzDUTqK4sFnjnklwwUW1dyCa904iNQtqJv/57r4Az73xQd4CKd1zplDeT4rCMqz2Bqr2RjaMZjF9vxDibuBN4K+llF6gGdiT0ueU1gZwMq39CqAB6JFSRnT6NyfOkVJGhBC9Wv9Os0HV1Xmw27PbPqq0HMVF30CEvoHCie6ptRXMnd2A22mnekoFjXUVtHsHdfvtfuf8MIN0Nlgs0FhbwepFTdy78TJsNivR6BBbf/cOew6dpaNncMTx8aCxsXpc7lMoxmu8gVDE0KZ08FgXn72tArcz85RYbu83XyHxMPBt4vPst4H/BdxLPO1OOjH0dywxk/5kOGaI1zuQqYuiTFg8twFf7yC+lM962VIvu7ie1w8ZJxDUo77axRduX0JjbQUuh43u7n6AEXmY2r2DPLPrOAODoXHJGzSZE9CNlnbvAB06iwSAzp5Bjp3oymhTmszv10iY5LW0kVKel1JGpZRDwCPE1UkQ3wnMTOnaApwxae8EaoUQ9rT2YdfSjk8he7WXYpJRX+1i7fJmGmrcWCzmlehcDivXr2ge4Re/ed28eMW5Gvew+g/rV7TkbItYLhppaawaoWIytnt0KNXTBKNqb+RHXjsJIUSTlDKx9PoT4JD2+zPANiHEPwIzgPnAG8R3BfM1T6bTxI3bW6SUMSHETuDjxO0U9wC/TbnWPcBr2vEdUkqlISpTlsyfyo0fnsmmj1zMe2f7+P6/HzTsGwwPYbFYsFmtI7xYbrt2Lqsvuwh/f4iLZ0yh2uMkGI4apnKwAM2NlQwGI3hy5qqfAAAgAElEQVR9QdPALDO7R1dfkF88J/nULQuLPn33ZCWf2hvKCyo7F9hfAtcBU4UQp4AHgeuEEEuJq39OAJ8FkFK+I4T4FfBH4nbhz0kpo9p17geeI+4Cu1VK+Y52iy8DTwghvgO0Aj/T2n8G/EIIcZT4DuKOUT+touRI1Ho4cKSDF/edxuW0EYtl9kRpbesgGh3i4LGuZExEhdtOh3eQoJZF1mW3svry6XxiwwLDySMGnOroZ+2yGdy4apbpZGGWNwjg1UPn8LjtKl31BJJtNLZRTM39ty+biGFPKGVddOjeh3aM5VAUWeK0WwnppPu+atF0XA5rzgblXJk5rYqv3b2cJ3cc46XW07rJ/xpq3Hzn01dkXE2m2yTyvU6+TGadeSHJtEMw+n/82NVz2LRm9jiMsDAUouiQ2vcqJhzRMoWrFk2nvnp4regtGxYYeqMUkpPtfp584Sg3fnimYXbY7hR3WjM2r5vHmkXTDY97s7yOYmxJRGMbqZiMbEt7Dp0tO9uSiidTTDinOv383W2LAYat7tq9A3kHt+XKm7KDVZdMo8HENvHcGx/EkwOa2BRsViufuFHw7vvdurW3S9VAWk66eTPbUmfPYNlF1ishoZhwvP5w0qib+seXScdfSHwDYf5+236MQhmGYrCz9Qw2m9XUppCYTJfMb2TnvtMjjk9Uuup8J/mxrBddrJh976bWVpSkkB8NSkgoioJUo27qhGZkUB4rohls4kaJ/NIn07pqJzOnVTEQCGf0ihpLRjvJl2O9aDMvqNWLmib9TiodJSQURYOeR1IgNDwC28LERsobJfJLn0y7fSG6faGsvKLGktFM8uVcL9rIC+rejZclAyfLBSUkFEVDV19wRK2GdGJApdtGfyA342GhhEtdtYtQOEowHE1OkGaT6cFj3dy+bv6EqZhymeTTVVLlXC/aqCbFeKVWKSaUkFAUDdmmBR8IRFm1cBpvHDZO3Z1OrgLC5bAm4ylS8Q+EeXDr3mFqm2KdTLMdl5FKatPVF5d9vWhVk0K5wCqKiGxDdmKgucyOrDqnh9UKdVn2TRDW4jYS6T9s2i/ByNCI7KHFmu4h23EZZUZ9etd7ql60QgkJRfHgcWX/ddwrzxtOYOkMDYHbkdumOREvkfg3ahBA0doWT0qcaTKdiNThCQNspnGZqaQ2XT1HN9+VqhddPih1k6JoGAhmX/hl99vnWbeimfUrW2ht66TbFwCMdyOBUIS1y5t57dA5w/Kk+ZAIsjMydH78ujls2942YS6kmdJQdPQMmqqk/AMhVS+6zFFCQlGyHDjSxTfv+zDR6BCtRzrp8Y8MXkvg9Ye4ZkkTB450FFRIwIUgO73JND29QybvokIHrRkZYBMV2vbJdkN7TapKSunmyxclJBQlS1dfgH97vo3XDp3Pqv+/Pv0OXp0o6NEQSwuyS51Mc/EuGuugtfRJPt01Vg9ld1BAmdskCloLVTEhvP5OdgIC4Jx3EKdjbL7yrW2d+AZCw+wO2XgXJRjPspoDwTCvHDQustRQ41J2B0WSst5JTK78t+WJUUI+I4xsFvHo6AheXwBnIv4hFK8z4R8MEQyb36irL8CDW9+g1x/K2YXUbNIei6C1bc8fMVS5WYDPf3wxLdNKq0SnYuwoayGhKD9CkSHWLJrO4Q96RhhyI9FYUm8PYHM6iIbC/PqlY1mlBknYRBK7gEh0CKdBvfVUVY7ZpF3oOItgOMrh940LPNbXuGhUtgdFCmUtJBpqrHT1Ze9Royh9GmrcfOJGATDCQGyzMmwybpxaSUeHb5iHUFdfIOt7vWhQB2PmtKrkNTNN2nXVLipcdtq9A8PGmq+Bu9cfNLXLLJxVp+wQimGUtZCwWJRVotxYOKsWyM1bJ9VDqLsvwPY3T3LwWDdeX4AplS68OdaHGAhEiERj2KyZJ22n3ca3Ht2bNGYvnT+VGHDgSGdeBm6zDKdup407N0zOpH2K/ClrIdHZW17FQ8oRp91COBLD5bQBMXYfOsfhD7wjJtZsVuYuh42mhkruunFhsn+Fy863Ht2bUzrzVBXSlCoXToMUIABnuweSv3f1BXnhreHpx3PNymqW4fQji5vwuMp6SlDooL4RiknNlZdPJxyO8eqhc8m21Il187p5hq6nZiR2IsFwlIWz6tidcv1MJIzW0aEhfrXjiKGAyIWEgTsbsq3zrFCAEhKKSc6hY90Y1XFvbeskGh0akXk2IUA+f+cKw+umxjV09QVxO62AhWA4igVzr6uE0Xrb9raC1e9O7E5asuhrFGCnUOihhISi5PC4bAwEs1MVdmtxB/rHArQe6dQ91trWOaKWRSrpwWiBUHw3cNWi6bgcVt3J3+208ZHFTWxeN8800C6B1ZK9i29dtSvnRIIqijo7UlWR5YgSEoqSweWwYrGQtYAAcDqsVLrtuvWmY2CYysPrC+DtCxLVsVWYTfDygx6+ed8qbDZrUp1TW+Vi4Yfq2LJhPh6XA4Cu3sz1u5sbqzjZ7s/qOZVXUuHRi4Jfs6SZjVfOmrSlW/VQQkJR9FiAlZdMZe+7+qt+03MtFsN602bUVbt4+qWjvH7o7AhbRaZI6myS4pl5GVktcO2yZjavm8uTO46xv62Tnv6goRpLeSXlTjaOCnpV/Z7ZdZyBwdCkLd2qhxISiqIn7vLZpXvMabewZcMCHv291D0eDEVZv6IFm9WStB9kg8ft4NlXTyQ/p9oqbrt2blaR1GbqHDMvo2uXzmDL+vk8ueMoB4924vUHqa1yUlnh4HTHyNKZyispe7LNkVXOpVvTUd8sxbjhcloJhvLz5AlF9JXzoUiMlmlVNBhM2vU1bupr3GxZv4BrFjfxwNa9hvewWKC+2s3ieQ0cOGI0QcTrcPcHwrrHMyXFS13BmnkZpa9ie/whevyhYelDlFdS7mRb87tYqw1OBEpIKMaNfAVEJpw2q+GqfPG8huSk3VjnMRYm1S6+cPsSGmsr6PUHedFAPZVehztBqlFaD7MVbLpaymwVOxCI8MAnVzIYjCivpBzJZXdgpg4sl9KtCcrH+qKYlLidNhrrPGxeN0+roBb/402UHT1wpINt29uIDg2ZVmpbLhppaazC5bCZlv00wuOyc9u1cw0NmmZZXhNqqcQElWkVOxiMDOuvyI5csvJmU9WvXFBCQlHSXHX5dFwOW9L3f/HcBuCCgbfbF0pOxtGhIYZiMS2mIY7baeP6Fc3DdgBmE4QRPf7gsEkmlUwr2PSSpsVaM7vUyfW9Xlh4XCjd+rGr55Sdek+pmxQli9tp479dcyHKOBiOcvCYvoE7YUtIVxUFQlEsFsuIHcDmdfNwOu384bUTWcUqmE3eueq3zYza5baKLSS5vle9oMOWGbV0dPjGa8hFgdpJKEqWUDiKf+BCnIPZZNzVFzRMnaG3mrdZrWy6dl7WwWxmk3c+OwO9VawqBDR68nmv6erAckPtJBQlS3qUsZmxESBkkCPJyFulrsZlaOi2WuKuufVZeBjlszNQqTPGBvVecyejkBBCbAU+CrRLKRdpbfXAk8Bs4ARwu5TSK4SwAD8AbgEGgE9KKfdp59wDfF277HeklI9p7SuAR4EK4Fng81LKmNE9Rv3EiklDjz/Iv794lDuvn59UF+WabA+MV/Nup900luHGVbOynmTyTaqnUmeMDeq9Zk82O4lHgX8GHk9p+1vgBSnlQ0KIv9U+fxm4GZiv/VwBPAxcoU34DwIriS/A3hJCPKNN+g8DnwH2EBcSNwG/N7lHwXBZIKhqmBY9Tnt8Ij/vHV7wJzoEO946zdBQDLvNmgyWc9qthCLZu9smakykEwhFWLusmXBkiINHu+jpDw7bOeSSmkGtYBWlSkYhIaV8WQgxO635VuA67ffHgBeJT+C3Ao9LKWPAHiFErRCiSev7vJSyG0AI8TxwkxDiRaBGSvma1v44sIm4kDC6R8FQ8qE0cNjthKPG/1vpFeDMBITbaaPSbcfrC2q1rEfWmIC4y+rBY120eweTifamVDpYPLc+ZwGRilrBKkqNfG0SF0kpzwJIKc8KIaZp7c3AyZR+p7Q2s/ZTOu1m9zClrs6D3aCucDrZGiUVE0t/IEJ/wDgjay7ccMWHuOuWS/iXXx/khTcvfCUTcQueCifAMBVT4nvS2x9mZ+sZqqvcfHrT5QUZz1jQ2Fg90UPICTXesWW04y204VqvHmgsj/a88XoHMnfSKMy0oxgPLOT3xXA5rIQjQ0kbwMYrZ9HZ6Wd/W7tu/5f3ncJiNS9ru/vAGW5eNbMo1UWNjdUl5aKpxju25DJeI2GSr5A4L4Ro0lb4TUDiL+4UMDOlXwtwRmu/Lq39Ra29Rae/2T0KhhUYm0QRikKT78qhwmnja3evpLG2Ijmpm6Xp7uk3rjedoNxy9yjKm3zjJJ4B7tF+vwf4bUr73UIIixBiNdCrqYyeA24QQtQJIeqAG4DntGM+IcRqzTPq7rRr6d2jYLiVA3DJYAGap3pwOXL7yvb0h3HarcNW/fmk3UhFRT0ryomMf3FCiF8Cr8V/FaeEEPcBDwEbhBBHgA3aZ4h7Jx0HjgKPAH8BoBmsvw3s1X6+lTBiA38O/FQ75xhxozUm9ygYQ2obUTLEgNOdA1y1aDofviT7lBm1lc4RE3o+aTdSmcxRz8FwlHbvwIjgQkX5YjGq/1uqdHT4sn6gex/aMZZDUYwBDTVu7r/tcr75c+OU36msXd7MXTeIEe2JrKxvHe7Aa5BzKUHCu6m+2sVyMbL2QDFhpoM2K7STbZ2F8RxvMTKZx9vYWK1rjFMKF0VJ4fUFsFkwjIROpXmqhy3r5+seS8QtbLxqNl/68W7dehVup42v3rWCKZVOw9Tc2VQ4m2iyEQDZ1llQlB9KSChKirpqF411HsNI6ATNjZV841MfNl0FR4eGeHrXccOCRhYLSYN3tcc54tyJWHnnwxMvHOGFty7Ux0gIgFgsxp9tEKoKm8KU4vo2K8qCDB6mplS47LgcNjavm0dLY6Vun2wEBMRXz3oFhBIEglHD9N9m9SGKiWA4yu639dOU7H77XHInlG2dBUX5oYSEYtxZvjB/o/Hpjn5+/vs/8r+fl5zpHFnvGeKTe8QkQhvMazwkaKyr0PViyrU+RPq542kY7ugZJBDSv1cgFKWjZ1DVr1CYotRNinHnzXfNJ2czYsCuA+YJ/Lr6AnT3BWhqGLnTSKycQ5Ehw9VzgtWLmnTVLPnUP54w9VQmx5RYTNWvUJiihISiqHDaLYY2glz43e4T3HPzwuQElz5J11U7cTltuqtsqwWuXjKDezdeRnf3yN1KPvWPMxmGx8oA3ljnwe20EtCpL54o/Qr5Z6lVTH6UkFAUDRbgK3et4OUDZ9m573TG/mbs+eN52k56WS6msXndvBGTdLfPOLJ6KAaHjnex9XfvsPHKWcmVfupEnsvK21w9Fa+Yd/BY15jsMFwOG1dd3sSOt0a+z0TpV1BZahXGKCGhKBrqa9xMr6/kxg/PHLWQgAv1rSPRKG8f69btk8gKm74r6OoL8syu4wwMhpJCJlVVtGT+VK5f0cz+I10ZV96ZKualGs/HwvX0zuvnY7VY2Cc78PqC1KXEe6SjstQq0lFCQlE0eNx27DYLU6qMK8Llw6uHzhM2qEoXCkf54p1L+edfv02Pf+TuorWtc0Rt7K6+IDveOs2aRdN54JMrDWMoEpippxKBenr3LZTrqdolKEaD8m5SFA0n2/08uePoqNNmpBMKDzGlyql7rK7ajdNmpVdHQAB0+wK0HunUPbb70Dm++fM3+N3uE0RN8s6bPY/RaWPhelrutZoV+aGEhKKoeOtwB76BuIpn7fLmUcVUpGJUfW7Zgqk01nkMXUBrK126O4wE3b4Quw+d44s/eoVt29uIGiQE27xuHutXttBQ48ZqiacXWbu8mfpqY+GVagBXOZUUE4VSNymKCq8/yINb32DlwmmsX9FSENuE22njEzcupMrj1PXesVmthobopQumcvBoZ0bVVyA0ZGpLMFL52KwWUwN4KUV2KyYnZS0kqpxgskhUTBA9/rjBORSOGOrsc2HqFDcuh7leXs8FdM2SGZp3k/5ErkcmW0K6YTj1vt2+ALWVLpamGMBVTiXFRFPWQmJQCYii5uCx7oKUmD3V0c+TO46yZf0CQ+8dvZV+y4xaOjp8KRN5R8YdRa4FiWxWK5vXzSMaHaL1SCdef5CDRzuxWS1suvriguVUKoVEhIripKyFhNLuFjdmtoBcyXZS1RMiqQLkF89JXj1kHPGdTxqL9BxSid3CQCCSc2R3OkpdpRgt6luimBRkqlhXCG8hl8PGp25ZyPqVLbid+sIm1zQWZoF2h9/3jjqnUqkkIlQUL2UtJMr64ScRFmClmGbaR29SzcdjKLGr+IfPXcVVi6ZTX+1KeiutX9mScxoLs0C7Hn+QhbPqdI9lI4xGk4hQoUhQ1uompxUCqoRpyeNy2rj9+nlUuO28cvCsbj6mxfMaDPM4ZaOCSdfp26xWPrZmNpvXzcsYTGeGWaCd03HhufLJqZRPIkKFIp2yFhJKQEwOAqEoT+96j7tuEGy6+mK2PX+Ew+930+0LJb2jDhzpwGa1sHndPLZtPzLMtdbMY0gvMWBlhZOBQHiEgMkHswysgVCU3+0+kXe0dD6JCBWKdJTGRTEpeKn1NL947jAuh43//tFLWTJvKnDBfTaRx+kbP9/LS636sRd6Kph0nX63L8TJdn9Bdfybrp6D26n/p5gYUz7R0maR3ioFuCJblJBQTAqGYrCz9QxP7jhKMBzl4LEu3X6nO/oN3Wq704zbgVAkY2GiBKPR8fsHQgR1UnnD6A3uepHe+dhOFOVLWaubFJOPtw53cOWlF2UsKKSHBXjujQ/YsmEBNqsVb5+xTj+d0ej4x1ItpJL7KUaL2kkoJhVef5D/79cHcRm4qJqR2I1s234EgLoa47Ke6dRVu/KezMdDLaSS+ynyRQkJxaSjtz9sWNc5GxL2DYfNmnU2Wo/bMaoJWKmFFMWKUjcpJi2JgkLxQjtuPG47J9v9Gc9L7Ciqq9wjcisRi9fZTqd/MJw0MOfDeKuFVJoORbYoIaGYtATDUb76ieU4HTamVLmw2yyaO2t8wrdgnjxwz6Gz3LxqZnLyPn66l+89sV+3b48/mLRJjGYCHuvKcCpNhyJXlJBQTFpqK100anr4YDhKV2+A266dy23XzqWjZ5BnX3ufPX88b3h+Z89gcuJ3OWzMaZ5iWDGvrtpNlcfBtu1tRT0Bq6yyilxRQkIxaVm6YCp2m2XExO1xO+gfDNHtC+FyWAkalDadWlsxzBhtFvi2bMFUnt71XlFPwJnSdBSqXKpiclEcyxuFosDMnFbFlvXzdRPcnWz30+2LZ5g1EhAAqxc1jZg0jQzMm66eU/R5krJJ06FQpKN2EopJxZQqJ8vnT2XLhgVEorGsg+HSjdzLFkzl3o2X0d3dP6yfkYG53TtQ9HmSVJoORT6MSkgIIU4APuKlGSJSypVCiHrgSWA2cAK4XUrpFUJYgB8AtwADwCellPu069wDfF277HeklI9p7SuAR4EK4Fng81LKApShiWMHIoW6mKIo+MKfLuZDF9UA0NU7kLFIUIJQmpHb5bBhsxlvtNMNzKUwAWdSlylVk0KPQqib1kopl0opV2qf/xZ4QUo5H3hB+wxwMzBf+/kM8DCAJlQeBK4AVgEPCiES+ZEf1vomzrupAONNogTE5OPl/ReK91S47Fgt2Z1XV+2msc6Td8BZqeRJUvEYilwZC3XTrcB12u+PAS8CX9baH9d2AnuEELVCiCat7/NSym4AIcTzwE1CiBeBGinla1r748Am4PdjMGZFkWJBPy7BiIPHupPxCoPBSNblTwsxkevVyc42rfd4odJ0KHJltEIiBvyXECIG/KuU8ifARVLKswBSyrNCJKvBNAMnU849pbWZtZ/SaTelrs6D3a6+9JOB61fO5FMbL8PrCxAIRvjHX+7jbOeA6TleXwCb00Hj1Eqqp1TQWOumoycwop9F22E01lawelET9268DJvNSiAUwdsXpE5Lx9HYWJ3TmD9/54ph13A7x9/sl+2YW8Z4HNmS6zueaMptvKP9Bq+RUp7RBMHzQojDJn31Nv6xPNpN8XrNJxFFadBU7+Fja2bz6O8O0drWQVdfUPcLkU5dtYtoKExHhw+AJfOm6urgnXYLwXCMSCTKwGCIc+29PPXi8WGusmuWNLPxyll5xThEw1GOnfCP+0q9sbE6+eylQD7jncho8cn8fo2EyaiEhJTyjPZvuxDiP4jbFM4LIZq0XUQT0K51PwXMTDm9BTijtV+X1v6i1t6i019RBpztHuBvfvwKwfCFdUE2mqMPTb/wRQ+Go6xd1kx0KMbBo114fQGcDhuBUDR53USdCflBz7CUHV19QZ7ZdZyBwZBujIPRRKUimscO9W4nhryFhBCiErBKKX3a7zcA3wKeAe4BHtL+/a12yjPA/UKIJ4gbqXs1QfIc8HcpxuobgK9IKbuFED4hxGrgdeBu4If5jldReqQKiGzZ19bJ137y2ojqcYvnNnDN0hn88KmDusn/Tnfo53RKDzLLNFGpiOaxQ73biWE04vci4BUhxAHgDeA/pZR/IC4cNgghjgAbtM8Qd2E9DhwFHgH+AkAzWH8b2Kv9fCthxAb+HPipds4xlNFakQV61eN2tp7h+b2n8GpBdOkYGbjTg8z0gvMSlekyRTQXQ0BdqaLe7cSR905CSnkcWKLT3gVcr9MeAz5ncK2twFad9jeBRfmOUaFI5fD7XmqrXXh92UcWp8Y4ZJqorlkywzCgrtsXoKNnkJbGqtwHrsgqWnyigxUnK0qRpygbun1B+gfDOZ2T6hqbaaIiFjMsUhSLwfd/tZ9t29uIDhmnAlHokwhW1KNYghUnK0pIKMqKUGT4BO1yWHE79f8MKlw2Nl09J/k500TVWOcxLVKUMJI/ueNoHiMvb0olWHEyUtZCwp1lNK5i8uJ22giG9Ff2wVAU/8AFG0Y2E9WFiGbjla2eDj0YjtLuHSh73brZe1DR4hNDWSf4CxQsC5SiVOntD1PjcdA3MFINlZ4qHDJHVScimq9Z3MQDW/fq3jNVh67cOuNk8x5UtPjEUNZCQqGwWtAVEKCfKtxsokqNnWis8xgWKHI6bFR5HIBy60yQy3sY6+p9iuEoIaEoa/RcXxtqjFOFJ0idqPRWwQtn1XH53AZebB0Z/xkIRXl613vcdu3cghYBCoajnO3sJzqKWtsTgSqGVNyUtZBQqcIV6TgdVr5293Jqq9ymqcJT0VsF7z50DqcdbFaI6pg8MrnM5uLWOUxI+YLUV5eWykq5txY3xf8NGkPU4mRyYJQOPB+/hFB4iKdePJ5V32A4yql2n+EqOBTRFxCQ2WU2F7fOYQF+seEBfqWAcm8tbspaSCh39clBY22Fbvt1y2bw3U9fwdplM4Z5xKxd3kx9tdPweoff95p6GUWHhti2vY2vP7KHB7buzbqwUSqZXGazcevMJKRKJRJZubcWN2Wtbgoq76ZJwXnvIDOnVTEQiIzwOLJZrdx148IRCflCoSi7D53TvV6PP0ivP2iYSjtdvZQPqS6zkFsNilT1kpmAKiVVTSnU4ihXylpIKCYPA4EwD3zywwwGI7qukekeMXduWMBbbe0EdGIkzFQcZkZWI/TqZ6e7zObi1pmtkColVY1yby1elJBQTAq6fUEGg5GsV80el52PLJ6hO9kunFVreJ6ZkdWIjyxuyjj5ZevWmYuQKkVVjXJvLT7KWkhUuyCHXG+KcaK20klPv362ViPqq13YrBbePdFNy7Qqqj0jbQ7pKqd0FYfTYQNi7D50jsMfeHWLDk2pcuFy2nTTjTvtFpaLabR90EOPPzhC7VWIyS+TkLJYoF6pahQFpKyFhMrKUXw01LhYNKeBl/bnVl8qGI7y5X95jaFY3NupubGKr929HKfdbhrNm1Bx/OI5yaspNgrzokP6xiyr1co9Ny0EGDOVScITSM8WUV/t4pufvRJ7LFZyOwhF8VLW3k2h3BKCKsYBl9NGn39kTepUnPYLX1u300aV245/MJIMjBuKwcl2P999fB8A27Yf0a0BsW37keR15Ade3Xulewj1+oO6dgyI53rq9QeTKpOxmKjNPIGWi0ZmN00pOwGh8l7FGav3UNY7iYBygS06znQOcKbTuE6522mjwmkl5B9iSqWDRXPqee3Qed2+pzv8/Ow/3zE8/lLraYjFWL9yZtbBXFOqXIbpNuprxsdQrDyB4qi8V3HM3kMhKGshoSg9AqFo0h7Q2x9m99v6AgDiO4pMx3dqaTOMVDh11S5C4SjBlFQXC2fV6brPjpehWHkCxVF5r+KYvYfP37li1NdXQkJRMjjt1hH1IArBwWPdLJ43lZ37To841h8I8+DWvdTVuKh0O+gfDNHtC2k1KCyEwtEJW8mXsyeQyvcUJ9N7CIRGn3hICQlFyTAWAgLiKqX1K1qwWS0pnk5WAqGhpP2hu2+4V1Gi/apF07nrRlEWE1IxofI9xcn0Hrx9wVFP8uWjuFOUPEY5mkZLXbWb+ho3W9Yv4DufvoJv3LuKWJbR+PKDnrEZlMIUle8pTqb3UGdS/CpblJBQlAx6ab0LQaot4UJtiOx2LYlVaykwmbyAVL6nOJneg9s5emWRUjcpSoqmBg9nu4y9n3K+Xr1npC0h220EpbFqnaxeQMrLK85YvwclJBQlRVfPYEGvFwxHiURjpJaOaKzz4HZaDeMhUimFVetk9QJSXl5xxvo9lO4yQlGWhKKF1Tl5/UGOn+4dpoJxOWxcdXmT6Xlup431K1uKftU6EAzzysGzusdKJZV4JsYyeLGUGKv3oHYSirLGAnzvif00pKlg7rx+Pm6Xg9+/ekI3CYfHZeOaxU0jdiHFxrbnj+jmmQJzL6D0PFeK8kUJCUVZMHNaFSfb/SPaE8bwdBWMzWpl07XzePbVE7rX6/aFeGDr3hHCpZgIhh4ryvwAAArPSURBVKMcfr/b8HhdtWuEPWWy2i8U+aP+1xWTErfTlqxEt35lC1+7eznrV7bQUOPGYjF2p01VwdTVxFNwmFHMpUJ7/UG8PuNsugtn1Y3YJQwrhUpxP59ifFA7CcWkwO20DYt+3nT1xfgHwsPUJQnj3vHTvXzvif261+n2BejoGaSlsQq3086yBY1ZFfiZqChf30CIU+1+3fToZhlj3U4bd24YbrRWUcwKPZSQUEwKPC47X71rBY21FcmJzONyjOjnctiY0zzFMElfLAbf/9V+lotp3H/7smHuhd2+gKF37HhH+YYiEb77+D5Od/iHpUf//l9dm+yT8KHXE3IfWdyExzX8z19FMSv0KGt1k1oTTR56/MFkCvFMAWNmAUgQtzdsf/MUW3/3TtK98DufvoJv3ruK+uqRxYxg/OMlvvv4Pk62+0ekR//SD3cN67d53bykmi1V/abnlaWimBV6lPVOYowCeBUTQF21i+fe+ICDx7qyMrhe2CF06O4oAPYcOsvNq2bicthwOWy0NFaxXEzTXZmPZ7yEbyDE6Y6RRniAE+f68A2EkqqnXHzozXYepRAPohgbil5ICCFuAn5AfOH/UynlQ4W6tionUTyMNsOr025Lpv2GzAFjicnzmsVNPLB1r+41O3sGR6hYiiHK91TKDiKdoaH48Utm1w9rzzZjbDE8n6K4KGohIYSwAT8CNgCngL1CiGeklH+c2JEpCs2y+Q28/q6+0TQTLqeVgEGZwUwG18Y6j6F9YmptxQgVSzFE+bZMq8Jq0c9lZbXGj+dLMTyforgodpvEKuColPK4lDIEPAHcOsFjUuSJ06Hvd9pQ4+aumy5hZp6T22Wz6+nx6wuJTAn4zOwTqxc1mapmJirKt9rjpLlR/13Nnl4zwsspH1QUsyJBUe8kgGbgZMrnU8AVZifU1Xmw27P7Yl9U7+J8d2lk8Cx1PvqRi7FaLDyz6/iIY2uWzOBDLXX88Itr+cnTb7Pn0Fm8fUGm1rpxO+26QXAJKlw2Pn/Hcr70w120e0fmdZpaW8Hc2Q2m2TDvv30Zngonew6dpbNnkKm1Faxe1MS9Gy/DVqTh1N//q2v50g93ceJcH0ND8R3E7Ok1fO8vr8ZZgMyf40ljY/VEDyEnym28xf5t0lt6mtqbvd7sM4TefcNCvvfEgVzHpMgCl8NCKByjrtrFctHIrVd9iMap1QwMhkbouzdeOYuODh8AH79mDhuv/FBS1WG3WXhyx1FeOXhWN73EmsubiIYiLJ7boGtwXTy3AV/vIL4M4920ZjY3r5o5TMVis1mT4ypGvn73yhFxEk6nvajHnE5jY7Ua7xiSy3iNhEmxC4lTwMyUzy3AGYO+OXPJ7IZCXWrSYaTzTuWiOjcLZ9dx6JiX7r4ANZVOFn6ojrtujKe1SNdp22zZ6bvTjaxb1i9g09UXs+35Ixx+30uPPzjCoFoIg2splgOt9jhHGKkVikJiieWQO3+8EULYgTbgeuA0sBfYIqV8x+icjg5fTg/U0TfIl3/82qjGWexYgKoKK77BC95DNgvMb5nCx9fO4bw3gK8/zIemVzFjahWDwQhVHge/fvEY+9o66O0fru+3WuDqJdP5xA0LsVmtWSeDK8QqLNO9CpmYrtRWjVB6Y1bjHVty3EnoGg2LWkgACCFuAb5P3AV2q5Tyu2b9cxUSCd490cV/7DrOmXYfkSiklxKwMtxl1m0Dmw2CYYhod3Tbwe6AAU01XuUB/0D8PAtgt4GnwobH5WBwIEjP4IWhepwwoKXZcVqhxmMlFLPgslkIhKI47VYqXE5qKuxYbVYuqq+kutrFuU4/NR4HlW4ndnt8wr7ikouon1JBR88gxGI0agbIYDhKh3cALJZhkclmJCZdm9XC6Y5+qj0OZjRW5TUBT+Y/sGKh1Masxju2FEJIFLu6CSnls8CzY32fS2Y3cM2HZ0+qL0BLmgeMy2GjZVpuRqxUFUzDlIrcB6lQKEqa4nTdUCgUCkVRoISEQqFQKAxRQkKhUCgUhighoVAoFApDit67SaFQKBQTh9pJKBQKhcIQJSQUCoVCYYgSEgqFQqEwRAkJhUKhUBiihIRCoVAoDFFCQqFQKBSGKCGhUCgUCkOKPsHfeCGEuAn4AfFssz+VUj40wUNCCLEV+CjQLqVcpLXVA08Cs4ETwO1SSq8QwkJ8/LcAA8AnpZT7xnm8M4HHgenEk9/+REr5g2IdsxDCDbwMuIj/LTwlpXxQCHEx8VK59cA+4C4pZUgI4dKebwXQBWyWUp4Yr/GmjNsGvAmcllJ+tJjHK4Q4AfiAKBCRUq4s1u+DNt5a4KfAIuIFzu4FZDGOVwghtHElmAM8QPz/vGDjVTsJkn90PwJuBi4F7hRCXDqxowLgUeCmtLa/BV6QUs4HXtA+Q3zs87WfzwAPj9MYU4kAfy2lvARYDXxOe4/FOuYgsE5KuQRYCtwkhFgN/D3wT9p4vcB9Wv/7AK+Uch7wT1q/ieDzwLspn4t9vGullEullCu1z8X6fYD4JPoHKeVCYAnx91yU45VxlkoplxJfCAwA/1Ho8SohEWcVcFRKeVxKGSK+Krt1gseElPJloDut+VbgMe33x4BNKe2PSyljUso9QK0Qoml8RhpHSnk2sTKRUvqI/4E1F+uYtfsmCmg7tJ8YsA54ymC8ied4CrheW52NG0KIFuD/Ir7aRbt/0Y7XgKL8PgghaoBrgJ8BSClDUsqeYh1vGtcDx6SU71Pg8SohEacZOJny+ZTWVoxcJKU8C/FJGZimtRfVMwghZgPLgNcp4jELIWxCiP1AO/A8cAzokVJGdMaUHK92vBcY7xq43wf+hgs1sBoo7vHGgP8SQrwlhPiM1las34c5QAfwcyFEqxDip0KIyiIebyp3AL/Ufi/oeJWQiKO3uiq1pFZF8wxCiCrg18AXpJR9Jl0nfMxSyqi2XW8hvqO8xGRMEzpeIUTCPvVWSrPZmCb8/QJrpJTLias6PieEuMak70SP1w4sBx6WUi4D+rmgqtFjoscLgBDCCXwM+PcMXfMarxIScU4BM1M+twBnJmgsmTif2CJq/7Zr7UXxDEIIB3EB8W9Syt9ozUU9ZgBNrfAicVtKrVZfPX1MyfFqx6cwUh04lqwBPqYZg58grmb6fhGPFynlGe3fduL68lUU7/fhFHBKSvm69vkp4kKjWMeb4GZgn5TyvPa5oONVQiLOXmC+EOJiTSrfATwzwWMy4hngHu33e4DfprTfLYSwaMbX3sSWc7zQ9N0/A96VUv5jyqGiHLMQolHzZkEIUQGsJ25H2Ql83GC8ief4OLBDSjluK0cp5VeklC1SytnEv6M7pJR/VqzjFUJUCiGqE78DNwCHKNLvg5TyHHBS8xqCuJ7/j8U63hTu5IKqKTGugo1XucAS19cKIe4HniPuArtVSvnOBA8LIcQvgeuAqUKIU8CDwEPAr4QQ9wEfAH+qdX+WuGvbUeJeDp8a9wHHV7p3AW9ren6Ar1K8Y24CHtO826zAr6SU/0cI8UfgCSHEd4BWNEOm9u8vhBBHia/I7xjn8RrxZYpzvBcB/6HNuXZgm5TyD0KIvRTn9wHgL4F/0xaLx7UxWCnS8QohPMAG4LMpzQX9e1P1JBQKhUJhiFI3KRQKhcIQJSQUCoVCYYgSEgqFQqEwRAkJhUKhUBiihIRCoVAoDFFCQqFQKBSGKCGhUCgUCkP+f7E9X1mAtLJ1AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for i in which:\n",
    "    feature = x[:,i]\n",
    "    if len(feature[feature==-999]) > 0: #If there is some misplaced value we do not include them in the scatterplot\n",
    "        print(\"ATTENTION: missing values ({h}) in {i}th feature removed!\".format(i=i, h = len(feature[feature==-999])))\n",
    "    feature = feature[feature>-999]\n",
    "    plt.scatter(feature, range(len(feature)))\n",
    "    print(\"Scatter plot for {i}th feature :\".format(i=i))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Histograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hist_plot(data,i):\n",
    "    # An \"interface\" to matplotlib.axes.Axes.hist() method\n",
    "    n, bins, patches = plt.hist(x=data, bins=50, color='#0504aa',\n",
    "                                alpha=0.7, rwidth=0.85)\n",
    "    plt.grid(axis='y', alpha=0.75)\n",
    "    plt.xlabel('Value')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.title('{i}th feature'.format(i=i))\n",
    "    maxfreq = n.max()\n",
    "    # Set a clean upper y-axis limit.\n",
    "    plt.ylim(top=np.ceil(maxfreq / 10) * 10 if maxfreq % 10 else maxfreq + 10)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/charles/anaconda3/lib/python3.6/site-packages/matplotlib/axes/_axes.py:6462: UserWarning: The 'normed' kwarg is deprecated, and has been replaced by the 'density' kwarg.\n",
      "  warnings.warn(\"The 'normed' kwarg is deprecated, and has been \"\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for i in which:\n",
    "    feature = x_all[:,i]\n",
    "    feature = feature[feature>-999]\n",
    "#     hist_plot(feature,i)\n",
    "    sns.distplot(feature,bins=30,kde=True)\n",
    "    plt.title('{i}th feature'.format(i=i))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single feature-response plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scatter plot for 1th feature :\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/charles/anaconda3/lib/python3.6/site-packages/numpy/core/fromnumeric.py:2957: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "/home/charles/anaconda3/lib/python3.6/site-packages/numpy/core/_methods.py:80: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for i in which:\n",
    "    print(\"Scatter plot for {i}th feature :\".format(i=i))\n",
    "\n",
    "    t = x[:,i]\n",
    "    plt.scatter(t,y, alpha = 0.5)\n",
    "    mean_x1 = np.mean(t[y == 1])\n",
    "    mean_x0 = np.mean(t[y == -1])\n",
    "    plt.plot(mean_x0,-1,\"or\")\n",
    "    plt.plot(mean_x1,1,\"or\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature boxplots\n",
    "Visually check influence on response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Boxplot for 1th feature :\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for i in which:\n",
    "    feature = x[:,i]\n",
    "    print(\"Boxplot for {i}th feature :\".format(i=i))\n",
    "    plt.xlim(left = np.min(feature[feature>-999])-0.5)\n",
    "    plt.xlim(right = np.max(feature[feature>-999])+.5)\n",
    "    #the mean is dispayed as a triangle\n",
    "    yes_feat = feature[y==1]\n",
    "    no_feat = feature[y==-1]\n",
    "    plt.boxplot(yes_feat[yes_feat>-999],vert = False, positions=[0],showmeans=True)\n",
    "    plt.boxplot(no_feat[no_feat>-999], vert = False, positions=[-.25], showmeans=True)\n",
    "    plt.show()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Features with \"too many\" -999\n",
    "\n",
    "Checked to eventually remove them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feature 0: 15.245600000000001 of Na\n",
      "feature 1: 0.0 of Na\n",
      "feature 2: 0.0 of Na\n",
      "feature 3: 0.0 of Na\n",
      "feature 4: 0.0 of Na\n",
      "feature 5: 0.0 of Na\n",
      "feature 6: 0.0 of Na\n",
      "feature 7: 0.0 of Na\n",
      "feature 8: 0.0 of Na\n",
      "feature 9: 0.0 of Na\n",
      "feature 10: 0.0 of Na\n",
      "feature 11: 0.0 of Na\n",
      "feature 12: 0.0 of Na\n",
      "feature 13: 0.0 of Na\n",
      "feature 14: 0.0 of Na\n",
      "feature 15: 0.0 of Na\n",
      "feature 16: 0.0 of Na\n",
      "feature 17: 0.0 of Na\n",
      "feature 18: 0.0 of Na\n",
      "feature 19: 39.9652 of Na\n",
      "feature 20: 39.9652 of Na\n",
      "feature 21: 39.9652 of Na\n",
      "feature 22: 0.0 of Na\n"
     ]
    }
   ],
   "source": [
    "for i in range(x.shape[1]):\n",
    "    percentage = len(np.where(x[:,i] < -500)[0])/len(x[:,i])*100\n",
    "    print(\"feature {i}: {percentage} of Na\".format(i = i, percentage = percentage))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## explanation for deleting feature 19"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "due to high correlation between features 6,17,22, we decided to delete the feature 19 from our \n",
    "data set since it contained 39% of NANs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deletion feature 19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(250000, 22) (250000,)\n"
     ]
    }
   ],
   "source": [
    "x = np.delete(x_all,[4,5,6,12,19,26,27,28],1)\n",
    "print(x.shape, y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inputation of the data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we know that the features with missing variables are the 1, 19,20,21. Hence we only \"treat\" these ones by different methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "test_data = imputation(x,method = \"mean\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_1 = imputation(x,method = \"median\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Standardization of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = standardize_data(test_data)\n",
    "test_data_1 = standardize_data(test_data_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "f,W,error = least_squares(y,test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1,W1, error1 = least_squares(y,test_data_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.05972960110378267 0.0\n"
     ]
    }
   ],
   "source": [
    "print(max(abs(W-W1)),abs(error-error1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test, x_all_test, ids_test = load_csv_data('../Data/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = np.delete(x_all_test,[4,5,6,12,19,26,27,28],1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(568238, 30)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test = imputation(x_test,method = \"mean\")\n",
    "x_test = standardize_data(x_test)\n",
    "x_all_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = predict_labels(W,x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1., -1., -1., ..., -1., -1., -1.])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_csv_submission(ids_test, y_pred, \"../Data/test_Charles.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test for a new method to predict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a particular interesting feature is the feature PRI_num_jet, the 17 in our case, 22nd feature in the all dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feature 0: 26.15, 9.75, 5.86, 6.66\n",
      "feature 4: 100.0, 100.0, 0.0, 0.0\n",
      "feature 5: 100.0, 100.0, 0.0, 0.0\n",
      "feature 6: 100.0, 100.0, 0.0, 0.0\n",
      "feature 12: 100.0, 100.0, 0.0, 0.0\n",
      "feature 23: 100.0, 0.0, 0.0, 0.0\n",
      "feature 24: 100.0, 0.0, 0.0, 0.0\n",
      "feature 25: 100.0, 0.0, 0.0, 0.0\n",
      "feature 26: 100.0, 100.0, 0.0, 0.0\n",
      "feature 27: 100.0, 100.0, 0.0, 0.0\n",
      "feature 28: 100.0, 100.0, 0.0, 0.0\n"
     ]
    }
   ],
   "source": [
    "x_nj = x[:,17]\n",
    "x_nj_0 = x_all[x_nj == 0]\n",
    "x_nj_1 = x_all[x_nj == 1]\n",
    "x_nj_2 = x_all[x_nj == 2]\n",
    "x_nj_3 = x_all[x_nj == 3]\n",
    "\n",
    "\n",
    "\n",
    "for i in range(30):\n",
    "    per_0 = len(np.where(x_nj_0[:,i] < -500)[0])/len(x_nj_0[:,i])*100\n",
    "    per_1 = len(np.where(x_nj_1[:,i] < -500)[0])/len(x_nj_1[:,i])*100\n",
    "    per_2 = len(np.where(x_nj_2[:,i] < -500)[0])/len(x_nj_2[:,i])*100\n",
    "    per_3 = len(np.where(x_nj_3[:,i] < -500)[0])/len(x_nj_3[:,i])*100\n",
    "    \n",
    "    if per_0 != 0 or per_1 != 0 or per_2 != 0 or per_3 != 0:\n",
    "        print(\"feature {i}: {p}, {p1}, {p2}, {p3}\".format(i = i, p = round(per_0,2), p1 = round(per_1,2),\n",
    "                                                      p2 = round(per_2,2) ,p3 = round(per_3,2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from this, we derive the hypothesis that in order to better predict the classification, one may use different models in function of the value of the PRI_num_jet: if the values are 2 or 3, there are almost no missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.399652 0.310176 0.201516 0.088656\n"
     ]
    }
   ],
   "source": [
    "print(len(x_nj_0)/len(x_nj),len(x_nj_1)/len(x_nj),len(x_nj_2)/len(x_nj),len(x_nj_3)/len(x_nj))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This could at best predict more accurately for at most 30% of our data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# test for the function split_num_jet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "XT = x_all[0:18,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "D0,Y0,D1,Y1,D2,Y2 = split_num_jet(XT,y[0:18])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 7.53472543e-01  1.99161547e+00 -7.30757049e-01 -5.50551306e-16\n",
      " -1.62592055e+00 -5.50551306e-16 -5.20312656e-01  1.31902248e-01]\n"
     ]
    }
   ],
   "source": [
    "print(D0[:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.28384935  0.          0.90202764 -0.8014895   0.29203876 -1.67642624]\n"
     ]
    }
   ],
   "source": [
    "print(D1[:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.68210445  1.13344675 -1.45637331 -0.35917789]\n"
     ]
    }
   ],
   "source": [
    "print(D2[:,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The split function properly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "data0,y0, data1,y1, data2,y2 = split_num_jet(x_all,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "since the matrix has a row of 0 at the end, we don't care for it (no information given) and deleting it allows us to have an invertible matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "data0_test = np.delete(data0,[data0.shape[1]-1],1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "different tests to see what is going wrong"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(72543, 29)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(250000, 30)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_all.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1,w1,error1 = least_squares(y1,data1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "f0,w0,error0 = least_squares(y0,data0_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "f2,w2,error2 = least_squares(y2,data2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_multiple_models(matrix,w0,w1,w2,ids):\n",
    "    \n",
    "\n",
    "    #we split the data depending on the different values of num_jet\n",
    "    data0,id0, data1,id1, data2,id2 = split_num_jet(matrix,ids)\n",
    "\n",
    "    #in the process of cutting the data in different parts with split_num_jet\n",
    "    if w0.shape[0] != data0.shape[1]:\n",
    "        for i in range(data0.shape[1]-w0.shape[0]):\n",
    "            w0 = np.append(w0,[0])\n",
    "    \n",
    "    \n",
    "    #we predict separatly using the different models\n",
    "    y0 = predict_labels(w0,data0)\n",
    "    y1 = predict_labels(w1,data1)\n",
    "    y2 = predict_labels(w2,data2)\n",
    "    \n",
    "    #we merge the prediction to return a single vector of predictions and a vector of ids\n",
    "    y0 = y0.reshape(len(y0),1)\n",
    "    y1 = y1.reshape(len(y1),1)\n",
    "    y2 = y2.reshape(len(y2),1)\n",
    "    id0 = id0.reshape(len(id0),1)\n",
    "    id1 = id1.reshape(len(id1),1)\n",
    "    id2 = id2.reshape(len(id2),1)\n",
    "\n",
    "    y = np.vstack((y0,y1,y2))\n",
    "    ids_new = np.vstack((id0,id1,id2))\n",
    "    \n",
    "    #merge and sort so the indices are in the right order for the ouput\n",
    "    predictions = np.hstack((y,ids_new))\n",
    "    predictions[np.lexsort((predictions[:,0],predictions[:,1]))]\n",
    "    y = predictions[:,0]\n",
    "    ids_new = predictions[:,1]\n",
    "    \n",
    "    return y,ids_new\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y_test,ids_test = predict_multiple_models(x_all_test,w0,w1,w2,ids_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_csv_submission(ids_test, y_test, \"../Data/test_Charles_num_jet.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test with CV "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Regressions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def least_squares_prediction(x_test,y_train,x_train):\n",
    "    f,w,error = least_squares(y_train,x_train)\n",
    "    pred = x_test.dot(w)\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multiple_prediction(x_test,y,x):\n",
    "    \n",
    "    ids = np.zeros(x_test.shape[0])\n",
    "    \n",
    "    d0,y0,d1,y1,d2,y2 = split_num_jet(x,y)\n",
    "    data0_test = np.delete(d0,[d0.shape[1]-1],1)    \n",
    "    \n",
    "    w0,error0 = least_squares(y0,data0_test)\n",
    "    w1,error1 = least_squares(y1,d1)\n",
    "    w2,error2 = least_squares(y2,d2)\n",
    "    \n",
    "    y,ids_new = predict_multiple_models(x_test,w0,w1,w2,ids)\n",
    "    \n",
    "    return y\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_all = imputation(x_all, method = \"median\")\n",
    "x_all = standardize_data(x_all)\n",
    "x_all_int = np.hstack((np.ones((x_all.shape[0],1)),x_all))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<function Regressions.linear_predictor>,\n",
       " array([ 2.90424413e-05, -3.52187698e-03, -3.28558737e-03, -1.75365145e-04,\n",
       "         1.29108496e-02,  2.03426627e-04, -9.93554325e-03,  1.86214895e-01,\n",
       "        -1.16187651e-04, -1.52046026e+00, -1.03678317e-01,  4.81708850e-02,\n",
       "         5.90678367e-03,  1.52501568e+00, -6.87222380e-05, -5.34879288e-04,\n",
       "         1.52700245e+00, -1.89589453e-05,  3.14021654e-04,  1.84750465e-03,\n",
       "        -1.17472459e-05, -2.12660090e-04, -3.92543245e-04,  9.44344202e-04,\n",
       "        -4.56784407e-04, -4.29888911e-04,  1.39347766e-03, -3.94961003e-03,\n",
       "        -6.55558593e-03,  1.51900881e+00]),\n",
       " 0.25513399999999997,\n",
       " 0.255104)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_validation(y,x_all,5,least_squares, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<function Regressions.linear_predictor>,\n",
       " array([ 3.19699913e-01,  3.61774818e-05, -3.57260221e-03, -3.12041220e-03,\n",
       "        -2.37801823e-04, -1.54238370e-03,  2.24734815e-04, -1.19059316e-02,\n",
       "         1.71098530e-01, -5.24712732e-05, -1.41942919e+00, -1.08110767e-01,\n",
       "         4.77593283e-02,  2.26009938e-02,  1.42371273e+00, -1.29912467e-04,\n",
       "        -4.98257275e-04,  1.42594982e+00, -1.78495752e-04,  4.23771251e-04,\n",
       "         1.84729330e-03,  1.08887663e-04, -2.50577350e-04, -1.05815289e-01,\n",
       "        -1.09868728e-04,  1.01683103e-04,  1.71165391e-04, -3.43672690e-05,\n",
       "        -3.28982541e-03, -5.91644646e-03,  1.41905262e+00]),\n",
       " 0.25504800000000005,\n",
       " 0.255212)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_validation(y,x_all_int,5,least_squares)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_all_test = np.delete(x_all,[23,24,25,26,27,28],1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<function Regressions.linear_predictor>,\n",
       " array([ 3.46866498e-05, -3.47231738e-03, -3.14353185e-03,  2.52574977e-05,\n",
       "         6.73433022e-03,  2.27801373e-04, -1.10961813e-02,  1.76754258e-01,\n",
       "        -5.67715700e-04, -1.53578266e+00, -1.06352610e-01,  5.22678852e-02,\n",
       "         4.11569578e-03,  1.54010502e+00, -2.69487077e-04, -2.08418756e-04,\n",
       "         1.54219424e+00, -1.44546509e-04,  7.19945223e-04,  1.76008186e-03,\n",
       "         3.06332106e-04, -2.13598322e-04,  2.68127097e-02,  1.53513227e+00]),\n",
       " 0.255211,\n",
       " 0.255376)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_validation(y,x_all_test,5,least_squares)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After removing the data that was supposed to cause colinearity, the idea of fitting multiple model seems to not be fitted for our problem, hence I wonn't go on with it and I will proceed to code the logistic regression instead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "w0 = np.zeros((x_all_int.shape[1],1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = np.array([[1,3],[2,4]])\n",
    "Y_test = np.array([[0],[1]])\n",
    "W_test = np.array([[1],[2]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "test de la fonction du gradient pour la rgression logistique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_test_gradient = np.ones(x_all_int.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(250000,)\n",
      "(250000, 31)\n",
      "(31,)\n"
     ]
    }
   ],
   "source": [
    "print(y.shape)\n",
    "print(x_all_int.shape)\n",
    "print(w_test_gradient.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[ 17984.76364828],\n",
       "        [  -917.74273089],\n",
       "        [ 27453.86548938],\n",
       "        [ 15744.28018769],\n",
       "        [ 50334.24362012],\n",
       "        [ 76759.2738093 ],\n",
       "        [ 65715.03016376],\n",
       "        [ 76888.13383543],\n",
       "        [-33351.18089429],\n",
       "        [ 35215.05851788],\n",
       "        [ 65691.03242199],\n",
       "        [ 37734.92001697],\n",
       "        [ 23727.90219349],\n",
       "        [ 76795.05683968],\n",
       "        [  2161.08516934],\n",
       "        [ 10023.67567463],\n",
       "        [  5858.46242216],\n",
       "        [ 41105.48875569],\n",
       "        [ 10532.85945436],\n",
       "        [  4434.91810462],\n",
       "        [ 39675.88393172],\n",
       "        [  6743.74590775],\n",
       "        [ 62639.0680865 ],\n",
       "        [ 81940.81482666],\n",
       "        [ 65859.60894707],\n",
       "        [ 64104.65437975],\n",
       "        [ 64079.79862172],\n",
       "        [ 76809.77128374],\n",
       "        [ 76804.41410359],\n",
       "        [ 76805.5636303 ],\n",
       "        [ 67799.67749281]]), 0.18872138012533557)"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_gradient(y,x_all_int,w_test_gradient,loss = \"logistic\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "le gradient est prcis  10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.99908895],\n",
       "       [0.9999546 ]])"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_logitstic(X_test,W_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "test de la prdiction: a marche aussi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.6881171418161356e+43"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.exp(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_all = imputation(x_all,method = \"median\")\n",
    "x_all = standardize_data(x_all)\n",
    "x_all_int = np.hstack((np.ones((x_all.shape[0],1)),x_all))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(0/999):loss = 0.303136\n",
      "Gradient Descent(1/999):loss = 0.267752\n",
      "Gradient Descent(2/999):loss = 0.27942\n",
      "Gradient Descent(3/999):loss = 0.267744\n",
      "Gradient Descent(4/999):loss = 0.270848\n",
      "Gradient Descent(5/999):loss = 0.26718\n",
      "Gradient Descent(6/999):loss = 0.266992\n",
      "Gradient Descent(7/999):loss = 0.265508\n",
      "Gradient Descent(8/999):loss = 0.264968\n",
      "Gradient Descent(9/999):loss = 0.264084\n",
      "Gradient Descent(10/999):loss = 0.263292\n",
      "Gradient Descent(11/999):loss = 0.262512\n",
      "Gradient Descent(12/999):loss = 0.261748\n",
      "Gradient Descent(13/999):loss = 0.261316\n",
      "Gradient Descent(14/999):loss = 0.260564\n",
      "Gradient Descent(15/999):loss = 0.25992\n",
      "Gradient Descent(16/999):loss = 0.25938\n",
      "Gradient Descent(17/999):loss = 0.258908\n",
      "Gradient Descent(18/999):loss = 0.258428\n",
      "Gradient Descent(19/999):loss = 0.258092\n",
      "Gradient Descent(20/999):loss = 0.257608\n",
      "Gradient Descent(21/999):loss = 0.257364\n",
      "Gradient Descent(22/999):loss = 0.25706\n",
      "Gradient Descent(23/999):loss = 0.256816\n",
      "Gradient Descent(24/999):loss = 0.256548\n",
      "Gradient Descent(25/999):loss = 0.2563\n",
      "Gradient Descent(26/999):loss = 0.256136\n",
      "Gradient Descent(27/999):loss = 0.255764\n",
      "Gradient Descent(28/999):loss = 0.25542\n",
      "Gradient Descent(29/999):loss = 0.25518\n",
      "Gradient Descent(30/999):loss = 0.254844\n",
      "Gradient Descent(31/999):loss = 0.25478\n",
      "Gradient Descent(32/999):loss = 0.25458\n",
      "Gradient Descent(33/999):loss = 0.254464\n",
      "Gradient Descent(34/999):loss = 0.254324\n",
      "Gradient Descent(35/999):loss = 0.254152\n",
      "Gradient Descent(36/999):loss = 0.254084\n",
      "Gradient Descent(37/999):loss = 0.254\n",
      "Gradient Descent(38/999):loss = 0.25386\n",
      "Gradient Descent(39/999):loss = 0.253652\n",
      "Gradient Descent(40/999):loss = 0.253516\n",
      "Gradient Descent(41/999):loss = 0.253448\n",
      "Gradient Descent(42/999):loss = 0.253296\n",
      "Gradient Descent(43/999):loss = 0.253184\n",
      "Gradient Descent(44/999):loss = 0.253124\n",
      "Gradient Descent(45/999):loss = 0.253044\n",
      "Gradient Descent(46/999):loss = 0.252916\n",
      "Gradient Descent(47/999):loss = 0.2528\n",
      "Gradient Descent(48/999):loss = 0.25276\n",
      "Gradient Descent(49/999):loss = 0.252708\n",
      "Gradient Descent(50/999):loss = 0.252604\n",
      "Gradient Descent(51/999):loss = 0.252524\n",
      "Gradient Descent(52/999):loss = 0.252368\n",
      "Gradient Descent(53/999):loss = 0.25224\n",
      "Gradient Descent(54/999):loss = 0.25204\n",
      "Gradient Descent(55/999):loss = 0.252\n",
      "Gradient Descent(56/999):loss = 0.25194\n",
      "Gradient Descent(57/999):loss = 0.251772\n",
      "Gradient Descent(58/999):loss = 0.25166\n",
      "Gradient Descent(59/999):loss = 0.251568\n",
      "Gradient Descent(60/999):loss = 0.251472\n",
      "Gradient Descent(61/999):loss = 0.251388\n",
      "Gradient Descent(62/999):loss = 0.251372\n",
      "Gradient Descent(63/999):loss = 0.251356\n",
      "Gradient Descent(64/999):loss = 0.251276\n",
      "Gradient Descent(65/999):loss = 0.251248\n",
      "Gradient Descent(66/999):loss = 0.251136\n",
      "Gradient Descent(67/999):loss = 0.251112\n",
      "Gradient Descent(68/999):loss = 0.251052\n",
      "Gradient Descent(69/999):loss = 0.25102\n",
      "Gradient Descent(70/999):loss = 0.251012\n",
      "Gradient Descent(71/999):loss = 0.251044\n",
      "Gradient Descent(72/999):loss = 0.250936\n",
      "Gradient Descent(73/999):loss = 0.250928\n",
      "Gradient Descent(74/999):loss = 0.250856\n",
      "Gradient Descent(75/999):loss = 0.250824\n",
      "Gradient Descent(76/999):loss = 0.250788\n",
      "Gradient Descent(77/999):loss = 0.250736\n",
      "Gradient Descent(78/999):loss = 0.250632\n",
      "Gradient Descent(79/999):loss = 0.250644\n",
      "Gradient Descent(80/999):loss = 0.2506\n",
      "Gradient Descent(81/999):loss = 0.250504\n",
      "Gradient Descent(82/999):loss = 0.250516\n",
      "Gradient Descent(83/999):loss = 0.250508\n",
      "Gradient Descent(84/999):loss = 0.25052\n",
      "Gradient Descent(85/999):loss = 0.250476\n",
      "Gradient Descent(86/999):loss = 0.250396\n",
      "Gradient Descent(87/999):loss = 0.250356\n",
      "Gradient Descent(88/999):loss = 0.250304\n",
      "Gradient Descent(89/999):loss = 0.250316\n",
      "Gradient Descent(90/999):loss = 0.250284\n",
      "Gradient Descent(91/999):loss = 0.25026\n",
      "Gradient Descent(92/999):loss = 0.250244\n",
      "Gradient Descent(93/999):loss = 0.250276\n",
      "Gradient Descent(94/999):loss = 0.2503\n",
      "Gradient Descent(95/999):loss = 0.250256\n",
      "Gradient Descent(96/999):loss = 0.250244\n",
      "Gradient Descent(97/999):loss = 0.250248\n",
      "Gradient Descent(98/999):loss = 0.25022\n",
      "Gradient Descent(99/999):loss = 0.250248\n",
      "Gradient Descent(100/999):loss = 0.250272\n",
      "Gradient Descent(101/999):loss = 0.250268\n",
      "Gradient Descent(102/999):loss = 0.250228\n",
      "Gradient Descent(103/999):loss = 0.250188\n",
      "Gradient Descent(104/999):loss = 0.25016\n",
      "Gradient Descent(105/999):loss = 0.250148\n",
      "Gradient Descent(106/999):loss = 0.250128\n",
      "Gradient Descent(107/999):loss = 0.250096\n",
      "Gradient Descent(108/999):loss = 0.25008\n",
      "Gradient Descent(109/999):loss = 0.250056\n",
      "Gradient Descent(110/999):loss = 0.250024\n",
      "Gradient Descent(111/999):loss = 0.250008\n",
      "Gradient Descent(112/999):loss = 0.249996\n",
      "Gradient Descent(113/999):loss = 0.249996\n",
      "Gradient Descent(114/999):loss = 0.249976\n",
      "Gradient Descent(115/999):loss = 0.249976\n",
      "Gradient Descent(116/999):loss = 0.249964\n",
      "Gradient Descent(117/999):loss = 0.24994\n",
      "Gradient Descent(118/999):loss = 0.249904\n",
      "Gradient Descent(119/999):loss = 0.249868\n",
      "Gradient Descent(120/999):loss = 0.249884\n",
      "Gradient Descent(121/999):loss = 0.249888\n",
      "Gradient Descent(122/999):loss = 0.249876\n",
      "Gradient Descent(123/999):loss = 0.2499\n",
      "Gradient Descent(124/999):loss = 0.249928\n",
      "Gradient Descent(125/999):loss = 0.249904\n",
      "Gradient Descent(126/999):loss = 0.249884\n",
      "Gradient Descent(127/999):loss = 0.249832\n",
      "Gradient Descent(128/999):loss = 0.249832\n",
      "Gradient Descent(129/999):loss = 0.249812\n",
      "Gradient Descent(130/999):loss = 0.24976\n",
      "Gradient Descent(131/999):loss = 0.2498\n",
      "Gradient Descent(132/999):loss = 0.249796\n",
      "Gradient Descent(133/999):loss = 0.2498\n",
      "Gradient Descent(134/999):loss = 0.24978\n",
      "Gradient Descent(135/999):loss = 0.24976\n",
      "Gradient Descent(136/999):loss = 0.249768\n",
      "Gradient Descent(137/999):loss = 0.249768\n",
      "Gradient Descent(138/999):loss = 0.24974\n",
      "Gradient Descent(139/999):loss = 0.24972\n",
      "Gradient Descent(140/999):loss = 0.249696\n",
      "Gradient Descent(141/999):loss = 0.249672\n",
      "Gradient Descent(142/999):loss = 0.249644\n",
      "Gradient Descent(143/999):loss = 0.24964\n",
      "Gradient Descent(144/999):loss = 0.249584\n",
      "Gradient Descent(145/999):loss = 0.24958\n",
      "Gradient Descent(146/999):loss = 0.24954\n",
      "Gradient Descent(147/999):loss = 0.249532\n",
      "Gradient Descent(148/999):loss = 0.249488\n",
      "Gradient Descent(149/999):loss = 0.249468\n",
      "Gradient Descent(150/999):loss = 0.249468\n",
      "Gradient Descent(151/999):loss = 0.249492\n",
      "Gradient Descent(152/999):loss = 0.249448\n",
      "Gradient Descent(153/999):loss = 0.249448\n",
      "Gradient Descent(154/999):loss = 0.24944\n",
      "Gradient Descent(155/999):loss = 0.249416\n",
      "Gradient Descent(156/999):loss = 0.249392\n",
      "Gradient Descent(157/999):loss = 0.249392\n",
      "Gradient Descent(158/999):loss = 0.249412\n",
      "Gradient Descent(159/999):loss = 0.249372\n",
      "Gradient Descent(160/999):loss = 0.24936\n",
      "Gradient Descent(161/999):loss = 0.249324\n",
      "Gradient Descent(162/999):loss = 0.249272\n",
      "Gradient Descent(163/999):loss = 0.24924\n",
      "Gradient Descent(164/999):loss = 0.249244\n",
      "Gradient Descent(165/999):loss = 0.249224\n",
      "Gradient Descent(166/999):loss = 0.2492\n",
      "Gradient Descent(167/999):loss = 0.24918\n",
      "Gradient Descent(168/999):loss = 0.249164\n",
      "Gradient Descent(169/999):loss = 0.24914\n",
      "Gradient Descent(170/999):loss = 0.249164\n",
      "Gradient Descent(171/999):loss = 0.249156\n",
      "Gradient Descent(172/999):loss = 0.249156\n",
      "Gradient Descent(173/999):loss = 0.249152\n",
      "Gradient Descent(174/999):loss = 0.249156\n",
      "Gradient Descent(175/999):loss = 0.249168\n",
      "Gradient Descent(176/999):loss = 0.249148\n",
      "Gradient Descent(177/999):loss = 0.24912\n",
      "Gradient Descent(178/999):loss = 0.249144\n",
      "Gradient Descent(179/999):loss = 0.24914\n",
      "Gradient Descent(180/999):loss = 0.249136\n",
      "Gradient Descent(181/999):loss = 0.249144\n",
      "Gradient Descent(182/999):loss = 0.249148\n",
      "Gradient Descent(183/999):loss = 0.249124\n",
      "Gradient Descent(184/999):loss = 0.249116\n",
      "Gradient Descent(185/999):loss = 0.249104\n",
      "Gradient Descent(186/999):loss = 0.249132\n",
      "Gradient Descent(187/999):loss = 0.249128\n",
      "Gradient Descent(188/999):loss = 0.249104\n",
      "Gradient Descent(189/999):loss = 0.249092\n",
      "Gradient Descent(190/999):loss = 0.249052\n",
      "Gradient Descent(191/999):loss = 0.24906\n",
      "Gradient Descent(192/999):loss = 0.248996\n",
      "Gradient Descent(193/999):loss = 0.249004\n",
      "Gradient Descent(194/999):loss = 0.248984\n",
      "Gradient Descent(195/999):loss = 0.24898\n",
      "Gradient Descent(196/999):loss = 0.248972\n",
      "Gradient Descent(197/999):loss = 0.248964\n",
      "Gradient Descent(198/999):loss = 0.248976\n",
      "Gradient Descent(199/999):loss = 0.248972\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(200/999):loss = 0.248952\n",
      "Gradient Descent(201/999):loss = 0.248956\n",
      "Gradient Descent(202/999):loss = 0.248928\n",
      "Gradient Descent(203/999):loss = 0.248936\n",
      "Gradient Descent(204/999):loss = 0.248944\n",
      "Gradient Descent(205/999):loss = 0.248916\n",
      "Gradient Descent(206/999):loss = 0.248912\n",
      "Gradient Descent(207/999):loss = 0.24894\n",
      "Gradient Descent(208/999):loss = 0.24894\n",
      "Gradient Descent(209/999):loss = 0.248964\n",
      "Gradient Descent(210/999):loss = 0.248964\n",
      "Gradient Descent(211/999):loss = 0.248944\n",
      "Gradient Descent(212/999):loss = 0.248928\n",
      "Gradient Descent(213/999):loss = 0.248936\n",
      "Gradient Descent(214/999):loss = 0.248944\n",
      "Gradient Descent(215/999):loss = 0.24894\n",
      "Gradient Descent(216/999):loss = 0.248928\n",
      "Gradient Descent(217/999):loss = 0.2489\n",
      "Gradient Descent(218/999):loss = 0.248888\n",
      "Gradient Descent(219/999):loss = 0.248864\n",
      "Gradient Descent(220/999):loss = 0.24886\n",
      "Gradient Descent(221/999):loss = 0.248836\n",
      "Gradient Descent(222/999):loss = 0.24884\n",
      "Gradient Descent(223/999):loss = 0.248856\n",
      "Gradient Descent(224/999):loss = 0.248824\n",
      "Gradient Descent(225/999):loss = 0.248816\n",
      "Gradient Descent(226/999):loss = 0.248824\n",
      "Gradient Descent(227/999):loss = 0.248832\n",
      "Gradient Descent(228/999):loss = 0.248824\n",
      "Gradient Descent(229/999):loss = 0.248808\n",
      "Gradient Descent(230/999):loss = 0.248816\n",
      "Gradient Descent(231/999):loss = 0.248816\n",
      "Gradient Descent(232/999):loss = 0.248804\n",
      "Gradient Descent(233/999):loss = 0.248788\n",
      "Gradient Descent(234/999):loss = 0.248796\n",
      "Gradient Descent(235/999):loss = 0.248808\n",
      "Gradient Descent(236/999):loss = 0.248812\n",
      "Gradient Descent(237/999):loss = 0.2488\n",
      "Gradient Descent(238/999):loss = 0.248784\n",
      "Gradient Descent(239/999):loss = 0.248768\n",
      "Gradient Descent(240/999):loss = 0.248756\n",
      "Gradient Descent(241/999):loss = 0.248752\n",
      "Gradient Descent(242/999):loss = 0.248752\n",
      "Gradient Descent(243/999):loss = 0.24876\n",
      "Gradient Descent(244/999):loss = 0.248748\n",
      "Gradient Descent(245/999):loss = 0.24876\n",
      "Gradient Descent(246/999):loss = 0.248768\n",
      "Gradient Descent(247/999):loss = 0.24878\n",
      "Gradient Descent(248/999):loss = 0.248788\n",
      "Gradient Descent(249/999):loss = 0.248768\n",
      "Gradient Descent(250/999):loss = 0.248756\n",
      "Gradient Descent(251/999):loss = 0.248756\n",
      "Gradient Descent(252/999):loss = 0.248752\n",
      "Gradient Descent(253/999):loss = 0.248756\n",
      "Gradient Descent(254/999):loss = 0.248732\n",
      "Gradient Descent(255/999):loss = 0.248752\n",
      "Gradient Descent(256/999):loss = 0.248748\n",
      "Gradient Descent(257/999):loss = 0.248736\n",
      "Gradient Descent(258/999):loss = 0.248716\n",
      "Gradient Descent(259/999):loss = 0.248716\n",
      "Gradient Descent(260/999):loss = 0.248732\n",
      "Gradient Descent(261/999):loss = 0.248736\n",
      "Gradient Descent(262/999):loss = 0.248744\n",
      "Gradient Descent(263/999):loss = 0.248752\n",
      "Gradient Descent(264/999):loss = 0.248748\n",
      "Gradient Descent(265/999):loss = 0.24874\n",
      "Gradient Descent(266/999):loss = 0.248724\n",
      "Gradient Descent(267/999):loss = 0.248724\n",
      "Gradient Descent(268/999):loss = 0.24872\n",
      "Gradient Descent(269/999):loss = 0.2487\n",
      "Gradient Descent(270/999):loss = 0.248704\n",
      "Gradient Descent(271/999):loss = 0.248688\n",
      "Gradient Descent(272/999):loss = 0.248668\n",
      "Gradient Descent(273/999):loss = 0.248672\n",
      "Gradient Descent(274/999):loss = 0.248672\n",
      "Gradient Descent(275/999):loss = 0.248672\n",
      "Gradient Descent(276/999):loss = 0.248664\n",
      "Gradient Descent(277/999):loss = 0.248652\n",
      "Gradient Descent(278/999):loss = 0.248648\n",
      "Gradient Descent(279/999):loss = 0.248664\n",
      "Gradient Descent(280/999):loss = 0.248672\n",
      "Gradient Descent(281/999):loss = 0.248668\n",
      "Gradient Descent(282/999):loss = 0.248668\n",
      "Gradient Descent(283/999):loss = 0.248672\n",
      "Gradient Descent(284/999):loss = 0.248676\n",
      "Gradient Descent(285/999):loss = 0.248668\n",
      "Gradient Descent(286/999):loss = 0.248672\n",
      "Gradient Descent(287/999):loss = 0.248668\n",
      "Gradient Descent(288/999):loss = 0.248668\n",
      "Gradient Descent(289/999):loss = 0.248676\n",
      "Gradient Descent(290/999):loss = 0.248672\n",
      "Gradient Descent(291/999):loss = 0.248656\n",
      "Gradient Descent(292/999):loss = 0.248644\n",
      "Gradient Descent(293/999):loss = 0.248632\n",
      "Gradient Descent(294/999):loss = 0.248624\n",
      "Gradient Descent(295/999):loss = 0.248632\n",
      "Gradient Descent(296/999):loss = 0.248636\n",
      "Gradient Descent(297/999):loss = 0.248628\n",
      "Gradient Descent(298/999):loss = 0.248628\n",
      "Gradient Descent(299/999):loss = 0.248616\n",
      "Gradient Descent(300/999):loss = 0.248612\n",
      "Gradient Descent(301/999):loss = 0.248624\n",
      "Gradient Descent(302/999):loss = 0.248616\n",
      "Gradient Descent(303/999):loss = 0.248608\n",
      "Gradient Descent(304/999):loss = 0.248608\n",
      "Gradient Descent(305/999):loss = 0.248608\n",
      "Gradient Descent(306/999):loss = 0.248596\n",
      "Gradient Descent(307/999):loss = 0.248588\n",
      "Gradient Descent(308/999):loss = 0.248592\n",
      "Gradient Descent(309/999):loss = 0.248568\n",
      "Gradient Descent(310/999):loss = 0.248572\n",
      "Gradient Descent(311/999):loss = 0.248588\n",
      "Gradient Descent(312/999):loss = 0.24858\n",
      "Gradient Descent(313/999):loss = 0.248584\n",
      "Gradient Descent(314/999):loss = 0.24858\n",
      "Gradient Descent(315/999):loss = 0.248568\n",
      "Gradient Descent(316/999):loss = 0.248564\n",
      "Gradient Descent(317/999):loss = 0.248568\n",
      "Gradient Descent(318/999):loss = 0.248556\n",
      "Gradient Descent(319/999):loss = 0.248552\n",
      "Gradient Descent(320/999):loss = 0.248568\n",
      "Gradient Descent(321/999):loss = 0.248572\n",
      "Gradient Descent(322/999):loss = 0.248572\n",
      "Gradient Descent(323/999):loss = 0.248576\n",
      "Gradient Descent(324/999):loss = 0.248584\n",
      "Gradient Descent(325/999):loss = 0.248564\n",
      "Gradient Descent(326/999):loss = 0.24856\n",
      "Gradient Descent(327/999):loss = 0.248556\n",
      "Gradient Descent(328/999):loss = 0.248552\n",
      "Gradient Descent(329/999):loss = 0.248544\n",
      "Gradient Descent(330/999):loss = 0.248548\n",
      "Gradient Descent(331/999):loss = 0.248544\n",
      "Gradient Descent(332/999):loss = 0.248556\n",
      "Gradient Descent(333/999):loss = 0.248572\n",
      "Gradient Descent(334/999):loss = 0.248584\n",
      "Gradient Descent(335/999):loss = 0.24858\n",
      "Gradient Descent(336/999):loss = 0.24858\n",
      "Gradient Descent(337/999):loss = 0.248568\n",
      "Gradient Descent(338/999):loss = 0.248564\n",
      "Gradient Descent(339/999):loss = 0.24856\n",
      "Gradient Descent(340/999):loss = 0.248552\n",
      "Gradient Descent(341/999):loss = 0.24854\n",
      "Gradient Descent(342/999):loss = 0.248548\n",
      "Gradient Descent(343/999):loss = 0.248556\n",
      "Gradient Descent(344/999):loss = 0.248536\n",
      "Gradient Descent(345/999):loss = 0.24854\n",
      "Gradient Descent(346/999):loss = 0.24856\n",
      "Gradient Descent(347/999):loss = 0.24856\n",
      "Gradient Descent(348/999):loss = 0.248552\n",
      "Gradient Descent(349/999):loss = 0.248552\n",
      "Gradient Descent(350/999):loss = 0.248544\n",
      "Gradient Descent(351/999):loss = 0.248552\n",
      "Gradient Descent(352/999):loss = 0.24854\n",
      "Gradient Descent(353/999):loss = 0.24854\n",
      "Gradient Descent(354/999):loss = 0.248532\n",
      "Gradient Descent(355/999):loss = 0.248536\n",
      "Gradient Descent(356/999):loss = 0.248536\n",
      "Gradient Descent(357/999):loss = 0.248536\n",
      "Gradient Descent(358/999):loss = 0.248552\n",
      "Gradient Descent(359/999):loss = 0.248548\n",
      "Gradient Descent(360/999):loss = 0.248544\n",
      "Gradient Descent(361/999):loss = 0.248548\n",
      "Gradient Descent(362/999):loss = 0.248548\n",
      "Gradient Descent(363/999):loss = 0.248544\n",
      "Gradient Descent(364/999):loss = 0.248548\n",
      "Gradient Descent(365/999):loss = 0.248552\n",
      "Gradient Descent(366/999):loss = 0.248552\n",
      "Gradient Descent(367/999):loss = 0.248532\n",
      "Gradient Descent(368/999):loss = 0.248528\n",
      "Gradient Descent(369/999):loss = 0.248516\n",
      "Gradient Descent(370/999):loss = 0.248508\n",
      "Gradient Descent(371/999):loss = 0.248504\n",
      "Gradient Descent(372/999):loss = 0.248508\n",
      "Gradient Descent(373/999):loss = 0.248508\n",
      "Gradient Descent(374/999):loss = 0.248496\n",
      "Gradient Descent(375/999):loss = 0.248488\n",
      "Gradient Descent(376/999):loss = 0.248496\n",
      "Gradient Descent(377/999):loss = 0.248492\n",
      "Gradient Descent(378/999):loss = 0.248496\n",
      "Gradient Descent(379/999):loss = 0.248496\n",
      "Gradient Descent(380/999):loss = 0.248488\n",
      "Gradient Descent(381/999):loss = 0.248484\n",
      "Gradient Descent(382/999):loss = 0.24848\n",
      "Gradient Descent(383/999):loss = 0.248484\n",
      "Gradient Descent(384/999):loss = 0.24848\n",
      "Gradient Descent(385/999):loss = 0.248476\n",
      "Gradient Descent(386/999):loss = 0.248476\n",
      "Gradient Descent(387/999):loss = 0.248476\n",
      "Gradient Descent(388/999):loss = 0.248488\n",
      "Gradient Descent(389/999):loss = 0.248484\n",
      "Gradient Descent(390/999):loss = 0.248484\n",
      "Gradient Descent(391/999):loss = 0.248468\n",
      "Gradient Descent(392/999):loss = 0.248468\n",
      "Gradient Descent(393/999):loss = 0.24846\n",
      "Gradient Descent(394/999):loss = 0.248448\n",
      "Gradient Descent(395/999):loss = 0.248452\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(396/999):loss = 0.248444\n",
      "Gradient Descent(397/999):loss = 0.248448\n",
      "Gradient Descent(398/999):loss = 0.248452\n",
      "Gradient Descent(399/999):loss = 0.248448\n",
      "Gradient Descent(400/999):loss = 0.24844\n",
      "Gradient Descent(401/999):loss = 0.24844\n",
      "Gradient Descent(402/999):loss = 0.248436\n",
      "Gradient Descent(403/999):loss = 0.248432\n",
      "Gradient Descent(404/999):loss = 0.248432\n",
      "Gradient Descent(405/999):loss = 0.248448\n",
      "Gradient Descent(406/999):loss = 0.248452\n",
      "Gradient Descent(407/999):loss = 0.24846\n",
      "Gradient Descent(408/999):loss = 0.248456\n",
      "Gradient Descent(409/999):loss = 0.24846\n",
      "Gradient Descent(410/999):loss = 0.248456\n",
      "Gradient Descent(411/999):loss = 0.24846\n",
      "Gradient Descent(412/999):loss = 0.248448\n",
      "Gradient Descent(413/999):loss = 0.24846\n",
      "Gradient Descent(414/999):loss = 0.248432\n",
      "Gradient Descent(415/999):loss = 0.248444\n",
      "Gradient Descent(416/999):loss = 0.248448\n",
      "Gradient Descent(417/999):loss = 0.248444\n",
      "Gradient Descent(418/999):loss = 0.24844\n",
      "Gradient Descent(419/999):loss = 0.248436\n",
      "Gradient Descent(420/999):loss = 0.248436\n",
      "Gradient Descent(421/999):loss = 0.248448\n",
      "Gradient Descent(422/999):loss = 0.248452\n",
      "Gradient Descent(423/999):loss = 0.248468\n",
      "Gradient Descent(424/999):loss = 0.248468\n",
      "Gradient Descent(425/999):loss = 0.248468\n",
      "Gradient Descent(426/999):loss = 0.24846\n",
      "Gradient Descent(427/999):loss = 0.248464\n",
      "Gradient Descent(428/999):loss = 0.248464\n",
      "Gradient Descent(429/999):loss = 0.248464\n",
      "Gradient Descent(430/999):loss = 0.248472\n",
      "Gradient Descent(431/999):loss = 0.248476\n",
      "Gradient Descent(432/999):loss = 0.248476\n",
      "Gradient Descent(433/999):loss = 0.248476\n",
      "Gradient Descent(434/999):loss = 0.248484\n",
      "Gradient Descent(435/999):loss = 0.24848\n",
      "Gradient Descent(436/999):loss = 0.248476\n",
      "Gradient Descent(437/999):loss = 0.248484\n",
      "Gradient Descent(438/999):loss = 0.248484\n",
      "Gradient Descent(439/999):loss = 0.248484\n",
      "Gradient Descent(440/999):loss = 0.248484\n",
      "Gradient Descent(441/999):loss = 0.248476\n",
      "Gradient Descent(442/999):loss = 0.248472\n",
      "Gradient Descent(443/999):loss = 0.248476\n",
      "Gradient Descent(444/999):loss = 0.248468\n",
      "Gradient Descent(445/999):loss = 0.248468\n",
      "Gradient Descent(446/999):loss = 0.248468\n",
      "Gradient Descent(447/999):loss = 0.248468\n",
      "Gradient Descent(448/999):loss = 0.24846\n",
      "Gradient Descent(449/999):loss = 0.248452\n",
      "Gradient Descent(450/999):loss = 0.248448\n",
      "Gradient Descent(451/999):loss = 0.248448\n",
      "Gradient Descent(452/999):loss = 0.248452\n",
      "Gradient Descent(453/999):loss = 0.248452\n",
      "Gradient Descent(454/999):loss = 0.248448\n",
      "Gradient Descent(455/999):loss = 0.248448\n",
      "Gradient Descent(456/999):loss = 0.248452\n",
      "Gradient Descent(457/999):loss = 0.24846\n",
      "Gradient Descent(458/999):loss = 0.248464\n",
      "Gradient Descent(459/999):loss = 0.248456\n",
      "Gradient Descent(460/999):loss = 0.248452\n",
      "Gradient Descent(461/999):loss = 0.248452\n",
      "Gradient Descent(462/999):loss = 0.24846\n",
      "Gradient Descent(463/999):loss = 0.248456\n",
      "Gradient Descent(464/999):loss = 0.248456\n",
      "Gradient Descent(465/999):loss = 0.248452\n",
      "Gradient Descent(466/999):loss = 0.248444\n",
      "Gradient Descent(467/999):loss = 0.248444\n",
      "Gradient Descent(468/999):loss = 0.248448\n",
      "Gradient Descent(469/999):loss = 0.248452\n",
      "Gradient Descent(470/999):loss = 0.248452\n",
      "Gradient Descent(471/999):loss = 0.248456\n",
      "Gradient Descent(472/999):loss = 0.248468\n",
      "Gradient Descent(473/999):loss = 0.248468\n",
      "Gradient Descent(474/999):loss = 0.248472\n",
      "Gradient Descent(475/999):loss = 0.248476\n",
      "Gradient Descent(476/999):loss = 0.248476\n",
      "Gradient Descent(477/999):loss = 0.248476\n",
      "Gradient Descent(478/999):loss = 0.248472\n",
      "Gradient Descent(479/999):loss = 0.248468\n",
      "Gradient Descent(480/999):loss = 0.248472\n",
      "Gradient Descent(481/999):loss = 0.248476\n",
      "Gradient Descent(482/999):loss = 0.248476\n",
      "Gradient Descent(483/999):loss = 0.24848\n",
      "Gradient Descent(484/999):loss = 0.24848\n",
      "Gradient Descent(485/999):loss = 0.24848\n",
      "Gradient Descent(486/999):loss = 0.24848\n",
      "Gradient Descent(487/999):loss = 0.248484\n",
      "Gradient Descent(488/999):loss = 0.248484\n",
      "Gradient Descent(489/999):loss = 0.248484\n",
      "Gradient Descent(490/999):loss = 0.248484\n",
      "Gradient Descent(491/999):loss = 0.248476\n",
      "Gradient Descent(492/999):loss = 0.248476\n",
      "Gradient Descent(493/999):loss = 0.248472\n",
      "Gradient Descent(494/999):loss = 0.248472\n",
      "Gradient Descent(495/999):loss = 0.248472\n",
      "Gradient Descent(496/999):loss = 0.248472\n",
      "Gradient Descent(497/999):loss = 0.248488\n",
      "Gradient Descent(498/999):loss = 0.248492\n",
      "Gradient Descent(499/999):loss = 0.248492\n",
      "Gradient Descent(500/999):loss = 0.248492\n",
      "Gradient Descent(501/999):loss = 0.248488\n",
      "Gradient Descent(502/999):loss = 0.248488\n",
      "Gradient Descent(503/999):loss = 0.248488\n",
      "Gradient Descent(504/999):loss = 0.248488\n",
      "Gradient Descent(505/999):loss = 0.248488\n",
      "Gradient Descent(506/999):loss = 0.24848\n",
      "Gradient Descent(507/999):loss = 0.248488\n",
      "Gradient Descent(508/999):loss = 0.248484\n",
      "Gradient Descent(509/999):loss = 0.248488\n",
      "Gradient Descent(510/999):loss = 0.248488\n",
      "Gradient Descent(511/999):loss = 0.248484\n",
      "Gradient Descent(512/999):loss = 0.248484\n",
      "Gradient Descent(513/999):loss = 0.248484\n",
      "Gradient Descent(514/999):loss = 0.248488\n",
      "Gradient Descent(515/999):loss = 0.248492\n",
      "Gradient Descent(516/999):loss = 0.248488\n",
      "Gradient Descent(517/999):loss = 0.24848\n",
      "Gradient Descent(518/999):loss = 0.24848\n",
      "Gradient Descent(519/999):loss = 0.248468\n",
      "Gradient Descent(520/999):loss = 0.248456\n",
      "Gradient Descent(521/999):loss = 0.248456\n",
      "Gradient Descent(522/999):loss = 0.248464\n",
      "Gradient Descent(523/999):loss = 0.24846\n",
      "Gradient Descent(524/999):loss = 0.248456\n",
      "Gradient Descent(525/999):loss = 0.248452\n",
      "Gradient Descent(526/999):loss = 0.248448\n",
      "Gradient Descent(527/999):loss = 0.248452\n",
      "Gradient Descent(528/999):loss = 0.248452\n",
      "Gradient Descent(529/999):loss = 0.248444\n",
      "Gradient Descent(530/999):loss = 0.248448\n",
      "Gradient Descent(531/999):loss = 0.248452\n",
      "Gradient Descent(532/999):loss = 0.248452\n",
      "Gradient Descent(533/999):loss = 0.24846\n",
      "Gradient Descent(534/999):loss = 0.24846\n",
      "Gradient Descent(535/999):loss = 0.24846\n",
      "Gradient Descent(536/999):loss = 0.248464\n",
      "Gradient Descent(537/999):loss = 0.248468\n",
      "Gradient Descent(538/999):loss = 0.248468\n",
      "Gradient Descent(539/999):loss = 0.248468\n",
      "Gradient Descent(540/999):loss = 0.248468\n",
      "Gradient Descent(541/999):loss = 0.248472\n",
      "Gradient Descent(542/999):loss = 0.248476\n",
      "Gradient Descent(543/999):loss = 0.24848\n",
      "Gradient Descent(544/999):loss = 0.24848\n",
      "Gradient Descent(545/999):loss = 0.24848\n",
      "Gradient Descent(546/999):loss = 0.248484\n",
      "Gradient Descent(547/999):loss = 0.248484\n",
      "Gradient Descent(548/999):loss = 0.248488\n",
      "Gradient Descent(549/999):loss = 0.248492\n",
      "Gradient Descent(550/999):loss = 0.248492\n",
      "Gradient Descent(551/999):loss = 0.2485\n",
      "Gradient Descent(552/999):loss = 0.2485\n",
      "Gradient Descent(553/999):loss = 0.248496\n",
      "Gradient Descent(554/999):loss = 0.248492\n",
      "Gradient Descent(555/999):loss = 0.248492\n",
      "Gradient Descent(556/999):loss = 0.24848\n",
      "Gradient Descent(557/999):loss = 0.248476\n",
      "Gradient Descent(558/999):loss = 0.248476\n",
      "Gradient Descent(559/999):loss = 0.24848\n",
      "Gradient Descent(560/999):loss = 0.248484\n",
      "Gradient Descent(561/999):loss = 0.248484\n",
      "Gradient Descent(562/999):loss = 0.248484\n",
      "Gradient Descent(563/999):loss = 0.248484\n",
      "Gradient Descent(564/999):loss = 0.248484\n",
      "Gradient Descent(565/999):loss = 0.24848\n",
      "Gradient Descent(566/999):loss = 0.24848\n",
      "Gradient Descent(567/999):loss = 0.24848\n",
      "Gradient Descent(568/999):loss = 0.24848\n",
      "Gradient Descent(569/999):loss = 0.248476\n",
      "Gradient Descent(570/999):loss = 0.248468\n",
      "Gradient Descent(571/999):loss = 0.248468\n",
      "Gradient Descent(572/999):loss = 0.248468\n",
      "Gradient Descent(573/999):loss = 0.248468\n",
      "Gradient Descent(574/999):loss = 0.248468\n",
      "Gradient Descent(575/999):loss = 0.248468\n",
      "Gradient Descent(576/999):loss = 0.248464\n",
      "Gradient Descent(577/999):loss = 0.248464\n",
      "Gradient Descent(578/999):loss = 0.248472\n",
      "Gradient Descent(579/999):loss = 0.248472\n",
      "Gradient Descent(580/999):loss = 0.248468\n",
      "Gradient Descent(581/999):loss = 0.248472\n",
      "Gradient Descent(582/999):loss = 0.248464\n",
      "Gradient Descent(583/999):loss = 0.248464\n",
      "Gradient Descent(584/999):loss = 0.248464\n",
      "Gradient Descent(585/999):loss = 0.248464\n",
      "Gradient Descent(586/999):loss = 0.24846\n",
      "Gradient Descent(587/999):loss = 0.24846\n",
      "Gradient Descent(588/999):loss = 0.24846\n",
      "Gradient Descent(589/999):loss = 0.24846\n",
      "Gradient Descent(590/999):loss = 0.248456\n",
      "Gradient Descent(591/999):loss = 0.248456\n",
      "Gradient Descent(592/999):loss = 0.248456\n",
      "Gradient Descent(593/999):loss = 0.248456\n",
      "Gradient Descent(594/999):loss = 0.248456\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(595/999):loss = 0.248452\n",
      "Gradient Descent(596/999):loss = 0.24846\n",
      "Gradient Descent(597/999):loss = 0.24846\n",
      "Gradient Descent(598/999):loss = 0.248456\n",
      "Gradient Descent(599/999):loss = 0.248456\n",
      "Gradient Descent(600/999):loss = 0.248456\n",
      "Gradient Descent(601/999):loss = 0.248456\n",
      "Gradient Descent(602/999):loss = 0.248456\n",
      "Gradient Descent(603/999):loss = 0.248456\n",
      "Gradient Descent(604/999):loss = 0.248452\n",
      "Gradient Descent(605/999):loss = 0.248452\n",
      "Gradient Descent(606/999):loss = 0.248456\n",
      "Gradient Descent(607/999):loss = 0.248456\n",
      "Gradient Descent(608/999):loss = 0.248452\n",
      "Gradient Descent(609/999):loss = 0.248452\n",
      "Gradient Descent(610/999):loss = 0.248456\n",
      "Gradient Descent(611/999):loss = 0.248456\n",
      "Gradient Descent(612/999):loss = 0.248464\n",
      "Gradient Descent(613/999):loss = 0.248464\n",
      "Gradient Descent(614/999):loss = 0.248464\n",
      "Gradient Descent(615/999):loss = 0.248464\n",
      "Gradient Descent(616/999):loss = 0.248468\n",
      "Gradient Descent(617/999):loss = 0.248468\n",
      "Gradient Descent(618/999):loss = 0.248468\n",
      "Gradient Descent(619/999):loss = 0.248464\n",
      "Gradient Descent(620/999):loss = 0.248464\n",
      "Gradient Descent(621/999):loss = 0.248464\n",
      "Gradient Descent(622/999):loss = 0.24846\n",
      "Gradient Descent(623/999):loss = 0.248452\n",
      "Gradient Descent(624/999):loss = 0.248456\n",
      "Gradient Descent(625/999):loss = 0.248452\n",
      "Gradient Descent(626/999):loss = 0.248452\n",
      "Gradient Descent(627/999):loss = 0.248444\n",
      "Gradient Descent(628/999):loss = 0.24844\n",
      "Gradient Descent(629/999):loss = 0.24844\n",
      "Gradient Descent(630/999):loss = 0.248436\n",
      "Gradient Descent(631/999):loss = 0.248432\n",
      "Gradient Descent(632/999):loss = 0.248428\n",
      "Gradient Descent(633/999):loss = 0.248428\n",
      "Gradient Descent(634/999):loss = 0.248424\n",
      "Gradient Descent(635/999):loss = 0.24842\n",
      "Gradient Descent(636/999):loss = 0.248416\n",
      "Gradient Descent(637/999):loss = 0.248416\n",
      "Gradient Descent(638/999):loss = 0.248416\n",
      "Gradient Descent(639/999):loss = 0.248416\n",
      "Gradient Descent(640/999):loss = 0.248416\n",
      "Gradient Descent(641/999):loss = 0.248416\n",
      "Gradient Descent(642/999):loss = 0.248404\n",
      "Gradient Descent(643/999):loss = 0.248404\n",
      "Gradient Descent(644/999):loss = 0.248408\n",
      "Gradient Descent(645/999):loss = 0.248408\n",
      "Gradient Descent(646/999):loss = 0.248408\n",
      "Gradient Descent(647/999):loss = 0.248408\n",
      "Gradient Descent(648/999):loss = 0.248408\n",
      "Gradient Descent(649/999):loss = 0.248416\n",
      "Gradient Descent(650/999):loss = 0.248416\n",
      "Gradient Descent(651/999):loss = 0.248416\n",
      "Gradient Descent(652/999):loss = 0.248416\n",
      "Gradient Descent(653/999):loss = 0.248416\n",
      "Gradient Descent(654/999):loss = 0.248416\n",
      "Gradient Descent(655/999):loss = 0.248416\n",
      "Gradient Descent(656/999):loss = 0.248416\n",
      "Gradient Descent(657/999):loss = 0.248404\n",
      "Gradient Descent(658/999):loss = 0.248404\n",
      "Gradient Descent(659/999):loss = 0.248412\n",
      "Gradient Descent(660/999):loss = 0.248412\n",
      "Gradient Descent(661/999):loss = 0.248416\n",
      "Gradient Descent(662/999):loss = 0.248416\n",
      "Gradient Descent(663/999):loss = 0.248408\n",
      "Gradient Descent(664/999):loss = 0.248408\n",
      "Gradient Descent(665/999):loss = 0.248408\n",
      "Gradient Descent(666/999):loss = 0.248408\n",
      "Gradient Descent(667/999):loss = 0.2484\n",
      "Gradient Descent(668/999):loss = 0.2484\n",
      "Gradient Descent(669/999):loss = 0.2484\n",
      "Gradient Descent(670/999):loss = 0.2484\n",
      "Gradient Descent(671/999):loss = 0.248404\n",
      "Gradient Descent(672/999):loss = 0.248404\n",
      "Gradient Descent(673/999):loss = 0.248408\n",
      "Gradient Descent(674/999):loss = 0.248416\n",
      "Gradient Descent(675/999):loss = 0.248416\n",
      "Gradient Descent(676/999):loss = 0.24842\n",
      "Gradient Descent(677/999):loss = 0.24842\n",
      "Gradient Descent(678/999):loss = 0.24842\n",
      "Gradient Descent(679/999):loss = 0.24842\n",
      "Gradient Descent(680/999):loss = 0.24842\n",
      "Gradient Descent(681/999):loss = 0.24842\n",
      "Gradient Descent(682/999):loss = 0.248424\n",
      "Gradient Descent(683/999):loss = 0.248424\n",
      "Gradient Descent(684/999):loss = 0.248428\n",
      "Gradient Descent(685/999):loss = 0.248428\n",
      "Gradient Descent(686/999):loss = 0.248432\n",
      "Gradient Descent(687/999):loss = 0.248432\n",
      "Gradient Descent(688/999):loss = 0.248432\n",
      "Gradient Descent(689/999):loss = 0.248432\n",
      "Gradient Descent(690/999):loss = 0.248436\n",
      "Gradient Descent(691/999):loss = 0.24844\n",
      "Gradient Descent(692/999):loss = 0.248436\n",
      "Gradient Descent(693/999):loss = 0.24844\n",
      "Gradient Descent(694/999):loss = 0.24844\n",
      "Gradient Descent(695/999):loss = 0.24844\n",
      "Gradient Descent(696/999):loss = 0.24844\n",
      "Gradient Descent(697/999):loss = 0.24844\n",
      "Gradient Descent(698/999):loss = 0.24844\n",
      "Gradient Descent(699/999):loss = 0.24844\n",
      "Gradient Descent(700/999):loss = 0.248432\n",
      "Gradient Descent(701/999):loss = 0.248432\n",
      "Gradient Descent(702/999):loss = 0.248432\n",
      "Gradient Descent(703/999):loss = 0.248428\n",
      "Gradient Descent(704/999):loss = 0.248428\n",
      "Gradient Descent(705/999):loss = 0.248428\n",
      "Gradient Descent(706/999):loss = 0.248428\n",
      "Gradient Descent(707/999):loss = 0.248424\n",
      "Gradient Descent(708/999):loss = 0.248424\n",
      "Gradient Descent(709/999):loss = 0.248424\n",
      "Gradient Descent(710/999):loss = 0.248424\n",
      "Gradient Descent(711/999):loss = 0.248424\n",
      "Gradient Descent(712/999):loss = 0.248424\n",
      "Gradient Descent(713/999):loss = 0.248432\n",
      "Gradient Descent(714/999):loss = 0.248432\n",
      "Gradient Descent(715/999):loss = 0.248432\n",
      "Gradient Descent(716/999):loss = 0.248432\n",
      "Gradient Descent(717/999):loss = 0.248432\n",
      "Gradient Descent(718/999):loss = 0.248432\n",
      "Gradient Descent(719/999):loss = 0.248432\n",
      "Gradient Descent(720/999):loss = 0.248432\n",
      "Gradient Descent(721/999):loss = 0.248432\n",
      "Gradient Descent(722/999):loss = 0.248428\n",
      "Gradient Descent(723/999):loss = 0.248428\n",
      "Gradient Descent(724/999):loss = 0.248428\n",
      "Gradient Descent(725/999):loss = 0.248424\n",
      "Gradient Descent(726/999):loss = 0.248424\n",
      "Gradient Descent(727/999):loss = 0.248424\n",
      "Gradient Descent(728/999):loss = 0.248424\n",
      "Gradient Descent(729/999):loss = 0.248428\n",
      "Gradient Descent(730/999):loss = 0.248428\n",
      "Gradient Descent(731/999):loss = 0.248428\n",
      "Gradient Descent(732/999):loss = 0.248428\n",
      "Gradient Descent(733/999):loss = 0.248424\n",
      "Gradient Descent(734/999):loss = 0.248424\n",
      "Gradient Descent(735/999):loss = 0.248424\n",
      "Gradient Descent(736/999):loss = 0.248428\n",
      "Gradient Descent(737/999):loss = 0.248428\n",
      "Gradient Descent(738/999):loss = 0.248428\n",
      "Gradient Descent(739/999):loss = 0.248428\n",
      "Gradient Descent(740/999):loss = 0.248428\n",
      "Gradient Descent(741/999):loss = 0.248432\n",
      "Gradient Descent(742/999):loss = 0.248432\n",
      "Gradient Descent(743/999):loss = 0.248432\n",
      "Gradient Descent(744/999):loss = 0.248436\n",
      "Gradient Descent(745/999):loss = 0.248436\n",
      "Gradient Descent(746/999):loss = 0.248436\n",
      "Gradient Descent(747/999):loss = 0.24844\n",
      "Gradient Descent(748/999):loss = 0.24844\n",
      "Gradient Descent(749/999):loss = 0.24844\n",
      "Gradient Descent(750/999):loss = 0.24844\n",
      "Gradient Descent(751/999):loss = 0.24844\n",
      "Gradient Descent(752/999):loss = 0.24844\n",
      "Gradient Descent(753/999):loss = 0.24844\n",
      "Gradient Descent(754/999):loss = 0.24844\n",
      "Gradient Descent(755/999):loss = 0.248444\n",
      "Gradient Descent(756/999):loss = 0.248444\n",
      "Gradient Descent(757/999):loss = 0.248444\n",
      "Gradient Descent(758/999):loss = 0.248448\n",
      "Gradient Descent(759/999):loss = 0.248444\n",
      "Gradient Descent(760/999):loss = 0.248448\n",
      "Gradient Descent(761/999):loss = 0.248448\n",
      "Gradient Descent(762/999):loss = 0.24844\n",
      "Gradient Descent(763/999):loss = 0.248436\n",
      "Gradient Descent(764/999):loss = 0.248436\n",
      "Gradient Descent(765/999):loss = 0.248436\n",
      "Gradient Descent(766/999):loss = 0.248436\n",
      "Gradient Descent(767/999):loss = 0.248436\n",
      "Gradient Descent(768/999):loss = 0.248436\n",
      "Gradient Descent(769/999):loss = 0.248436\n",
      "Gradient Descent(770/999):loss = 0.248436\n",
      "Gradient Descent(771/999):loss = 0.248436\n",
      "Gradient Descent(772/999):loss = 0.248436\n",
      "Gradient Descent(773/999):loss = 0.248436\n",
      "Gradient Descent(774/999):loss = 0.248436\n",
      "Gradient Descent(775/999):loss = 0.248436\n",
      "Gradient Descent(776/999):loss = 0.248436\n",
      "Gradient Descent(777/999):loss = 0.248436\n",
      "Gradient Descent(778/999):loss = 0.248436\n",
      "Gradient Descent(779/999):loss = 0.248436\n",
      "Gradient Descent(780/999):loss = 0.248436\n",
      "Gradient Descent(781/999):loss = 0.248432\n",
      "Gradient Descent(782/999):loss = 0.248432\n",
      "Gradient Descent(783/999):loss = 0.248432\n",
      "Gradient Descent(784/999):loss = 0.248432\n",
      "Gradient Descent(785/999):loss = 0.248436\n",
      "Gradient Descent(786/999):loss = 0.248436\n",
      "Gradient Descent(787/999):loss = 0.248436\n",
      "Gradient Descent(788/999):loss = 0.248436\n",
      "Gradient Descent(789/999):loss = 0.248436\n",
      "Gradient Descent(790/999):loss = 0.248436\n",
      "Gradient Descent(791/999):loss = 0.248432\n",
      "Gradient Descent(792/999):loss = 0.248432\n",
      "Gradient Descent(793/999):loss = 0.248432\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(794/999):loss = 0.248432\n",
      "Gradient Descent(795/999):loss = 0.248432\n",
      "Gradient Descent(796/999):loss = 0.248432\n",
      "Gradient Descent(797/999):loss = 0.248432\n",
      "Gradient Descent(798/999):loss = 0.248432\n",
      "Gradient Descent(799/999):loss = 0.248428\n",
      "Gradient Descent(800/999):loss = 0.248424\n",
      "Gradient Descent(801/999):loss = 0.248424\n",
      "Gradient Descent(802/999):loss = 0.248424\n",
      "Gradient Descent(803/999):loss = 0.248424\n",
      "Gradient Descent(804/999):loss = 0.24842\n",
      "Gradient Descent(805/999):loss = 0.24842\n",
      "Gradient Descent(806/999):loss = 0.24842\n",
      "Gradient Descent(807/999):loss = 0.24842\n",
      "Gradient Descent(808/999):loss = 0.248424\n",
      "Gradient Descent(809/999):loss = 0.248424\n",
      "Gradient Descent(810/999):loss = 0.24842\n",
      "Gradient Descent(811/999):loss = 0.24842\n",
      "Gradient Descent(812/999):loss = 0.24842\n",
      "Gradient Descent(813/999):loss = 0.248424\n",
      "Gradient Descent(814/999):loss = 0.248424\n",
      "Gradient Descent(815/999):loss = 0.248428\n",
      "Gradient Descent(816/999):loss = 0.248428\n",
      "Gradient Descent(817/999):loss = 0.248428\n",
      "Gradient Descent(818/999):loss = 0.248428\n",
      "Gradient Descent(819/999):loss = 0.248428\n",
      "Gradient Descent(820/999):loss = 0.248436\n",
      "Gradient Descent(821/999):loss = 0.248436\n",
      "Gradient Descent(822/999):loss = 0.248436\n",
      "Gradient Descent(823/999):loss = 0.248428\n",
      "Gradient Descent(824/999):loss = 0.248432\n",
      "Gradient Descent(825/999):loss = 0.248432\n",
      "Gradient Descent(826/999):loss = 0.248432\n",
      "Gradient Descent(827/999):loss = 0.248428\n",
      "Gradient Descent(828/999):loss = 0.248428\n",
      "Gradient Descent(829/999):loss = 0.248428\n",
      "Gradient Descent(830/999):loss = 0.248428\n",
      "Gradient Descent(831/999):loss = 0.248428\n",
      "Gradient Descent(832/999):loss = 0.248428\n",
      "Gradient Descent(833/999):loss = 0.248424\n",
      "Gradient Descent(834/999):loss = 0.248424\n",
      "Gradient Descent(835/999):loss = 0.248424\n",
      "Gradient Descent(836/999):loss = 0.248424\n",
      "Gradient Descent(837/999):loss = 0.248424\n",
      "Gradient Descent(838/999):loss = 0.24842\n",
      "Gradient Descent(839/999):loss = 0.24842\n",
      "Gradient Descent(840/999):loss = 0.248416\n",
      "Gradient Descent(841/999):loss = 0.248416\n",
      "Gradient Descent(842/999):loss = 0.248416\n",
      "Gradient Descent(843/999):loss = 0.248416\n",
      "Gradient Descent(844/999):loss = 0.248416\n",
      "Gradient Descent(845/999):loss = 0.248416\n",
      "Gradient Descent(846/999):loss = 0.248412\n",
      "Gradient Descent(847/999):loss = 0.248408\n",
      "Gradient Descent(848/999):loss = 0.248408\n",
      "Gradient Descent(849/999):loss = 0.248408\n",
      "Gradient Descent(850/999):loss = 0.248408\n",
      "Gradient Descent(851/999):loss = 0.248408\n",
      "Gradient Descent(852/999):loss = 0.248408\n",
      "Gradient Descent(853/999):loss = 0.248404\n",
      "Gradient Descent(854/999):loss = 0.248404\n",
      "Gradient Descent(855/999):loss = 0.248404\n",
      "Gradient Descent(856/999):loss = 0.248404\n",
      "Gradient Descent(857/999):loss = 0.2484\n",
      "Gradient Descent(858/999):loss = 0.248404\n",
      "Gradient Descent(859/999):loss = 0.2484\n",
      "Gradient Descent(860/999):loss = 0.2484\n",
      "Gradient Descent(861/999):loss = 0.248396\n",
      "Gradient Descent(862/999):loss = 0.248396\n",
      "Gradient Descent(863/999):loss = 0.248396\n",
      "Gradient Descent(864/999):loss = 0.248396\n",
      "Gradient Descent(865/999):loss = 0.248396\n",
      "Gradient Descent(866/999):loss = 0.248396\n",
      "Gradient Descent(867/999):loss = 0.248396\n",
      "Gradient Descent(868/999):loss = 0.248396\n",
      "Gradient Descent(869/999):loss = 0.248392\n",
      "Gradient Descent(870/999):loss = 0.248396\n",
      "Gradient Descent(871/999):loss = 0.248396\n",
      "Gradient Descent(872/999):loss = 0.248396\n",
      "Gradient Descent(873/999):loss = 0.248396\n",
      "Gradient Descent(874/999):loss = 0.248396\n",
      "Gradient Descent(875/999):loss = 0.248388\n",
      "Gradient Descent(876/999):loss = 0.248392\n",
      "Gradient Descent(877/999):loss = 0.248388\n",
      "Gradient Descent(878/999):loss = 0.248388\n",
      "Gradient Descent(879/999):loss = 0.248384\n",
      "Gradient Descent(880/999):loss = 0.248384\n",
      "Gradient Descent(881/999):loss = 0.248384\n",
      "Gradient Descent(882/999):loss = 0.248384\n",
      "Gradient Descent(883/999):loss = 0.248384\n",
      "Gradient Descent(884/999):loss = 0.248384\n",
      "Gradient Descent(885/999):loss = 0.248384\n",
      "Gradient Descent(886/999):loss = 0.248384\n",
      "Gradient Descent(887/999):loss = 0.248384\n",
      "Gradient Descent(888/999):loss = 0.248388\n",
      "Gradient Descent(889/999):loss = 0.248392\n",
      "Gradient Descent(890/999):loss = 0.248396\n",
      "Gradient Descent(891/999):loss = 0.248396\n",
      "Gradient Descent(892/999):loss = 0.248396\n",
      "Gradient Descent(893/999):loss = 0.2484\n",
      "Gradient Descent(894/999):loss = 0.2484\n",
      "Gradient Descent(895/999):loss = 0.248404\n",
      "Gradient Descent(896/999):loss = 0.248404\n",
      "Gradient Descent(897/999):loss = 0.248404\n",
      "Gradient Descent(898/999):loss = 0.248404\n",
      "Gradient Descent(899/999):loss = 0.248404\n",
      "Gradient Descent(900/999):loss = 0.248404\n",
      "Gradient Descent(901/999):loss = 0.248408\n",
      "Gradient Descent(902/999):loss = 0.248412\n",
      "Gradient Descent(903/999):loss = 0.248412\n",
      "Gradient Descent(904/999):loss = 0.248412\n",
      "Gradient Descent(905/999):loss = 0.248412\n",
      "Gradient Descent(906/999):loss = 0.248412\n",
      "Gradient Descent(907/999):loss = 0.248408\n",
      "Gradient Descent(908/999):loss = 0.248412\n",
      "Gradient Descent(909/999):loss = 0.248412\n",
      "Gradient Descent(910/999):loss = 0.248416\n",
      "Gradient Descent(911/999):loss = 0.248416\n",
      "Gradient Descent(912/999):loss = 0.248416\n",
      "Gradient Descent(913/999):loss = 0.248416\n",
      "Gradient Descent(914/999):loss = 0.248416\n",
      "Gradient Descent(915/999):loss = 0.248416\n",
      "Gradient Descent(916/999):loss = 0.248416\n",
      "Gradient Descent(917/999):loss = 0.248416\n",
      "Gradient Descent(918/999):loss = 0.248416\n",
      "Gradient Descent(919/999):loss = 0.248416\n",
      "Gradient Descent(920/999):loss = 0.248416\n",
      "Gradient Descent(921/999):loss = 0.248416\n",
      "Gradient Descent(922/999):loss = 0.248416\n",
      "Gradient Descent(923/999):loss = 0.24842\n",
      "Gradient Descent(924/999):loss = 0.24842\n",
      "Gradient Descent(925/999):loss = 0.248416\n",
      "Gradient Descent(926/999):loss = 0.248416\n",
      "Gradient Descent(927/999):loss = 0.248416\n",
      "Gradient Descent(928/999):loss = 0.248416\n",
      "Gradient Descent(929/999):loss = 0.248416\n",
      "Gradient Descent(930/999):loss = 0.248416\n",
      "Gradient Descent(931/999):loss = 0.248416\n",
      "Gradient Descent(932/999):loss = 0.248416\n",
      "Gradient Descent(933/999):loss = 0.248416\n",
      "Gradient Descent(934/999):loss = 0.24842\n",
      "Gradient Descent(935/999):loss = 0.24842\n",
      "Gradient Descent(936/999):loss = 0.24842\n",
      "Gradient Descent(937/999):loss = 0.24842\n",
      "Gradient Descent(938/999):loss = 0.24842\n",
      "Gradient Descent(939/999):loss = 0.24842\n",
      "Gradient Descent(940/999):loss = 0.24842\n",
      "Gradient Descent(941/999):loss = 0.24842\n",
      "Gradient Descent(942/999):loss = 0.248416\n",
      "Gradient Descent(943/999):loss = 0.248416\n",
      "Gradient Descent(944/999):loss = 0.24842\n",
      "Gradient Descent(945/999):loss = 0.24842\n",
      "Gradient Descent(946/999):loss = 0.24842\n",
      "Gradient Descent(947/999):loss = 0.24842\n",
      "Gradient Descent(948/999):loss = 0.24842\n",
      "Gradient Descent(949/999):loss = 0.24842\n",
      "Gradient Descent(950/999):loss = 0.248412\n",
      "Gradient Descent(951/999):loss = 0.248412\n",
      "Gradient Descent(952/999):loss = 0.248412\n",
      "Gradient Descent(953/999):loss = 0.248412\n",
      "Gradient Descent(954/999):loss = 0.248412\n",
      "Gradient Descent(955/999):loss = 0.248412\n",
      "Gradient Descent(956/999):loss = 0.248408\n",
      "Gradient Descent(957/999):loss = 0.248404\n",
      "Gradient Descent(958/999):loss = 0.248404\n",
      "Gradient Descent(959/999):loss = 0.248404\n",
      "Gradient Descent(960/999):loss = 0.248404\n",
      "Gradient Descent(961/999):loss = 0.248404\n",
      "Gradient Descent(962/999):loss = 0.248404\n",
      "Gradient Descent(963/999):loss = 0.248404\n",
      "Gradient Descent(964/999):loss = 0.248404\n",
      "Gradient Descent(965/999):loss = 0.248408\n",
      "Gradient Descent(966/999):loss = 0.248408\n",
      "Gradient Descent(967/999):loss = 0.248408\n",
      "Gradient Descent(968/999):loss = 0.248408\n",
      "Gradient Descent(969/999):loss = 0.2484\n",
      "Gradient Descent(970/999):loss = 0.2484\n",
      "Gradient Descent(971/999):loss = 0.2484\n",
      "Gradient Descent(972/999):loss = 0.248404\n",
      "Gradient Descent(973/999):loss = 0.248404\n",
      "Gradient Descent(974/999):loss = 0.248404\n",
      "Gradient Descent(975/999):loss = 0.248404\n",
      "Gradient Descent(976/999):loss = 0.248404\n",
      "Gradient Descent(977/999):loss = 0.248404\n",
      "Gradient Descent(978/999):loss = 0.248404\n",
      "Gradient Descent(979/999):loss = 0.248404\n",
      "Gradient Descent(980/999):loss = 0.248404\n",
      "Gradient Descent(981/999):loss = 0.248408\n",
      "Gradient Descent(982/999):loss = 0.248408\n",
      "Gradient Descent(983/999):loss = 0.248408\n",
      "Gradient Descent(984/999):loss = 0.248408\n",
      "Gradient Descent(985/999):loss = 0.248408\n",
      "Gradient Descent(986/999):loss = 0.248408\n",
      "Gradient Descent(987/999):loss = 0.248404\n",
      "Gradient Descent(988/999):loss = 0.248404\n",
      "Gradient Descent(989/999):loss = 0.248404\n",
      "Gradient Descent(990/999):loss = 0.248408\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(991/999):loss = 0.248408\n",
      "Gradient Descent(992/999):loss = 0.248408\n",
      "Gradient Descent(993/999):loss = 0.248408\n",
      "Gradient Descent(994/999):loss = 0.248408\n",
      "Gradient Descent(995/999):loss = 0.248412\n",
      "Gradient Descent(996/999):loss = 0.248412\n",
      "Gradient Descent(997/999):loss = 0.248412\n",
      "Gradient Descent(998/999):loss = 0.248412\n",
      "Gradient Descent(999/999):loss = 0.248412\n"
     ]
    }
   ],
   "source": [
    "w0 = np.zeros((x_all_int.shape[1],1))\n",
    "weights, loss = logistic_regression(y,x_all_int,w0,max_iters= 1000,  printing = True, gamma = 0.000005 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<function Regressions.pred_logistic>,\n",
       " array([-8.59719516e-01, -3.10398077e-02, -6.98315178e-01, -7.48188127e-01,\n",
       "         2.84356006e-01, -7.69243480e-03,  4.14745357e-01,  4.32831241e-02,\n",
       "         8.83859082e-01, -9.56927041e-02,  1.13644547e-01, -5.27008149e-01,\n",
       "         2.78513816e-01,  2.03438374e-01,  5.15337678e-01, -3.08028537e-03,\n",
       "        -2.85934184e-03,  7.04525929e-01, -1.92083766e-03,  7.05946088e-03,\n",
       "         1.61979545e-01,  7.21970131e-04, -1.23938409e-01,  5.23153001e-02,\n",
       "        -1.65107805e-01,  5.56621553e-04,  9.21699176e-05, -1.12148132e-01,\n",
       "         5.19658443e-03, -3.50652223e-03, -1.42280335e-01]),\n",
       " 0.25034844444444443,\n",
       " 0.2503639999999999)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_validation(y,x_all_int,10, logistic_regression,w0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test, x_all_test, ids_test = load_csv_data('../Data/test.csv')\n",
    "x_all_test = imputation(x_all_test,method = \"median\")\n",
    "x_all_test = standardize_data(x_all_test)\n",
    "x_all_test = np.hstack((np.ones((x_all_test.shape[0],1)),x_all_test))\n",
    "\n",
    "y_hat = pred_logistic(x_all_test,weights)\n",
    "\n",
    "#create_csv_submission(ids_test, y_test, \"../Data/test_Charles_num_jet.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_cat = categories(y_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_cat[y_cat == 0]=-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_csv_submission(ids_test, y_cat, \"../Data/test_logit_no_transformation.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([-1.06979771e+00,  2.85547445e-01, -7.93056176e-01, -9.62257096e-01,\n",
       "         2.72204740e-01, -1.96613420e-01,  1.38276181e+00, -2.27016962e-01,\n",
       "         1.02715045e+00, -1.02121262e-01, -2.00195754e-01, -6.02977452e-01,\n",
       "         2.29164760e-01, -2.04591912e-01,  7.60091068e-01, -4.91786401e-03,\n",
       "        -4.61720611e-03,  8.56184541e-01, -2.03856002e-03,  8.75315141e-03,\n",
       "         2.71155913e-01,  1.30596316e-03, -3.41221012e-01, -7.00607755e-01,\n",
       "         9.29427287e-02,  1.10824263e-01,  1.10821135e-01, -2.17464019e-01,\n",
       "        -2.07325312e-01, -2.07785073e-01, -6.02869870e-01]), 0.288152)"
      ]
     },
     "execution_count": 272,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg_logistic_regression(y,x_all_int,lambda_ = 0.5, initial_w = w0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# test for william function: multi_cross_validation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(0/999):loss = 0.303136\n",
      "Gradient Descent(1/999):loss = 0.267752\n",
      "Gradient Descent(2/999):loss = 0.27942\n",
      "Gradient Descent(3/999):loss = 0.267744\n",
      "Gradient Descent(4/999):loss = 0.270848\n",
      "Gradient Descent(5/999):loss = 0.26718\n",
      "Gradient Descent(6/999):loss = 0.266992\n",
      "Gradient Descent(7/999):loss = 0.265508\n",
      "Gradient Descent(8/999):loss = 0.264968\n",
      "Gradient Descent(9/999):loss = 0.264084\n",
      "Gradient Descent(10/999):loss = 0.263292\n",
      "Gradient Descent(11/999):loss = 0.262512\n",
      "Gradient Descent(12/999):loss = 0.261748\n",
      "Gradient Descent(13/999):loss = 0.261316\n",
      "Gradient Descent(14/999):loss = 0.260564\n",
      "Gradient Descent(15/999):loss = 0.25992\n",
      "Gradient Descent(16/999):loss = 0.25938\n",
      "Gradient Descent(17/999):loss = 0.258908\n",
      "Gradient Descent(18/999):loss = 0.258428\n",
      "Gradient Descent(19/999):loss = 0.258092\n",
      "Gradient Descent(20/999):loss = 0.257608\n",
      "Gradient Descent(21/999):loss = 0.257364\n",
      "Gradient Descent(22/999):loss = 0.25706\n",
      "Gradient Descent(23/999):loss = 0.256816\n",
      "Gradient Descent(24/999):loss = 0.256548\n",
      "Gradient Descent(25/999):loss = 0.2563\n",
      "Gradient Descent(26/999):loss = 0.256136\n",
      "Gradient Descent(27/999):loss = 0.255764\n",
      "Gradient Descent(28/999):loss = 0.25542\n",
      "Gradient Descent(29/999):loss = 0.25518\n",
      "Gradient Descent(30/999):loss = 0.254844\n",
      "Gradient Descent(31/999):loss = 0.25478\n",
      "Gradient Descent(32/999):loss = 0.25458\n",
      "Gradient Descent(33/999):loss = 0.254464\n",
      "Gradient Descent(34/999):loss = 0.254324\n",
      "Gradient Descent(35/999):loss = 0.254152\n",
      "Gradient Descent(36/999):loss = 0.254084\n",
      "Gradient Descent(37/999):loss = 0.254\n",
      "Gradient Descent(38/999):loss = 0.25386\n",
      "Gradient Descent(39/999):loss = 0.253652\n",
      "Gradient Descent(40/999):loss = 0.253516\n",
      "Gradient Descent(41/999):loss = 0.253448\n",
      "Gradient Descent(42/999):loss = 0.253296\n",
      "Gradient Descent(43/999):loss = 0.253184\n",
      "Gradient Descent(44/999):loss = 0.253124\n",
      "Gradient Descent(45/999):loss = 0.253044\n",
      "Gradient Descent(46/999):loss = 0.252916\n",
      "Gradient Descent(47/999):loss = 0.2528\n",
      "Gradient Descent(48/999):loss = 0.25276\n",
      "Gradient Descent(49/999):loss = 0.252708\n",
      "Gradient Descent(50/999):loss = 0.252604\n",
      "Gradient Descent(51/999):loss = 0.252524\n",
      "Gradient Descent(52/999):loss = 0.252368\n",
      "Gradient Descent(53/999):loss = 0.25224\n",
      "Gradient Descent(54/999):loss = 0.25204\n",
      "Gradient Descent(55/999):loss = 0.252\n",
      "Gradient Descent(56/999):loss = 0.25194\n",
      "Gradient Descent(57/999):loss = 0.251772\n",
      "Gradient Descent(58/999):loss = 0.25166\n",
      "Gradient Descent(59/999):loss = 0.251568\n",
      "Gradient Descent(60/999):loss = 0.251472\n",
      "Gradient Descent(61/999):loss = 0.251388\n",
      "Gradient Descent(62/999):loss = 0.251372\n",
      "Gradient Descent(63/999):loss = 0.251356\n",
      "Gradient Descent(64/999):loss = 0.251276\n",
      "Gradient Descent(65/999):loss = 0.251248\n",
      "Gradient Descent(66/999):loss = 0.251136\n",
      "Gradient Descent(67/999):loss = 0.251112\n",
      "Gradient Descent(68/999):loss = 0.251052\n",
      "Gradient Descent(69/999):loss = 0.25102\n",
      "Gradient Descent(70/999):loss = 0.251012\n",
      "Gradient Descent(71/999):loss = 0.251044\n",
      "Gradient Descent(72/999):loss = 0.250936\n",
      "Gradient Descent(73/999):loss = 0.250928\n",
      "Gradient Descent(74/999):loss = 0.250856\n",
      "Gradient Descent(75/999):loss = 0.250824\n",
      "Gradient Descent(76/999):loss = 0.250788\n",
      "Gradient Descent(77/999):loss = 0.250736\n",
      "Gradient Descent(78/999):loss = 0.250632\n",
      "Gradient Descent(79/999):loss = 0.250644\n",
      "Gradient Descent(80/999):loss = 0.2506\n",
      "Gradient Descent(81/999):loss = 0.250504\n",
      "Gradient Descent(82/999):loss = 0.250516\n",
      "Gradient Descent(83/999):loss = 0.250508\n",
      "Gradient Descent(84/999):loss = 0.25052\n",
      "Gradient Descent(85/999):loss = 0.250476\n",
      "Gradient Descent(86/999):loss = 0.250396\n",
      "Gradient Descent(87/999):loss = 0.250356\n",
      "Gradient Descent(88/999):loss = 0.250304\n",
      "Gradient Descent(89/999):loss = 0.250316\n",
      "Gradient Descent(90/999):loss = 0.250284\n",
      "Gradient Descent(91/999):loss = 0.25026\n",
      "Gradient Descent(92/999):loss = 0.250244\n",
      "Gradient Descent(93/999):loss = 0.250276\n",
      "Gradient Descent(94/999):loss = 0.2503\n",
      "Gradient Descent(95/999):loss = 0.250256\n",
      "Gradient Descent(96/999):loss = 0.250244\n",
      "Gradient Descent(97/999):loss = 0.250248\n",
      "Gradient Descent(98/999):loss = 0.25022\n",
      "Gradient Descent(99/999):loss = 0.250248\n",
      "Gradient Descent(100/999):loss = 0.250272\n",
      "Gradient Descent(101/999):loss = 0.250268\n",
      "Gradient Descent(102/999):loss = 0.250228\n",
      "Gradient Descent(103/999):loss = 0.250188\n",
      "Gradient Descent(104/999):loss = 0.25016\n",
      "Gradient Descent(105/999):loss = 0.250148\n",
      "Gradient Descent(106/999):loss = 0.250128\n",
      "Gradient Descent(107/999):loss = 0.250096\n",
      "Gradient Descent(108/999):loss = 0.25008\n",
      "Gradient Descent(109/999):loss = 0.250056\n",
      "Gradient Descent(110/999):loss = 0.250024\n",
      "Gradient Descent(111/999):loss = 0.250008\n",
      "Gradient Descent(112/999):loss = 0.249996\n",
      "Gradient Descent(113/999):loss = 0.249996\n",
      "Gradient Descent(114/999):loss = 0.249976\n",
      "Gradient Descent(115/999):loss = 0.249976\n",
      "Gradient Descent(116/999):loss = 0.249964\n",
      "Gradient Descent(117/999):loss = 0.24994\n",
      "Gradient Descent(118/999):loss = 0.249904\n",
      "Gradient Descent(119/999):loss = 0.249868\n",
      "Gradient Descent(120/999):loss = 0.249884\n",
      "Gradient Descent(121/999):loss = 0.249888\n",
      "Gradient Descent(122/999):loss = 0.249876\n",
      "Gradient Descent(123/999):loss = 0.2499\n",
      "Gradient Descent(124/999):loss = 0.249928\n",
      "Gradient Descent(125/999):loss = 0.249904\n",
      "Gradient Descent(126/999):loss = 0.249884\n",
      "Gradient Descent(127/999):loss = 0.249832\n",
      "Gradient Descent(128/999):loss = 0.249832\n",
      "Gradient Descent(129/999):loss = 0.249812\n",
      "Gradient Descent(130/999):loss = 0.24976\n",
      "Gradient Descent(131/999):loss = 0.2498\n",
      "Gradient Descent(132/999):loss = 0.249796\n",
      "Gradient Descent(133/999):loss = 0.2498\n",
      "Gradient Descent(134/999):loss = 0.24978\n",
      "Gradient Descent(135/999):loss = 0.24976\n",
      "Gradient Descent(136/999):loss = 0.249768\n",
      "Gradient Descent(137/999):loss = 0.249768\n",
      "Gradient Descent(138/999):loss = 0.24974\n",
      "Gradient Descent(139/999):loss = 0.24972\n",
      "Gradient Descent(140/999):loss = 0.249696\n",
      "Gradient Descent(141/999):loss = 0.249672\n",
      "Gradient Descent(142/999):loss = 0.249644\n",
      "Gradient Descent(143/999):loss = 0.24964\n",
      "Gradient Descent(144/999):loss = 0.249584\n",
      "Gradient Descent(145/999):loss = 0.24958\n",
      "Gradient Descent(146/999):loss = 0.24954\n",
      "Gradient Descent(147/999):loss = 0.249532\n",
      "Gradient Descent(148/999):loss = 0.249488\n",
      "Gradient Descent(149/999):loss = 0.249468\n",
      "Gradient Descent(150/999):loss = 0.249468\n",
      "Gradient Descent(151/999):loss = 0.249492\n",
      "Gradient Descent(152/999):loss = 0.249448\n",
      "Gradient Descent(153/999):loss = 0.249448\n",
      "Gradient Descent(154/999):loss = 0.24944\n",
      "Gradient Descent(155/999):loss = 0.249416\n",
      "Gradient Descent(156/999):loss = 0.249392\n",
      "Gradient Descent(157/999):loss = 0.249392\n",
      "Gradient Descent(158/999):loss = 0.249412\n",
      "Gradient Descent(159/999):loss = 0.249372\n",
      "Gradient Descent(160/999):loss = 0.24936\n",
      "Gradient Descent(161/999):loss = 0.249324\n",
      "Gradient Descent(162/999):loss = 0.249272\n",
      "Gradient Descent(163/999):loss = 0.24924\n",
      "Gradient Descent(164/999):loss = 0.249244\n",
      "Gradient Descent(165/999):loss = 0.249224\n",
      "Gradient Descent(166/999):loss = 0.2492\n",
      "Gradient Descent(167/999):loss = 0.24918\n",
      "Gradient Descent(168/999):loss = 0.249164\n",
      "Gradient Descent(169/999):loss = 0.24914\n",
      "Gradient Descent(170/999):loss = 0.249164\n",
      "Gradient Descent(171/999):loss = 0.249156\n",
      "Gradient Descent(172/999):loss = 0.249156\n",
      "Gradient Descent(173/999):loss = 0.249152\n",
      "Gradient Descent(174/999):loss = 0.249156\n",
      "Gradient Descent(175/999):loss = 0.249168\n",
      "Gradient Descent(176/999):loss = 0.249148\n",
      "Gradient Descent(177/999):loss = 0.24912\n",
      "Gradient Descent(178/999):loss = 0.249144\n",
      "Gradient Descent(179/999):loss = 0.24914\n",
      "Gradient Descent(180/999):loss = 0.249136\n",
      "Gradient Descent(181/999):loss = 0.249144\n",
      "Gradient Descent(182/999):loss = 0.249148\n",
      "Gradient Descent(183/999):loss = 0.249124\n",
      "Gradient Descent(184/999):loss = 0.249116\n",
      "Gradient Descent(185/999):loss = 0.249104\n",
      "Gradient Descent(186/999):loss = 0.249132\n",
      "Gradient Descent(187/999):loss = 0.249128\n",
      "Gradient Descent(188/999):loss = 0.249104\n",
      "Gradient Descent(189/999):loss = 0.249092\n",
      "Gradient Descent(190/999):loss = 0.249052\n",
      "Gradient Descent(191/999):loss = 0.24906\n",
      "Gradient Descent(192/999):loss = 0.248996\n",
      "Gradient Descent(193/999):loss = 0.249004\n",
      "Gradient Descent(194/999):loss = 0.248984\n",
      "Gradient Descent(195/999):loss = 0.24898\n",
      "Gradient Descent(196/999):loss = 0.248972\n",
      "Gradient Descent(197/999):loss = 0.248964\n",
      "Gradient Descent(198/999):loss = 0.248976\n",
      "Gradient Descent(199/999):loss = 0.248972\n",
      "Gradient Descent(200/999):loss = 0.248952\n",
      "Gradient Descent(201/999):loss = 0.248956\n",
      "Gradient Descent(202/999):loss = 0.248928\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(203/999):loss = 0.248936\n",
      "Gradient Descent(204/999):loss = 0.248944\n",
      "Gradient Descent(205/999):loss = 0.248916\n",
      "Gradient Descent(206/999):loss = 0.248912\n",
      "Gradient Descent(207/999):loss = 0.24894\n",
      "Gradient Descent(208/999):loss = 0.24894\n",
      "Gradient Descent(209/999):loss = 0.248964\n",
      "Gradient Descent(210/999):loss = 0.248964\n",
      "Gradient Descent(211/999):loss = 0.248944\n",
      "Gradient Descent(212/999):loss = 0.248928\n",
      "Gradient Descent(213/999):loss = 0.248936\n",
      "Gradient Descent(214/999):loss = 0.248944\n",
      "Gradient Descent(215/999):loss = 0.24894\n",
      "Gradient Descent(216/999):loss = 0.248928\n",
      "Gradient Descent(217/999):loss = 0.2489\n",
      "Gradient Descent(218/999):loss = 0.248888\n",
      "Gradient Descent(219/999):loss = 0.248864\n",
      "Gradient Descent(220/999):loss = 0.24886\n",
      "Gradient Descent(221/999):loss = 0.248836\n",
      "Gradient Descent(222/999):loss = 0.24884\n",
      "Gradient Descent(223/999):loss = 0.248856\n",
      "Gradient Descent(224/999):loss = 0.248824\n",
      "Gradient Descent(225/999):loss = 0.248816\n",
      "Gradient Descent(226/999):loss = 0.248824\n",
      "Gradient Descent(227/999):loss = 0.248832\n",
      "Gradient Descent(228/999):loss = 0.248824\n",
      "Gradient Descent(229/999):loss = 0.248808\n",
      "Gradient Descent(230/999):loss = 0.248816\n",
      "Gradient Descent(231/999):loss = 0.248816\n",
      "Gradient Descent(232/999):loss = 0.248804\n",
      "Gradient Descent(233/999):loss = 0.248788\n",
      "Gradient Descent(234/999):loss = 0.248796\n",
      "Gradient Descent(235/999):loss = 0.248808\n",
      "Gradient Descent(236/999):loss = 0.248812\n",
      "Gradient Descent(237/999):loss = 0.2488\n",
      "Gradient Descent(238/999):loss = 0.248784\n",
      "Gradient Descent(239/999):loss = 0.248768\n",
      "Gradient Descent(240/999):loss = 0.248756\n",
      "Gradient Descent(241/999):loss = 0.248752\n",
      "Gradient Descent(242/999):loss = 0.248752\n",
      "Gradient Descent(243/999):loss = 0.24876\n",
      "Gradient Descent(244/999):loss = 0.248748\n",
      "Gradient Descent(245/999):loss = 0.24876\n",
      "Gradient Descent(246/999):loss = 0.248768\n",
      "Gradient Descent(247/999):loss = 0.24878\n",
      "Gradient Descent(248/999):loss = 0.248788\n",
      "Gradient Descent(249/999):loss = 0.248768\n",
      "Gradient Descent(250/999):loss = 0.248756\n",
      "Gradient Descent(251/999):loss = 0.248756\n",
      "Gradient Descent(252/999):loss = 0.248752\n",
      "Gradient Descent(253/999):loss = 0.248756\n",
      "Gradient Descent(254/999):loss = 0.248732\n",
      "Gradient Descent(255/999):loss = 0.248752\n",
      "Gradient Descent(256/999):loss = 0.248748\n",
      "Gradient Descent(257/999):loss = 0.248736\n",
      "Gradient Descent(258/999):loss = 0.248716\n",
      "Gradient Descent(259/999):loss = 0.248716\n",
      "Gradient Descent(260/999):loss = 0.248732\n",
      "Gradient Descent(261/999):loss = 0.248736\n",
      "Gradient Descent(262/999):loss = 0.248744\n",
      "Gradient Descent(263/999):loss = 0.248752\n",
      "Gradient Descent(264/999):loss = 0.248748\n",
      "Gradient Descent(265/999):loss = 0.24874\n",
      "Gradient Descent(266/999):loss = 0.248724\n",
      "Gradient Descent(267/999):loss = 0.248724\n",
      "Gradient Descent(268/999):loss = 0.24872\n",
      "Gradient Descent(269/999):loss = 0.2487\n",
      "Gradient Descent(270/999):loss = 0.248704\n",
      "Gradient Descent(271/999):loss = 0.248688\n",
      "Gradient Descent(272/999):loss = 0.248668\n",
      "Gradient Descent(273/999):loss = 0.248672\n",
      "Gradient Descent(274/999):loss = 0.248672\n",
      "Gradient Descent(275/999):loss = 0.248672\n",
      "Gradient Descent(276/999):loss = 0.248664\n",
      "Gradient Descent(277/999):loss = 0.248652\n",
      "Gradient Descent(278/999):loss = 0.248648\n",
      "Gradient Descent(279/999):loss = 0.248664\n",
      "Gradient Descent(280/999):loss = 0.248672\n",
      "Gradient Descent(281/999):loss = 0.248668\n",
      "Gradient Descent(282/999):loss = 0.248668\n",
      "Gradient Descent(283/999):loss = 0.248672\n",
      "Gradient Descent(284/999):loss = 0.248676\n",
      "Gradient Descent(285/999):loss = 0.248668\n",
      "Gradient Descent(286/999):loss = 0.248672\n",
      "Gradient Descent(287/999):loss = 0.248668\n",
      "Gradient Descent(288/999):loss = 0.248668\n",
      "Gradient Descent(289/999):loss = 0.248676\n",
      "Gradient Descent(290/999):loss = 0.248672\n",
      "Gradient Descent(291/999):loss = 0.248656\n",
      "Gradient Descent(292/999):loss = 0.248644\n",
      "Gradient Descent(293/999):loss = 0.248632\n",
      "Gradient Descent(294/999):loss = 0.248624\n",
      "Gradient Descent(295/999):loss = 0.248632\n",
      "Gradient Descent(296/999):loss = 0.248636\n",
      "Gradient Descent(297/999):loss = 0.248628\n",
      "Gradient Descent(298/999):loss = 0.248628\n",
      "Gradient Descent(299/999):loss = 0.248616\n",
      "Gradient Descent(300/999):loss = 0.248612\n",
      "Gradient Descent(301/999):loss = 0.248624\n",
      "Gradient Descent(302/999):loss = 0.248616\n",
      "Gradient Descent(303/999):loss = 0.248608\n",
      "Gradient Descent(304/999):loss = 0.248608\n",
      "Gradient Descent(305/999):loss = 0.248608\n",
      "Gradient Descent(306/999):loss = 0.248596\n",
      "Gradient Descent(307/999):loss = 0.248588\n",
      "Gradient Descent(308/999):loss = 0.248592\n",
      "Gradient Descent(309/999):loss = 0.248568\n",
      "Gradient Descent(310/999):loss = 0.248572\n",
      "Gradient Descent(311/999):loss = 0.248588\n",
      "Gradient Descent(312/999):loss = 0.24858\n",
      "Gradient Descent(313/999):loss = 0.248584\n",
      "Gradient Descent(314/999):loss = 0.24858\n",
      "Gradient Descent(315/999):loss = 0.248568\n",
      "Gradient Descent(316/999):loss = 0.248564\n",
      "Gradient Descent(317/999):loss = 0.248568\n",
      "Gradient Descent(318/999):loss = 0.248556\n",
      "Gradient Descent(319/999):loss = 0.248552\n",
      "Gradient Descent(320/999):loss = 0.248568\n",
      "Gradient Descent(321/999):loss = 0.248572\n",
      "Gradient Descent(322/999):loss = 0.248572\n",
      "Gradient Descent(323/999):loss = 0.248576\n",
      "Gradient Descent(324/999):loss = 0.248584\n",
      "Gradient Descent(325/999):loss = 0.248564\n",
      "Gradient Descent(326/999):loss = 0.24856\n",
      "Gradient Descent(327/999):loss = 0.248556\n",
      "Gradient Descent(328/999):loss = 0.248552\n",
      "Gradient Descent(329/999):loss = 0.248544\n",
      "Gradient Descent(330/999):loss = 0.248548\n",
      "Gradient Descent(331/999):loss = 0.248544\n",
      "Gradient Descent(332/999):loss = 0.248556\n",
      "Gradient Descent(333/999):loss = 0.248572\n",
      "Gradient Descent(334/999):loss = 0.248584\n",
      "Gradient Descent(335/999):loss = 0.24858\n",
      "Gradient Descent(336/999):loss = 0.24858\n",
      "Gradient Descent(337/999):loss = 0.248568\n",
      "Gradient Descent(338/999):loss = 0.248564\n",
      "Gradient Descent(339/999):loss = 0.24856\n",
      "Gradient Descent(340/999):loss = 0.248552\n",
      "Gradient Descent(341/999):loss = 0.24854\n",
      "Gradient Descent(342/999):loss = 0.248548\n",
      "Gradient Descent(343/999):loss = 0.248556\n",
      "Gradient Descent(344/999):loss = 0.248536\n",
      "Gradient Descent(345/999):loss = 0.24854\n",
      "Gradient Descent(346/999):loss = 0.24856\n",
      "Gradient Descent(347/999):loss = 0.24856\n",
      "Gradient Descent(348/999):loss = 0.248552\n",
      "Gradient Descent(349/999):loss = 0.248552\n",
      "Gradient Descent(350/999):loss = 0.248544\n",
      "Gradient Descent(351/999):loss = 0.248552\n",
      "Gradient Descent(352/999):loss = 0.24854\n",
      "Gradient Descent(353/999):loss = 0.24854\n",
      "Gradient Descent(354/999):loss = 0.248532\n",
      "Gradient Descent(355/999):loss = 0.248536\n",
      "Gradient Descent(356/999):loss = 0.248536\n",
      "Gradient Descent(357/999):loss = 0.248536\n",
      "Gradient Descent(358/999):loss = 0.248552\n",
      "Gradient Descent(359/999):loss = 0.248548\n",
      "Gradient Descent(360/999):loss = 0.248544\n",
      "Gradient Descent(361/999):loss = 0.248548\n",
      "Gradient Descent(362/999):loss = 0.248548\n",
      "Gradient Descent(363/999):loss = 0.248544\n",
      "Gradient Descent(364/999):loss = 0.248548\n",
      "Gradient Descent(365/999):loss = 0.248552\n",
      "Gradient Descent(366/999):loss = 0.248552\n",
      "Gradient Descent(367/999):loss = 0.248532\n",
      "Gradient Descent(368/999):loss = 0.248528\n",
      "Gradient Descent(369/999):loss = 0.248516\n",
      "Gradient Descent(370/999):loss = 0.248508\n",
      "Gradient Descent(371/999):loss = 0.248504\n",
      "Gradient Descent(372/999):loss = 0.248508\n",
      "Gradient Descent(373/999):loss = 0.248508\n",
      "Gradient Descent(374/999):loss = 0.248496\n",
      "Gradient Descent(375/999):loss = 0.248488\n",
      "Gradient Descent(376/999):loss = 0.248496\n",
      "Gradient Descent(377/999):loss = 0.248492\n",
      "Gradient Descent(378/999):loss = 0.248496\n",
      "Gradient Descent(379/999):loss = 0.248496\n",
      "Gradient Descent(380/999):loss = 0.248488\n",
      "Gradient Descent(381/999):loss = 0.248484\n",
      "Gradient Descent(382/999):loss = 0.24848\n",
      "Gradient Descent(383/999):loss = 0.248484\n",
      "Gradient Descent(384/999):loss = 0.24848\n",
      "Gradient Descent(385/999):loss = 0.248476\n",
      "Gradient Descent(386/999):loss = 0.248476\n",
      "Gradient Descent(387/999):loss = 0.248476\n",
      "Gradient Descent(388/999):loss = 0.248488\n",
      "Gradient Descent(389/999):loss = 0.248484\n",
      "Gradient Descent(390/999):loss = 0.248484\n",
      "Gradient Descent(391/999):loss = 0.248468\n",
      "Gradient Descent(392/999):loss = 0.248468\n",
      "Gradient Descent(393/999):loss = 0.24846\n",
      "Gradient Descent(394/999):loss = 0.248448\n",
      "Gradient Descent(395/999):loss = 0.248452\n",
      "Gradient Descent(396/999):loss = 0.248444\n",
      "Gradient Descent(397/999):loss = 0.248448\n",
      "Gradient Descent(398/999):loss = 0.248452\n",
      "Gradient Descent(399/999):loss = 0.248448\n",
      "Gradient Descent(400/999):loss = 0.24844\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(401/999):loss = 0.24844\n",
      "Gradient Descent(402/999):loss = 0.248436\n",
      "Gradient Descent(403/999):loss = 0.248432\n",
      "Gradient Descent(404/999):loss = 0.248432\n",
      "Gradient Descent(405/999):loss = 0.248448\n",
      "Gradient Descent(406/999):loss = 0.248452\n",
      "Gradient Descent(407/999):loss = 0.24846\n",
      "Gradient Descent(408/999):loss = 0.248456\n",
      "Gradient Descent(409/999):loss = 0.24846\n",
      "Gradient Descent(410/999):loss = 0.248456\n",
      "Gradient Descent(411/999):loss = 0.24846\n",
      "Gradient Descent(412/999):loss = 0.248448\n",
      "Gradient Descent(413/999):loss = 0.24846\n",
      "Gradient Descent(414/999):loss = 0.248432\n",
      "Gradient Descent(415/999):loss = 0.248444\n",
      "Gradient Descent(416/999):loss = 0.248448\n",
      "Gradient Descent(417/999):loss = 0.248444\n",
      "Gradient Descent(418/999):loss = 0.24844\n",
      "Gradient Descent(419/999):loss = 0.248436\n",
      "Gradient Descent(420/999):loss = 0.248436\n",
      "Gradient Descent(421/999):loss = 0.248448\n",
      "Gradient Descent(422/999):loss = 0.248452\n",
      "Gradient Descent(423/999):loss = 0.248468\n",
      "Gradient Descent(424/999):loss = 0.248468\n",
      "Gradient Descent(425/999):loss = 0.248468\n",
      "Gradient Descent(426/999):loss = 0.24846\n",
      "Gradient Descent(427/999):loss = 0.248464\n",
      "Gradient Descent(428/999):loss = 0.248464\n",
      "Gradient Descent(429/999):loss = 0.248464\n",
      "Gradient Descent(430/999):loss = 0.248472\n",
      "Gradient Descent(431/999):loss = 0.248476\n",
      "Gradient Descent(432/999):loss = 0.248476\n",
      "Gradient Descent(433/999):loss = 0.248476\n",
      "Gradient Descent(434/999):loss = 0.248484\n",
      "Gradient Descent(435/999):loss = 0.24848\n",
      "Gradient Descent(436/999):loss = 0.248476\n",
      "Gradient Descent(437/999):loss = 0.248484\n",
      "Gradient Descent(438/999):loss = 0.248484\n",
      "Gradient Descent(439/999):loss = 0.248484\n",
      "Gradient Descent(440/999):loss = 0.248484\n",
      "Gradient Descent(441/999):loss = 0.248476\n",
      "Gradient Descent(442/999):loss = 0.248472\n",
      "Gradient Descent(443/999):loss = 0.248476\n",
      "Gradient Descent(444/999):loss = 0.248468\n",
      "Gradient Descent(445/999):loss = 0.248468\n",
      "Gradient Descent(446/999):loss = 0.248468\n",
      "Gradient Descent(447/999):loss = 0.248468\n",
      "Gradient Descent(448/999):loss = 0.24846\n",
      "Gradient Descent(449/999):loss = 0.248452\n",
      "Gradient Descent(450/999):loss = 0.248448\n",
      "Gradient Descent(451/999):loss = 0.248448\n",
      "Gradient Descent(452/999):loss = 0.248452\n",
      "Gradient Descent(453/999):loss = 0.248452\n",
      "Gradient Descent(454/999):loss = 0.248448\n",
      "Gradient Descent(455/999):loss = 0.248448\n",
      "Gradient Descent(456/999):loss = 0.248452\n",
      "Gradient Descent(457/999):loss = 0.24846\n",
      "Gradient Descent(458/999):loss = 0.248464\n",
      "Gradient Descent(459/999):loss = 0.248456\n",
      "Gradient Descent(460/999):loss = 0.248452\n",
      "Gradient Descent(461/999):loss = 0.248452\n",
      "Gradient Descent(462/999):loss = 0.24846\n",
      "Gradient Descent(463/999):loss = 0.248456\n",
      "Gradient Descent(464/999):loss = 0.248456\n",
      "Gradient Descent(465/999):loss = 0.248452\n",
      "Gradient Descent(466/999):loss = 0.248444\n",
      "Gradient Descent(467/999):loss = 0.248444\n",
      "Gradient Descent(468/999):loss = 0.248448\n",
      "Gradient Descent(469/999):loss = 0.248452\n",
      "Gradient Descent(470/999):loss = 0.248452\n",
      "Gradient Descent(471/999):loss = 0.248456\n",
      "Gradient Descent(472/999):loss = 0.248468\n",
      "Gradient Descent(473/999):loss = 0.248468\n",
      "Gradient Descent(474/999):loss = 0.248472\n",
      "Gradient Descent(475/999):loss = 0.248476\n",
      "Gradient Descent(476/999):loss = 0.248476\n",
      "Gradient Descent(477/999):loss = 0.248476\n",
      "Gradient Descent(478/999):loss = 0.248472\n",
      "Gradient Descent(479/999):loss = 0.248468\n",
      "Gradient Descent(480/999):loss = 0.248472\n",
      "Gradient Descent(481/999):loss = 0.248476\n",
      "Gradient Descent(482/999):loss = 0.248476\n",
      "Gradient Descent(483/999):loss = 0.24848\n",
      "Gradient Descent(484/999):loss = 0.24848\n",
      "Gradient Descent(485/999):loss = 0.24848\n",
      "Gradient Descent(486/999):loss = 0.24848\n",
      "Gradient Descent(487/999):loss = 0.248484\n",
      "Gradient Descent(488/999):loss = 0.248484\n",
      "Gradient Descent(489/999):loss = 0.248484\n",
      "Gradient Descent(490/999):loss = 0.248484\n",
      "Gradient Descent(491/999):loss = 0.248476\n",
      "Gradient Descent(492/999):loss = 0.248476\n",
      "Gradient Descent(493/999):loss = 0.248472\n",
      "Gradient Descent(494/999):loss = 0.248472\n",
      "Gradient Descent(495/999):loss = 0.248472\n",
      "Gradient Descent(496/999):loss = 0.248472\n",
      "Gradient Descent(497/999):loss = 0.248488\n",
      "Gradient Descent(498/999):loss = 0.248492\n",
      "Gradient Descent(499/999):loss = 0.248492\n",
      "Gradient Descent(500/999):loss = 0.248492\n",
      "Gradient Descent(501/999):loss = 0.248488\n",
      "Gradient Descent(502/999):loss = 0.248488\n",
      "Gradient Descent(503/999):loss = 0.248488\n",
      "Gradient Descent(504/999):loss = 0.248488\n",
      "Gradient Descent(505/999):loss = 0.248488\n",
      "Gradient Descent(506/999):loss = 0.24848\n",
      "Gradient Descent(507/999):loss = 0.248488\n",
      "Gradient Descent(508/999):loss = 0.248484\n",
      "Gradient Descent(509/999):loss = 0.248488\n",
      "Gradient Descent(510/999):loss = 0.248488\n",
      "Gradient Descent(511/999):loss = 0.248484\n",
      "Gradient Descent(512/999):loss = 0.248484\n",
      "Gradient Descent(513/999):loss = 0.248484\n",
      "Gradient Descent(514/999):loss = 0.248488\n",
      "Gradient Descent(515/999):loss = 0.248492\n",
      "Gradient Descent(516/999):loss = 0.248488\n",
      "Gradient Descent(517/999):loss = 0.24848\n",
      "Gradient Descent(518/999):loss = 0.24848\n",
      "Gradient Descent(519/999):loss = 0.248468\n",
      "Gradient Descent(520/999):loss = 0.248456\n",
      "Gradient Descent(521/999):loss = 0.248456\n",
      "Gradient Descent(522/999):loss = 0.248464\n",
      "Gradient Descent(523/999):loss = 0.24846\n",
      "Gradient Descent(524/999):loss = 0.248456\n",
      "Gradient Descent(525/999):loss = 0.248452\n",
      "Gradient Descent(526/999):loss = 0.248448\n",
      "Gradient Descent(527/999):loss = 0.248452\n",
      "Gradient Descent(528/999):loss = 0.248452\n",
      "Gradient Descent(529/999):loss = 0.248444\n",
      "Gradient Descent(530/999):loss = 0.248448\n",
      "Gradient Descent(531/999):loss = 0.248452\n",
      "Gradient Descent(532/999):loss = 0.248452\n",
      "Gradient Descent(533/999):loss = 0.24846\n",
      "Gradient Descent(534/999):loss = 0.24846\n",
      "Gradient Descent(535/999):loss = 0.24846\n",
      "Gradient Descent(536/999):loss = 0.248464\n",
      "Gradient Descent(537/999):loss = 0.248468\n",
      "Gradient Descent(538/999):loss = 0.248468\n",
      "Gradient Descent(539/999):loss = 0.248468\n",
      "Gradient Descent(540/999):loss = 0.248468\n",
      "Gradient Descent(541/999):loss = 0.248472\n",
      "Gradient Descent(542/999):loss = 0.248476\n",
      "Gradient Descent(543/999):loss = 0.24848\n",
      "Gradient Descent(544/999):loss = 0.24848\n",
      "Gradient Descent(545/999):loss = 0.24848\n",
      "Gradient Descent(546/999):loss = 0.248484\n",
      "Gradient Descent(547/999):loss = 0.248484\n",
      "Gradient Descent(548/999):loss = 0.248488\n",
      "Gradient Descent(549/999):loss = 0.248492\n",
      "Gradient Descent(550/999):loss = 0.248492\n",
      "Gradient Descent(551/999):loss = 0.2485\n",
      "Gradient Descent(552/999):loss = 0.2485\n",
      "Gradient Descent(553/999):loss = 0.248496\n",
      "Gradient Descent(554/999):loss = 0.248492\n",
      "Gradient Descent(555/999):loss = 0.248492\n",
      "Gradient Descent(556/999):loss = 0.24848\n",
      "Gradient Descent(557/999):loss = 0.248476\n",
      "Gradient Descent(558/999):loss = 0.248476\n",
      "Gradient Descent(559/999):loss = 0.24848\n",
      "Gradient Descent(560/999):loss = 0.248484\n",
      "Gradient Descent(561/999):loss = 0.248484\n",
      "Gradient Descent(562/999):loss = 0.248484\n",
      "Gradient Descent(563/999):loss = 0.248484\n",
      "Gradient Descent(564/999):loss = 0.248484\n",
      "Gradient Descent(565/999):loss = 0.24848\n",
      "Gradient Descent(566/999):loss = 0.24848\n",
      "Gradient Descent(567/999):loss = 0.24848\n",
      "Gradient Descent(568/999):loss = 0.24848\n",
      "Gradient Descent(569/999):loss = 0.248476\n",
      "Gradient Descent(570/999):loss = 0.248468\n",
      "Gradient Descent(571/999):loss = 0.248468\n",
      "Gradient Descent(572/999):loss = 0.248468\n",
      "Gradient Descent(573/999):loss = 0.248468\n",
      "Gradient Descent(574/999):loss = 0.248468\n",
      "Gradient Descent(575/999):loss = 0.248468\n",
      "Gradient Descent(576/999):loss = 0.248464\n",
      "Gradient Descent(577/999):loss = 0.248464\n",
      "Gradient Descent(578/999):loss = 0.248472\n",
      "Gradient Descent(579/999):loss = 0.248472\n",
      "Gradient Descent(580/999):loss = 0.248468\n",
      "Gradient Descent(581/999):loss = 0.248472\n",
      "Gradient Descent(582/999):loss = 0.248464\n",
      "Gradient Descent(583/999):loss = 0.248464\n",
      "Gradient Descent(584/999):loss = 0.248464\n",
      "Gradient Descent(585/999):loss = 0.248464\n",
      "Gradient Descent(586/999):loss = 0.24846\n",
      "Gradient Descent(587/999):loss = 0.24846\n",
      "Gradient Descent(588/999):loss = 0.24846\n",
      "Gradient Descent(589/999):loss = 0.24846\n",
      "Gradient Descent(590/999):loss = 0.248456\n",
      "Gradient Descent(591/999):loss = 0.248456\n",
      "Gradient Descent(592/999):loss = 0.248456\n",
      "Gradient Descent(593/999):loss = 0.248456\n",
      "Gradient Descent(594/999):loss = 0.248456\n",
      "Gradient Descent(595/999):loss = 0.248452\n",
      "Gradient Descent(596/999):loss = 0.24846\n",
      "Gradient Descent(597/999):loss = 0.24846\n",
      "Gradient Descent(598/999):loss = 0.248456\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(599/999):loss = 0.248456\n",
      "Gradient Descent(600/999):loss = 0.248456\n",
      "Gradient Descent(601/999):loss = 0.248456\n",
      "Gradient Descent(602/999):loss = 0.248456\n",
      "Gradient Descent(603/999):loss = 0.248456\n",
      "Gradient Descent(604/999):loss = 0.248452\n",
      "Gradient Descent(605/999):loss = 0.248452\n",
      "Gradient Descent(606/999):loss = 0.248456\n",
      "Gradient Descent(607/999):loss = 0.248456\n",
      "Gradient Descent(608/999):loss = 0.248452\n",
      "Gradient Descent(609/999):loss = 0.248452\n",
      "Gradient Descent(610/999):loss = 0.248456\n",
      "Gradient Descent(611/999):loss = 0.248456\n",
      "Gradient Descent(612/999):loss = 0.248464\n",
      "Gradient Descent(613/999):loss = 0.248464\n",
      "Gradient Descent(614/999):loss = 0.248464\n",
      "Gradient Descent(615/999):loss = 0.248464\n",
      "Gradient Descent(616/999):loss = 0.248468\n",
      "Gradient Descent(617/999):loss = 0.248468\n",
      "Gradient Descent(618/999):loss = 0.248468\n",
      "Gradient Descent(619/999):loss = 0.248464\n",
      "Gradient Descent(620/999):loss = 0.248464\n",
      "Gradient Descent(621/999):loss = 0.248464\n",
      "Gradient Descent(622/999):loss = 0.24846\n",
      "Gradient Descent(623/999):loss = 0.248452\n",
      "Gradient Descent(624/999):loss = 0.248456\n",
      "Gradient Descent(625/999):loss = 0.248452\n",
      "Gradient Descent(626/999):loss = 0.248452\n",
      "Gradient Descent(627/999):loss = 0.248444\n",
      "Gradient Descent(628/999):loss = 0.24844\n",
      "Gradient Descent(629/999):loss = 0.24844\n",
      "Gradient Descent(630/999):loss = 0.248436\n",
      "Gradient Descent(631/999):loss = 0.248432\n",
      "Gradient Descent(632/999):loss = 0.248428\n",
      "Gradient Descent(633/999):loss = 0.248428\n",
      "Gradient Descent(634/999):loss = 0.248424\n",
      "Gradient Descent(635/999):loss = 0.24842\n",
      "Gradient Descent(636/999):loss = 0.248416\n",
      "Gradient Descent(637/999):loss = 0.248416\n",
      "Gradient Descent(638/999):loss = 0.248416\n",
      "Gradient Descent(639/999):loss = 0.248416\n",
      "Gradient Descent(640/999):loss = 0.248416\n",
      "Gradient Descent(641/999):loss = 0.248416\n",
      "Gradient Descent(642/999):loss = 0.248404\n",
      "Gradient Descent(643/999):loss = 0.248404\n",
      "Gradient Descent(644/999):loss = 0.248408\n",
      "Gradient Descent(645/999):loss = 0.248408\n",
      "Gradient Descent(646/999):loss = 0.248408\n",
      "Gradient Descent(647/999):loss = 0.248408\n",
      "Gradient Descent(648/999):loss = 0.248408\n",
      "Gradient Descent(649/999):loss = 0.248416\n",
      "Gradient Descent(650/999):loss = 0.248416\n",
      "Gradient Descent(651/999):loss = 0.248416\n",
      "Gradient Descent(652/999):loss = 0.248416\n",
      "Gradient Descent(653/999):loss = 0.248416\n",
      "Gradient Descent(654/999):loss = 0.248416\n",
      "Gradient Descent(655/999):loss = 0.248416\n",
      "Gradient Descent(656/999):loss = 0.248416\n",
      "Gradient Descent(657/999):loss = 0.248404\n",
      "Gradient Descent(658/999):loss = 0.248404\n",
      "Gradient Descent(659/999):loss = 0.248412\n",
      "Gradient Descent(660/999):loss = 0.248412\n",
      "Gradient Descent(661/999):loss = 0.248416\n",
      "Gradient Descent(662/999):loss = 0.248416\n",
      "Gradient Descent(663/999):loss = 0.248408\n",
      "Gradient Descent(664/999):loss = 0.248408\n",
      "Gradient Descent(665/999):loss = 0.248408\n",
      "Gradient Descent(666/999):loss = 0.248408\n",
      "Gradient Descent(667/999):loss = 0.2484\n",
      "Gradient Descent(668/999):loss = 0.2484\n",
      "Gradient Descent(669/999):loss = 0.2484\n",
      "Gradient Descent(670/999):loss = 0.2484\n",
      "Gradient Descent(671/999):loss = 0.248404\n",
      "Gradient Descent(672/999):loss = 0.248404\n",
      "Gradient Descent(673/999):loss = 0.248408\n",
      "Gradient Descent(674/999):loss = 0.248416\n",
      "Gradient Descent(675/999):loss = 0.248416\n",
      "Gradient Descent(676/999):loss = 0.24842\n",
      "Gradient Descent(677/999):loss = 0.24842\n",
      "Gradient Descent(678/999):loss = 0.24842\n",
      "Gradient Descent(679/999):loss = 0.24842\n",
      "Gradient Descent(680/999):loss = 0.24842\n",
      "Gradient Descent(681/999):loss = 0.24842\n",
      "Gradient Descent(682/999):loss = 0.248424\n",
      "Gradient Descent(683/999):loss = 0.248424\n",
      "Gradient Descent(684/999):loss = 0.248428\n",
      "Gradient Descent(685/999):loss = 0.248428\n",
      "Gradient Descent(686/999):loss = 0.248432\n",
      "Gradient Descent(687/999):loss = 0.248432\n",
      "Gradient Descent(688/999):loss = 0.248432\n",
      "Gradient Descent(689/999):loss = 0.248432\n",
      "Gradient Descent(690/999):loss = 0.248436\n",
      "Gradient Descent(691/999):loss = 0.24844\n",
      "Gradient Descent(692/999):loss = 0.248436\n",
      "Gradient Descent(693/999):loss = 0.24844\n",
      "Gradient Descent(694/999):loss = 0.24844\n",
      "Gradient Descent(695/999):loss = 0.24844\n",
      "Gradient Descent(696/999):loss = 0.24844\n",
      "Gradient Descent(697/999):loss = 0.24844\n",
      "Gradient Descent(698/999):loss = 0.24844\n",
      "Gradient Descent(699/999):loss = 0.24844\n",
      "Gradient Descent(700/999):loss = 0.248432\n",
      "Gradient Descent(701/999):loss = 0.248432\n",
      "Gradient Descent(702/999):loss = 0.248432\n",
      "Gradient Descent(703/999):loss = 0.248428\n",
      "Gradient Descent(704/999):loss = 0.248428\n",
      "Gradient Descent(705/999):loss = 0.248428\n",
      "Gradient Descent(706/999):loss = 0.248428\n",
      "Gradient Descent(707/999):loss = 0.248424\n",
      "Gradient Descent(708/999):loss = 0.248424\n",
      "Gradient Descent(709/999):loss = 0.248424\n",
      "Gradient Descent(710/999):loss = 0.248424\n",
      "Gradient Descent(711/999):loss = 0.248424\n",
      "Gradient Descent(712/999):loss = 0.248424\n",
      "Gradient Descent(713/999):loss = 0.248432\n",
      "Gradient Descent(714/999):loss = 0.248432\n",
      "Gradient Descent(715/999):loss = 0.248432\n",
      "Gradient Descent(716/999):loss = 0.248432\n",
      "Gradient Descent(717/999):loss = 0.248432\n",
      "Gradient Descent(718/999):loss = 0.248432\n",
      "Gradient Descent(719/999):loss = 0.248432\n",
      "Gradient Descent(720/999):loss = 0.248432\n",
      "Gradient Descent(721/999):loss = 0.248432\n",
      "Gradient Descent(722/999):loss = 0.248428\n",
      "Gradient Descent(723/999):loss = 0.248428\n",
      "Gradient Descent(724/999):loss = 0.248428\n",
      "Gradient Descent(725/999):loss = 0.248424\n",
      "Gradient Descent(726/999):loss = 0.248424\n",
      "Gradient Descent(727/999):loss = 0.248424\n",
      "Gradient Descent(728/999):loss = 0.248424\n",
      "Gradient Descent(729/999):loss = 0.248428\n",
      "Gradient Descent(730/999):loss = 0.248428\n",
      "Gradient Descent(731/999):loss = 0.248428\n",
      "Gradient Descent(732/999):loss = 0.248428\n",
      "Gradient Descent(733/999):loss = 0.248424\n",
      "Gradient Descent(734/999):loss = 0.248424\n",
      "Gradient Descent(735/999):loss = 0.248424\n",
      "Gradient Descent(736/999):loss = 0.248428\n",
      "Gradient Descent(737/999):loss = 0.248428\n",
      "Gradient Descent(738/999):loss = 0.248428\n",
      "Gradient Descent(739/999):loss = 0.248428\n",
      "Gradient Descent(740/999):loss = 0.248428\n",
      "Gradient Descent(741/999):loss = 0.248432\n",
      "Gradient Descent(742/999):loss = 0.248432\n",
      "Gradient Descent(743/999):loss = 0.248432\n",
      "Gradient Descent(744/999):loss = 0.248436\n",
      "Gradient Descent(745/999):loss = 0.248436\n",
      "Gradient Descent(746/999):loss = 0.248436\n",
      "Gradient Descent(747/999):loss = 0.24844\n",
      "Gradient Descent(748/999):loss = 0.24844\n",
      "Gradient Descent(749/999):loss = 0.24844\n",
      "Gradient Descent(750/999):loss = 0.24844\n",
      "Gradient Descent(751/999):loss = 0.24844\n",
      "Gradient Descent(752/999):loss = 0.24844\n",
      "Gradient Descent(753/999):loss = 0.24844\n",
      "Gradient Descent(754/999):loss = 0.24844\n",
      "Gradient Descent(755/999):loss = 0.248444\n",
      "Gradient Descent(756/999):loss = 0.248444\n",
      "Gradient Descent(757/999):loss = 0.248444\n",
      "Gradient Descent(758/999):loss = 0.248448\n",
      "Gradient Descent(759/999):loss = 0.248444\n",
      "Gradient Descent(760/999):loss = 0.248448\n",
      "Gradient Descent(761/999):loss = 0.248448\n",
      "Gradient Descent(762/999):loss = 0.24844\n",
      "Gradient Descent(763/999):loss = 0.248436\n",
      "Gradient Descent(764/999):loss = 0.248436\n",
      "Gradient Descent(765/999):loss = 0.248436\n",
      "Gradient Descent(766/999):loss = 0.248436\n",
      "Gradient Descent(767/999):loss = 0.248436\n",
      "Gradient Descent(768/999):loss = 0.248436\n",
      "Gradient Descent(769/999):loss = 0.248436\n",
      "Gradient Descent(770/999):loss = 0.248436\n",
      "Gradient Descent(771/999):loss = 0.248436\n",
      "Gradient Descent(772/999):loss = 0.248436\n",
      "Gradient Descent(773/999):loss = 0.248436\n",
      "Gradient Descent(774/999):loss = 0.248436\n",
      "Gradient Descent(775/999):loss = 0.248436\n",
      "Gradient Descent(776/999):loss = 0.248436\n",
      "Gradient Descent(777/999):loss = 0.248436\n",
      "Gradient Descent(778/999):loss = 0.248436\n",
      "Gradient Descent(779/999):loss = 0.248436\n",
      "Gradient Descent(780/999):loss = 0.248436\n",
      "Gradient Descent(781/999):loss = 0.248432\n",
      "Gradient Descent(782/999):loss = 0.248432\n",
      "Gradient Descent(783/999):loss = 0.248432\n",
      "Gradient Descent(784/999):loss = 0.248432\n",
      "Gradient Descent(785/999):loss = 0.248436\n",
      "Gradient Descent(786/999):loss = 0.248436\n",
      "Gradient Descent(787/999):loss = 0.248436\n",
      "Gradient Descent(788/999):loss = 0.248436\n",
      "Gradient Descent(789/999):loss = 0.248436\n",
      "Gradient Descent(790/999):loss = 0.248436\n",
      "Gradient Descent(791/999):loss = 0.248432\n",
      "Gradient Descent(792/999):loss = 0.248432\n",
      "Gradient Descent(793/999):loss = 0.248432\n",
      "Gradient Descent(794/999):loss = 0.248432\n",
      "Gradient Descent(795/999):loss = 0.248432\n",
      "Gradient Descent(796/999):loss = 0.248432\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(797/999):loss = 0.248432\n",
      "Gradient Descent(798/999):loss = 0.248432\n",
      "Gradient Descent(799/999):loss = 0.248428\n",
      "Gradient Descent(800/999):loss = 0.248424\n",
      "Gradient Descent(801/999):loss = 0.248424\n",
      "Gradient Descent(802/999):loss = 0.248424\n",
      "Gradient Descent(803/999):loss = 0.248424\n",
      "Gradient Descent(804/999):loss = 0.24842\n",
      "Gradient Descent(805/999):loss = 0.24842\n",
      "Gradient Descent(806/999):loss = 0.24842\n",
      "Gradient Descent(807/999):loss = 0.24842\n",
      "Gradient Descent(808/999):loss = 0.248424\n",
      "Gradient Descent(809/999):loss = 0.248424\n",
      "Gradient Descent(810/999):loss = 0.24842\n",
      "Gradient Descent(811/999):loss = 0.24842\n",
      "Gradient Descent(812/999):loss = 0.24842\n",
      "Gradient Descent(813/999):loss = 0.248424\n",
      "Gradient Descent(814/999):loss = 0.248424\n",
      "Gradient Descent(815/999):loss = 0.248428\n",
      "Gradient Descent(816/999):loss = 0.248428\n",
      "Gradient Descent(817/999):loss = 0.248428\n",
      "Gradient Descent(818/999):loss = 0.248428\n",
      "Gradient Descent(819/999):loss = 0.248428\n",
      "Gradient Descent(820/999):loss = 0.248436\n",
      "Gradient Descent(821/999):loss = 0.248436\n",
      "Gradient Descent(822/999):loss = 0.248436\n",
      "Gradient Descent(823/999):loss = 0.248428\n",
      "Gradient Descent(824/999):loss = 0.248432\n",
      "Gradient Descent(825/999):loss = 0.248432\n",
      "Gradient Descent(826/999):loss = 0.248432\n",
      "Gradient Descent(827/999):loss = 0.248428\n",
      "Gradient Descent(828/999):loss = 0.248428\n",
      "Gradient Descent(829/999):loss = 0.248428\n",
      "Gradient Descent(830/999):loss = 0.248428\n",
      "Gradient Descent(831/999):loss = 0.248428\n",
      "Gradient Descent(832/999):loss = 0.248428\n",
      "Gradient Descent(833/999):loss = 0.248424\n",
      "Gradient Descent(834/999):loss = 0.248424\n",
      "Gradient Descent(835/999):loss = 0.248424\n",
      "Gradient Descent(836/999):loss = 0.248424\n",
      "Gradient Descent(837/999):loss = 0.248424\n",
      "Gradient Descent(838/999):loss = 0.24842\n",
      "Gradient Descent(839/999):loss = 0.24842\n",
      "Gradient Descent(840/999):loss = 0.248416\n",
      "Gradient Descent(841/999):loss = 0.248416\n",
      "Gradient Descent(842/999):loss = 0.248416\n",
      "Gradient Descent(843/999):loss = 0.248416\n",
      "Gradient Descent(844/999):loss = 0.248416\n",
      "Gradient Descent(845/999):loss = 0.248416\n",
      "Gradient Descent(846/999):loss = 0.248412\n",
      "Gradient Descent(847/999):loss = 0.248408\n",
      "Gradient Descent(848/999):loss = 0.248408\n",
      "Gradient Descent(849/999):loss = 0.248408\n",
      "Gradient Descent(850/999):loss = 0.248408\n",
      "Gradient Descent(851/999):loss = 0.248408\n",
      "Gradient Descent(852/999):loss = 0.248408\n",
      "Gradient Descent(853/999):loss = 0.248404\n",
      "Gradient Descent(854/999):loss = 0.248404\n",
      "Gradient Descent(855/999):loss = 0.248404\n",
      "Gradient Descent(856/999):loss = 0.248404\n",
      "Gradient Descent(857/999):loss = 0.2484\n",
      "Gradient Descent(858/999):loss = 0.248404\n",
      "Gradient Descent(859/999):loss = 0.2484\n",
      "Gradient Descent(860/999):loss = 0.2484\n",
      "Gradient Descent(861/999):loss = 0.248396\n",
      "Gradient Descent(862/999):loss = 0.248396\n",
      "Gradient Descent(863/999):loss = 0.248396\n",
      "Gradient Descent(864/999):loss = 0.248396\n",
      "Gradient Descent(865/999):loss = 0.248396\n",
      "Gradient Descent(866/999):loss = 0.248396\n",
      "Gradient Descent(867/999):loss = 0.248396\n",
      "Gradient Descent(868/999):loss = 0.248396\n",
      "Gradient Descent(869/999):loss = 0.248392\n",
      "Gradient Descent(870/999):loss = 0.248396\n",
      "Gradient Descent(871/999):loss = 0.248396\n",
      "Gradient Descent(872/999):loss = 0.248396\n",
      "Gradient Descent(873/999):loss = 0.248396\n",
      "Gradient Descent(874/999):loss = 0.248396\n",
      "Gradient Descent(875/999):loss = 0.248388\n",
      "Gradient Descent(876/999):loss = 0.248392\n",
      "Gradient Descent(877/999):loss = 0.248388\n",
      "Gradient Descent(878/999):loss = 0.248388\n",
      "Gradient Descent(879/999):loss = 0.248384\n",
      "Gradient Descent(880/999):loss = 0.248384\n",
      "Gradient Descent(881/999):loss = 0.248384\n",
      "Gradient Descent(882/999):loss = 0.248384\n",
      "Gradient Descent(883/999):loss = 0.248384\n",
      "Gradient Descent(884/999):loss = 0.248384\n",
      "Gradient Descent(885/999):loss = 0.248384\n",
      "Gradient Descent(886/999):loss = 0.248384\n",
      "Gradient Descent(887/999):loss = 0.248384\n",
      "Gradient Descent(888/999):loss = 0.248388\n",
      "Gradient Descent(889/999):loss = 0.248392\n",
      "Gradient Descent(890/999):loss = 0.248396\n",
      "Gradient Descent(891/999):loss = 0.248396\n",
      "Gradient Descent(892/999):loss = 0.248396\n",
      "Gradient Descent(893/999):loss = 0.2484\n",
      "Gradient Descent(894/999):loss = 0.2484\n",
      "Gradient Descent(895/999):loss = 0.248404\n",
      "Gradient Descent(896/999):loss = 0.248404\n",
      "Gradient Descent(897/999):loss = 0.248404\n",
      "Gradient Descent(898/999):loss = 0.248404\n",
      "Gradient Descent(899/999):loss = 0.248404\n",
      "Gradient Descent(900/999):loss = 0.248404\n",
      "Gradient Descent(901/999):loss = 0.248408\n",
      "Gradient Descent(902/999):loss = 0.248412\n",
      "Gradient Descent(903/999):loss = 0.248412\n",
      "Gradient Descent(904/999):loss = 0.248412\n",
      "Gradient Descent(905/999):loss = 0.248412\n",
      "Gradient Descent(906/999):loss = 0.248412\n",
      "Gradient Descent(907/999):loss = 0.248408\n",
      "Gradient Descent(908/999):loss = 0.248412\n",
      "Gradient Descent(909/999):loss = 0.248412\n",
      "Gradient Descent(910/999):loss = 0.248416\n",
      "Gradient Descent(911/999):loss = 0.248416\n",
      "Gradient Descent(912/999):loss = 0.248416\n",
      "Gradient Descent(913/999):loss = 0.248416\n",
      "Gradient Descent(914/999):loss = 0.248416\n",
      "Gradient Descent(915/999):loss = 0.248416\n",
      "Gradient Descent(916/999):loss = 0.248416\n",
      "Gradient Descent(917/999):loss = 0.248416\n",
      "Gradient Descent(918/999):loss = 0.248416\n",
      "Gradient Descent(919/999):loss = 0.248416\n",
      "Gradient Descent(920/999):loss = 0.248416\n",
      "Gradient Descent(921/999):loss = 0.248416\n",
      "Gradient Descent(922/999):loss = 0.248416\n",
      "Gradient Descent(923/999):loss = 0.24842\n",
      "Gradient Descent(924/999):loss = 0.24842\n",
      "Gradient Descent(925/999):loss = 0.248416\n",
      "Gradient Descent(926/999):loss = 0.248416\n",
      "Gradient Descent(927/999):loss = 0.248416\n",
      "Gradient Descent(928/999):loss = 0.248416\n",
      "Gradient Descent(929/999):loss = 0.248416\n",
      "Gradient Descent(930/999):loss = 0.248416\n",
      "Gradient Descent(931/999):loss = 0.248416\n",
      "Gradient Descent(932/999):loss = 0.248416\n",
      "Gradient Descent(933/999):loss = 0.248416\n",
      "Gradient Descent(934/999):loss = 0.24842\n",
      "Gradient Descent(935/999):loss = 0.24842\n",
      "Gradient Descent(936/999):loss = 0.24842\n",
      "Gradient Descent(937/999):loss = 0.24842\n",
      "Gradient Descent(938/999):loss = 0.24842\n",
      "Gradient Descent(939/999):loss = 0.24842\n",
      "Gradient Descent(940/999):loss = 0.24842\n",
      "Gradient Descent(941/999):loss = 0.24842\n",
      "Gradient Descent(942/999):loss = 0.248416\n",
      "Gradient Descent(943/999):loss = 0.248416\n",
      "Gradient Descent(944/999):loss = 0.24842\n",
      "Gradient Descent(945/999):loss = 0.24842\n",
      "Gradient Descent(946/999):loss = 0.24842\n",
      "Gradient Descent(947/999):loss = 0.24842\n",
      "Gradient Descent(948/999):loss = 0.24842\n",
      "Gradient Descent(949/999):loss = 0.24842\n",
      "Gradient Descent(950/999):loss = 0.248412\n",
      "Gradient Descent(951/999):loss = 0.248412\n",
      "Gradient Descent(952/999):loss = 0.248412\n",
      "Gradient Descent(953/999):loss = 0.248412\n",
      "Gradient Descent(954/999):loss = 0.248412\n",
      "Gradient Descent(955/999):loss = 0.248412\n",
      "Gradient Descent(956/999):loss = 0.248408\n",
      "Gradient Descent(957/999):loss = 0.248404\n",
      "Gradient Descent(958/999):loss = 0.248404\n",
      "Gradient Descent(959/999):loss = 0.248404\n",
      "Gradient Descent(960/999):loss = 0.248404\n",
      "Gradient Descent(961/999):loss = 0.248404\n",
      "Gradient Descent(962/999):loss = 0.248404\n",
      "Gradient Descent(963/999):loss = 0.248404\n",
      "Gradient Descent(964/999):loss = 0.248404\n",
      "Gradient Descent(965/999):loss = 0.248408\n",
      "Gradient Descent(966/999):loss = 0.248408\n",
      "Gradient Descent(967/999):loss = 0.248408\n",
      "Gradient Descent(968/999):loss = 0.248408\n",
      "Gradient Descent(969/999):loss = 0.2484\n",
      "Gradient Descent(970/999):loss = 0.2484\n",
      "Gradient Descent(971/999):loss = 0.2484\n",
      "Gradient Descent(972/999):loss = 0.248404\n",
      "Gradient Descent(973/999):loss = 0.248404\n",
      "Gradient Descent(974/999):loss = 0.248404\n",
      "Gradient Descent(975/999):loss = 0.248404\n",
      "Gradient Descent(976/999):loss = 0.248404\n",
      "Gradient Descent(977/999):loss = 0.248404\n",
      "Gradient Descent(978/999):loss = 0.248404\n",
      "Gradient Descent(979/999):loss = 0.248404\n",
      "Gradient Descent(980/999):loss = 0.248404\n",
      "Gradient Descent(981/999):loss = 0.248408\n",
      "Gradient Descent(982/999):loss = 0.248408\n",
      "Gradient Descent(983/999):loss = 0.248408\n",
      "Gradient Descent(984/999):loss = 0.248408\n",
      "Gradient Descent(985/999):loss = 0.248408\n",
      "Gradient Descent(986/999):loss = 0.248408\n",
      "Gradient Descent(987/999):loss = 0.248404\n",
      "Gradient Descent(988/999):loss = 0.248404\n",
      "Gradient Descent(989/999):loss = 0.248404\n",
      "Gradient Descent(990/999):loss = 0.248408\n",
      "Gradient Descent(991/999):loss = 0.248408\n",
      "Gradient Descent(992/999):loss = 0.248408\n",
      "Gradient Descent(993/999):loss = 0.248408\n",
      "Gradient Descent(994/999):loss = 0.248408\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(995/999):loss = 0.248412\n",
      "Gradient Descent(996/999):loss = 0.248412\n",
      "Gradient Descent(997/999):loss = 0.248412\n",
      "Gradient Descent(998/999):loss = 0.248412\n",
      "Gradient Descent(999/999):loss = 0.248412\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(<function Regressions.pred_logistic>,\n",
       " array([-8.59719516e-01, -3.10398077e-02, -6.98315178e-01, -7.48188127e-01,\n",
       "         2.84356006e-01, -7.69243480e-03,  4.14745357e-01,  4.32831241e-02,\n",
       "         8.83859082e-01, -9.56927041e-02,  1.13644547e-01, -5.27008149e-01,\n",
       "         2.78513816e-01,  2.03438374e-01,  5.15337678e-01, -3.08028537e-03,\n",
       "        -2.85934184e-03,  7.04525929e-01, -1.92083766e-03,  7.05946088e-03,\n",
       "         1.61979545e-01,  7.21970131e-04, -1.23938409e-01,  5.23153001e-02,\n",
       "        -1.65107805e-01,  5.56621553e-04,  9.21699176e-05, -1.12148132e-01,\n",
       "         5.19658443e-03, -3.50652223e-03, -1.42280335e-01]),\n",
       " 0.25034844444444443,\n",
       " 0.2503639999999999)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logistic_regression(y,x_all_int,w0,max_iters= 1000,  printing = True, gamma = 0.000005 )\n",
    "cross_validation(y,x_all_int,10, logistic_regression,w0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/charles/Desktop/machine learning/projets/ML_CS433_projet1/src/data_utility.py:86: RuntimeWarning: invalid value encountered in log\n",
      "  return np.log(feature + n)\n",
      "/home/charles/Desktop/machine learning/projets/ML_CS433_projet1/src/data_utility.py:273: RuntimeWarning: invalid value encountered in greater_equal\n",
      "  y_cat[y_hat >= 0.5] = 1\n",
      "/home/charles/Desktop/machine learning/projets/ML_CS433_projet1/src/data_utility.py:274: RuntimeWarning: invalid value encountered in less\n",
      "  y_cat[y_hat < 0.5] = 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<function logistic_regression at 0x7fbe935ef048>, array([[0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.]])] [<function id at 0x7fbe935ec378>, []]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "w = w0\n",
    "lambdas = np.logspace(-7.5,-5.5,6)\n",
    "\n",
    "k_fold = 10\n",
    "\n",
    "l_p_1 = [4,9,14,17,29]\n",
    "# l_p_2 = [20,23]\n",
    "# l_p_3 = [3]\n",
    "\n",
    "# multi_transform = [[log_plus, l_p_1, 1],\n",
    "#                                           [log_plus, l_p_2, 2],\n",
    "#                                           [log_plus, l_p_3, 3]]\n",
    "\n",
    "# [feature_transform, [np.log, [10,13]]]\n",
    "transformations = [[id,[]],\n",
    "                  [feature_transform, [log_plus, l_p_1, 1]]]\n",
    "\n",
    "methods = [[ridge_regression, lambdas],\n",
    "          [logistic_regression, [w]]]\n",
    "#           [reg_logistic_regression, [{'lambda_':10**(-6), 'w':w}]]]\n",
    "\n",
    "predictor, w, loss_tr, loss_te, transformation, method = multi_cross_validation(y, x_all_int, k_fold, transformations=transformations, methods=methods, seed=2)\n",
    "\n",
    "print(method, transformation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "essai = np.ones(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "essai2 = 2*np.ones(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(essai*essai2).shape == essai.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_utility import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.3847000e+02,  5.1655000e+01,  9.7827000e+01, ...,\n",
       "        -2.4750000e+00,  1.1349700e+02,  1.1348701e+02],\n",
       "       [ 1.6093700e+02,  6.8768000e+01,  1.0323500e+02, ...,\n",
       "        -9.9900000e+02,  4.6226000e+01,  9.9800100e+05],\n",
       "       [-9.9900000e+02,  1.6217200e+02,  1.2595300e+02, ...,\n",
       "        -9.9900000e+02,  4.4251000e+01,  9.9800100e+05],\n",
       "       ...,\n",
       "       [ 1.0545700e+02,  6.0526000e+01,  7.5839000e+01, ...,\n",
       "        -9.9900000e+02,  4.1992000e+01,  9.9800100e+05],\n",
       "       [ 9.4951000e+01,  1.9362000e+01,  6.8812000e+01, ...,\n",
       "        -9.9900000e+02,  0.0000000e+00,  9.9800100e+05],\n",
       "       [-9.9900000e+02,  7.2756000e+01,  7.0831000e+01, ...,\n",
       "        -9.9900000e+02,  0.0000000e+00,  9.9800100e+05]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "interactions(x_all,4,5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
